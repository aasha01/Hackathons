{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced MLP\n",
    "- Advanced techniques for training neural networks\n",
    "    - Weight Initialization\n",
    "    - Nonlinearity (Activation function)\n",
    "    - Optimizers\n",
    "    - Batch Normalization\n",
    "    - Dropout (Regularization)\n",
    "    - Model Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\aasha\\\\anaconda3\\\\envs\\\\tfgpu\\\\python.exe'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-17bb7203622b>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2 \n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers import BatchNormalization, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "- MNIST dataset\n",
    "- source: http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x22416256848>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train=pd.read_csv(\"D:\\\\Hackeathon\\\\AnalyticsVidhya\\\\Digits_MNIST\\\\Train\\\\train.csv\")\n",
    "test=pd.read_csv(\"D:\\\\Hackeathon\\\\AnalyticsVidhya\\\\Digits_MNIST\\\\Test.csv\")\n",
    "\n",
    "train_image_path=\"D:\\\\Hackeathon\\\\AnalyticsVidhya\\\\Digits_MNIST\\\\Train\\\\Images\\\\train\\\\\"\n",
    "\n",
    "##Loading Training Images\n",
    "train_img=[]\n",
    "for i in train['filename']:\n",
    "    img=cv2.imread(os.path.join(train_image_path,i))\n",
    "    train_img.append(img)\n",
    "\n",
    "plt.imshow(train_img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x22416736ac8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_image_path=\"D:\\\\Hackeathon\\\\AnalyticsVidhya\\\\Digits_MNIST\\\\Train\\\\Images\\\\test\\\\\"\n",
    "\n",
    "##Loading Testing Images\n",
    "test_img=[]\n",
    "for i in test['filename']:\n",
    "    img=cv2.imread(os.path.join(test_image_path,i))\n",
    "    test_img.append(img)\n",
    "\n",
    "plt.imshow(test_img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(train_img)\n",
    "X_test_sub = np.array(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((49000, 28, 28, 3), (21000, 28, 28, 3))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test_sub.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Not needed as its a Hackathon\n",
    "# use only 33% of training data to expedite the training process\n",
    "X_train, x_test , y_train, y_test = train_test_split(X_train, train_label, test_size = 0.67, random_state = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting y data into categorical (one-hot encoding)\n",
    "y_train = to_categorical(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49000, 28, 28, 3) (21000, 28, 28, 3) (49000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test_sub.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best CNN expected Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_cnn():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (3,3), activation='relu', kernel_initializer='he_normal', input_shape=(28,28,3)),\n",
    "        tf.keras.layers.Conv2D(64, (3,3), activation='relu', kernel_initializer='he_normal'),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu', kernel_initializer='he_normal'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_cnn_batch():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3,3), kernel_initializer='he_normal', input_shape=(28,28,3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(64, (3,3), kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(2,2))    \n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(128, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    adam = tf.keras.optimizers.Adam(lr = 0.001) \n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "#99.3 in Analytics Vidya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_cnn_batch_02():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3,3), kernel_initializer='he_normal', input_shape=(28,28,3), strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(64, (3,3), kernel_initializer='he_normal', strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(2,2))    \n",
    "    \n",
    "    model.add(Conv2D(32, (3,3), kernel_initializer='he_normal', input_shape=(28,28,3), strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(64, (3,3), kernel_initializer='he_normal', strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(2,2)) \n",
    "    \n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(128, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    adam = tf.keras.optimizers.Adam(lr = 0.001) \n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "#99.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model_cnn_batch_02()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_board = tf.keras.callbacks.TensorBoard('./logs/LeNet-MNIST-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((49000, 28, 28, 3), (49000, 10))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34300 samples, validate on 14700 samples\n",
      "Epoch 1/100\n",
      "34300/34300 [==============================] - 9s 258us/step - loss: 0.2483 - accuracy: 0.9307 - val_loss: 0.0711 - val_accuracy: 0.9793\n",
      "Epoch 2/100\n",
      "34300/34300 [==============================] - 7s 218us/step - loss: 0.0775 - accuracy: 0.9787 - val_loss: 0.0560 - val_accuracy: 0.9820\n",
      "Epoch 3/100\n",
      "34300/34300 [==============================] - 8s 224us/step - loss: 0.0542 - accuracy: 0.9849 - val_loss: 0.0444 - val_accuracy: 0.9863\n",
      "Epoch 4/100\n",
      "34300/34300 [==============================] - 8s 220us/step - loss: 0.0466 - accuracy: 0.9865 - val_loss: 0.0595 - val_accuracy: 0.9819\n",
      "Epoch 5/100\n",
      "34300/34300 [==============================] - 7s 216us/step - loss: 0.0400 - accuracy: 0.9882 - val_loss: 0.0298 - val_accuracy: 0.9911\n",
      "Epoch 6/100\n",
      "34300/34300 [==============================] - 7s 216us/step - loss: 0.0333 - accuracy: 0.9905 - val_loss: 0.0520 - val_accuracy: 0.9849\n",
      "Epoch 7/100\n",
      "34300/34300 [==============================] - 7s 216us/step - loss: 0.0291 - accuracy: 0.9911 - val_loss: 0.0325 - val_accuracy: 0.9901\n",
      "Epoch 8/100\n",
      "34300/34300 [==============================] - 7s 216us/step - loss: 0.0256 - accuracy: 0.9923 - val_loss: 0.0348 - val_accuracy: 0.9898\n",
      "Epoch 9/100\n",
      "34300/34300 [==============================] - 7s 217us/step - loss: 0.0248 - accuracy: 0.9925 - val_loss: 0.0393 - val_accuracy: 0.9882\n",
      "Epoch 10/100\n",
      "34300/34300 [==============================] - 7s 217us/step - loss: 0.0214 - accuracy: 0.9931 - val_loss: 0.0309 - val_accuracy: 0.9903\n",
      "Epoch 11/100\n",
      "34300/34300 [==============================] - 7s 218us/step - loss: 0.0191 - accuracy: 0.9941 - val_loss: 0.0361 - val_accuracy: 0.9903\n",
      "Epoch 12/100\n",
      "34300/34300 [==============================] - 7s 217us/step - loss: 0.0197 - accuracy: 0.9938 - val_loss: 0.0307 - val_accuracy: 0.9909\n",
      "Epoch 13/100\n",
      "34300/34300 [==============================] - 7s 217us/step - loss: 0.0185 - accuracy: 0.9942 - val_loss: 0.0298 - val_accuracy: 0.9912\n",
      "Epoch 14/100\n",
      "34300/34300 [==============================] - 7s 217us/step - loss: 0.0152 - accuracy: 0.9953 - val_loss: 0.0340 - val_accuracy: 0.9911\n",
      "Epoch 15/100\n",
      "34300/34300 [==============================] - 7s 218us/step - loss: 0.0155 - accuracy: 0.9952 - val_loss: 0.0397 - val_accuracy: 0.9887\n",
      "Epoch 16/100\n",
      "34300/34300 [==============================] - 7s 218us/step - loss: 0.0162 - accuracy: 0.9952 - val_loss: 0.0285 - val_accuracy: 0.9922\n",
      "Epoch 17/100\n",
      "34300/34300 [==============================] - 7s 218us/step - loss: 0.0137 - accuracy: 0.9956 - val_loss: 0.0350 - val_accuracy: 0.9903\n",
      "Epoch 18/100\n",
      "34300/34300 [==============================] - 7s 218us/step - loss: 0.0129 - accuracy: 0.9957 - val_loss: 0.0303 - val_accuracy: 0.9910\n",
      "Epoch 19/100\n",
      "34300/34300 [==============================] - 8s 219us/step - loss: 0.0140 - accuracy: 0.9952 - val_loss: 0.0300 - val_accuracy: 0.9908\n",
      "Epoch 20/100\n",
      "34300/34300 [==============================] - 8s 224us/step - loss: 0.0110 - accuracy: 0.9966 - val_loss: 0.0291 - val_accuracy: 0.9918\n",
      "Epoch 21/100\n",
      "34300/34300 [==============================] - 8s 231us/step - loss: 0.0114 - accuracy: 0.9964 - val_loss: 0.0257 - val_accuracy: 0.9931\n",
      "Epoch 22/100\n",
      "34300/34300 [==============================] - 8s 221us/step - loss: 0.0111 - accuracy: 0.9962 - val_loss: 0.0410 - val_accuracy: 0.9895\n",
      "Epoch 23/100\n",
      "34300/34300 [==============================] - 8s 221us/step - loss: 0.0109 - accuracy: 0.9968 - val_loss: 0.0300 - val_accuracy: 0.9924\n",
      "Epoch 24/100\n",
      "34300/34300 [==============================] - 8s 219us/step - loss: 0.0081 - accuracy: 0.9974 - val_loss: 0.0265 - val_accuracy: 0.9924\n",
      "Epoch 25/100\n",
      "34300/34300 [==============================] - 8s 226us/step - loss: 0.0075 - accuracy: 0.9976 - val_loss: 0.0295 - val_accuracy: 0.9927\n",
      "Epoch 26/100\n",
      "34300/34300 [==============================] - 8s 235us/step - loss: 0.0115 - accuracy: 0.9959 - val_loss: 0.0394 - val_accuracy: 0.9903\n",
      "Epoch 27/100\n",
      "34300/34300 [==============================] - 8s 226us/step - loss: 0.0087 - accuracy: 0.9969 - val_loss: 0.0282 - val_accuracy: 0.9929\n",
      "Epoch 28/100\n",
      "34300/34300 [==============================] - 8s 226us/step - loss: 0.0086 - accuracy: 0.9972 - val_loss: 0.0412 - val_accuracy: 0.9895\n",
      "Epoch 29/100\n",
      "34300/34300 [==============================] - 8s 226us/step - loss: 0.0071 - accuracy: 0.9977 - val_loss: 0.0263 - val_accuracy: 0.9932\n",
      "Epoch 30/100\n",
      "34300/34300 [==============================] - 8s 227us/step - loss: 0.0106 - accuracy: 0.9964 - val_loss: 0.0368 - val_accuracy: 0.9905\n",
      "Epoch 31/100\n",
      "34300/34300 [==============================] - 8s 228us/step - loss: 0.0073 - accuracy: 0.9976 - val_loss: 0.0265 - val_accuracy: 0.9933\n",
      "Epoch 32/100\n",
      "34300/34300 [==============================] - 8s 224us/step - loss: 0.0064 - accuracy: 0.9980 - val_loss: 0.0319 - val_accuracy: 0.9926\n",
      "Epoch 33/100\n",
      "34300/34300 [==============================] - 8s 225us/step - loss: 0.0063 - accuracy: 0.9980 - val_loss: 0.0348 - val_accuracy: 0.9909\n",
      "Epoch 34/100\n",
      "34300/34300 [==============================] - 8s 224us/step - loss: 0.0079 - accuracy: 0.9978 - val_loss: 0.0350 - val_accuracy: 0.9918\n",
      "Epoch 35/100\n",
      "34300/34300 [==============================] - 8s 225us/step - loss: 0.0082 - accuracy: 0.9973 - val_loss: 0.0392 - val_accuracy: 0.9910\n",
      "Epoch 36/100\n",
      "34300/34300 [==============================] - 8s 224us/step - loss: 0.0057 - accuracy: 0.9983 - val_loss: 0.0284 - val_accuracy: 0.9935\n",
      "Epoch 37/100\n",
      "34300/34300 [==============================] - 8s 227us/step - loss: 0.0048 - accuracy: 0.9986 - val_loss: 0.0314 - val_accuracy: 0.9931\n",
      "Epoch 38/100\n",
      "34300/34300 [==============================] - 8s 223us/step - loss: 0.0063 - accuracy: 0.9978 - val_loss: 0.0276 - val_accuracy: 0.9930\n",
      "Epoch 39/100\n",
      "34300/34300 [==============================] - 8s 227us/step - loss: 0.0051 - accuracy: 0.9985 - val_loss: 0.0314 - val_accuracy: 0.9927\n",
      "Epoch 40/100\n",
      "34300/34300 [==============================] - 8s 221us/step - loss: 0.0067 - accuracy: 0.9980 - val_loss: 0.0368 - val_accuracy: 0.9913\n",
      "Epoch 41/100\n",
      "34300/34300 [==============================] - 8s 231us/step - loss: 0.0069 - accuracy: 0.9978 - val_loss: 0.0330 - val_accuracy: 0.9923\n",
      "Epoch 42/100\n",
      "34300/34300 [==============================] - 8s 222us/step - loss: 0.0050 - accuracy: 0.9985 - val_loss: 0.0350 - val_accuracy: 0.9929\n",
      "Epoch 43/100\n",
      "34300/34300 [==============================] - 8s 225us/step - loss: 0.0054 - accuracy: 0.9983 - val_loss: 0.0273 - val_accuracy: 0.9930\n",
      "Epoch 44/100\n",
      "34300/34300 [==============================] - 8s 224us/step - loss: 0.0058 - accuracy: 0.9980 - val_loss: 0.0393 - val_accuracy: 0.9912\n",
      "Epoch 45/100\n",
      "34300/34300 [==============================] - 8s 222us/step - loss: 0.0047 - accuracy: 0.9986 - val_loss: 0.0280 - val_accuracy: 0.9937\n",
      "Epoch 46/100\n",
      "34300/34300 [==============================] - 8s 221us/step - loss: 0.0048 - accuracy: 0.9983 - val_loss: 0.0325 - val_accuracy: 0.9915\n",
      "Epoch 47/100\n",
      "34300/34300 [==============================] - 8s 222us/step - loss: 0.0049 - accuracy: 0.9983 - val_loss: 0.0353 - val_accuracy: 0.9928\n",
      "Epoch 48/100\n",
      "34300/34300 [==============================] - 8s 222us/step - loss: 0.0054 - accuracy: 0.9980 - val_loss: 0.0265 - val_accuracy: 0.9941\n",
      "Epoch 49/100\n",
      "34300/34300 [==============================] - 8s 224us/step - loss: 0.0048 - accuracy: 0.9985 - val_loss: 0.0328 - val_accuracy: 0.9922\n",
      "Epoch 50/100\n",
      "34300/34300 [==============================] - 8s 230us/step - loss: 0.0043 - accuracy: 0.9986 - val_loss: 0.0323 - val_accuracy: 0.9935\n",
      "Epoch 51/100\n",
      "34300/34300 [==============================] - 8s 225us/step - loss: 0.0040 - accuracy: 0.9986 - val_loss: 0.0298 - val_accuracy: 0.9935\n",
      "Epoch 52/100\n",
      "34300/34300 [==============================] - 8s 222us/step - loss: 0.0045 - accuracy: 0.9987 - val_loss: 0.0321 - val_accuracy: 0.9931\n",
      "Epoch 53/100\n",
      "34300/34300 [==============================] - 8s 223us/step - loss: 0.0050 - accuracy: 0.9983 - val_loss: 0.0425 - val_accuracy: 0.9918\n",
      "Epoch 54/100\n",
      "34300/34300 [==============================] - 8s 222us/step - loss: 0.0042 - accuracy: 0.9987 - val_loss: 0.0299 - val_accuracy: 0.9934\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34300/34300 [==============================] - 8s 220us/step - loss: 0.0050 - accuracy: 0.9982 - val_loss: 0.0368 - val_accuracy: 0.9927\n",
      "Epoch 56/100\n",
      "34300/34300 [==============================] - 8s 220us/step - loss: 0.0051 - accuracy: 0.9981 - val_loss: 0.0351 - val_accuracy: 0.9931\n",
      "Epoch 57/100\n",
      "34300/34300 [==============================] - 8s 219us/step - loss: 0.0038 - accuracy: 0.9987 - val_loss: 0.0316 - val_accuracy: 0.9933\n",
      "Epoch 58/100\n",
      "34300/34300 [==============================] - 8s 219us/step - loss: 0.0051 - accuracy: 0.9985 - val_loss: 0.0385 - val_accuracy: 0.9918\n",
      "Epoch 59/100\n",
      "34300/34300 [==============================] - 8s 221us/step - loss: 0.0041 - accuracy: 0.9985 - val_loss: 0.0330 - val_accuracy: 0.9933\n",
      "Epoch 60/100\n",
      "34300/34300 [==============================] - 8s 221us/step - loss: 0.0037 - accuracy: 0.9990 - val_loss: 0.0352 - val_accuracy: 0.9918\n",
      "Epoch 61/100\n",
      "34300/34300 [==============================] - 8s 220us/step - loss: 0.0046 - accuracy: 0.9984 - val_loss: 0.0345 - val_accuracy: 0.9920\n",
      "Epoch 62/100\n",
      "34300/34300 [==============================] - 8s 233us/step - loss: 0.0047 - accuracy: 0.9984 - val_loss: 0.0336 - val_accuracy: 0.9921\n",
      "Epoch 63/100\n",
      "34300/34300 [==============================] - 8s 221us/step - loss: 0.0041 - accuracy: 0.9985 - val_loss: 0.0332 - val_accuracy: 0.9939\n",
      "Epoch 64/100\n",
      "34300/34300 [==============================] - 8s 226us/step - loss: 0.0037 - accuracy: 0.9987 - val_loss: 0.0290 - val_accuracy: 0.9946\n",
      "Epoch 65/100\n",
      "34300/34300 [==============================] - 8s 228us/step - loss: 0.0036 - accuracy: 0.9987 - val_loss: 0.0288 - val_accuracy: 0.9938 loss: 0.0037 - ac\n",
      "Epoch 66/100\n",
      "34300/34300 [==============================] - 8s 236us/step - loss: 0.0032 - accuracy: 0.9988 - val_loss: 0.0330 - val_accuracy: 0.9924\n",
      "Epoch 67/100\n",
      "34300/34300 [==============================] - 8s 221us/step - loss: 0.0025 - accuracy: 0.9991 - val_loss: 0.0348 - val_accuracy: 0.9930\n",
      "Epoch 68/100\n",
      "34300/34300 [==============================] - 8s 221us/step - loss: 0.0035 - accuracy: 0.9988 - val_loss: 0.0295 - val_accuracy: 0.9937\n",
      "Epoch 69/100\n",
      "34300/34300 [==============================] - 8s 222us/step - loss: 0.0036 - accuracy: 0.9989 - val_loss: 0.0320 - val_accuracy: 0.9935\n",
      "Epoch 70/100\n",
      "34300/34300 [==============================] - 8s 222us/step - loss: 0.0031 - accuracy: 0.9990 - val_loss: 0.0337 - val_accuracy: 0.9934\n",
      "Epoch 71/100\n",
      "34300/34300 [==============================] - 8s 221us/step - loss: 0.0031 - accuracy: 0.9989 - val_loss: 0.0395 - val_accuracy: 0.9920\n",
      "Epoch 72/100\n",
      "34300/34300 [==============================] - 8s 221us/step - loss: 0.0030 - accuracy: 0.9989 - val_loss: 0.0316 - val_accuracy: 0.9930\n",
      "Epoch 73/100\n",
      "34300/34300 [==============================] - 8s 221us/step - loss: 0.0051 - accuracy: 0.9985 - val_loss: 0.0338 - val_accuracy: 0.9937\n",
      "Epoch 74/100\n",
      "34300/34300 [==============================] - 8s 222us/step - loss: 0.0033 - accuracy: 0.9988 - val_loss: 0.0289 - val_accuracy: 0.9935\n",
      "Epoch 75/100\n",
      "34300/34300 [==============================] - 8s 222us/step - loss: 0.0035 - accuracy: 0.9989 - val_loss: 0.0330 - val_accuracy: 0.9931\n",
      "Epoch 76/100\n",
      "34300/34300 [==============================] - 8s 221us/step - loss: 0.0039 - accuracy: 0.9985 - val_loss: 0.0309 - val_accuracy: 0.9940\n",
      "Epoch 77/100\n",
      "34300/34300 [==============================] - 8s 222us/step - loss: 0.0026 - accuracy: 0.9991 - val_loss: 0.0337 - val_accuracy: 0.9933\n",
      "Epoch 78/100\n",
      "34300/34300 [==============================] - 8s 222us/step - loss: 0.0032 - accuracy: 0.9989 - val_loss: 0.0280 - val_accuracy: 0.9936\n",
      "Epoch 79/100\n",
      "34300/34300 [==============================] - 8s 222us/step - loss: 0.0030 - accuracy: 0.9990 - val_loss: 0.0323 - val_accuracy: 0.9936\n",
      "Epoch 80/100\n",
      "34300/34300 [==============================] - 8s 222us/step - loss: 0.0040 - accuracy: 0.9988 - val_loss: 0.0393 - val_accuracy: 0.9933\n",
      "Epoch 81/100\n",
      "34300/34300 [==============================] - 8s 222us/step - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.0304 - val_accuracy: 0.9939\n",
      "Epoch 82/100\n",
      "34300/34300 [==============================] - 8s 223us/step - loss: 0.0039 - accuracy: 0.9987 - val_loss: 0.0315 - val_accuracy: 0.9944\n",
      "Epoch 83/100\n",
      "34300/34300 [==============================] - 8s 222us/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.0306 - val_accuracy: 0.9936\n",
      "Epoch 84/100\n",
      "34300/34300 [==============================] - 8s 221us/step - loss: 0.0025 - accuracy: 0.9992 - val_loss: 0.0308 - val_accuracy: 0.9940\n",
      "Epoch 85/100\n",
      "34300/34300 [==============================] - 8s 222us/step - loss: 0.0027 - accuracy: 0.9992 - val_loss: 0.0336 - val_accuracy: 0.9929\n",
      "Epoch 86/100\n",
      "34300/34300 [==============================] - 8s 222us/step - loss: 0.0031 - accuracy: 0.9990 - val_loss: 0.0399 - val_accuracy: 0.9930\n",
      "Epoch 87/100\n",
      "34300/34300 [==============================] - 8s 222us/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.0337 - val_accuracy: 0.9936\n",
      "Epoch 88/100\n",
      "34300/34300 [==============================] - 8s 222us/step - loss: 0.0027 - accuracy: 0.9989 - val_loss: 0.0352 - val_accuracy: 0.9934\n",
      "Epoch 89/100\n",
      "34300/34300 [==============================] - 8s 222us/step - loss: 0.0047 - accuracy: 0.9983 - val_loss: 0.0478 - val_accuracy: 0.9909\n",
      "Epoch 90/100\n",
      "34300/34300 [==============================] - 8s 222us/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.0350 - val_accuracy: 0.9931\n",
      "Epoch 91/100\n",
      "34300/34300 [==============================] - 8s 222us/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.0366 - val_accuracy: 0.9928\n",
      "Epoch 92/100\n",
      "34300/34300 [==============================] - 8s 223us/step - loss: 0.0032 - accuracy: 0.9990 - val_loss: 0.0315 - val_accuracy: 0.9946\n",
      "Epoch 93/100\n",
      "34300/34300 [==============================] - 8s 226us/step - loss: 0.0030 - accuracy: 0.9991 - val_loss: 0.0340 - val_accuracy: 0.9930\n",
      "Epoch 94/100\n",
      "34300/34300 [==============================] - 8s 227us/step - loss: 0.0027 - accuracy: 0.9989 - val_loss: 0.0318 - val_accuracy: 0.9934\n",
      "Epoch 95/100\n",
      "34300/34300 [==============================] - 8s 226us/step - loss: 0.0031 - accuracy: 0.9992 - val_loss: 0.0348 - val_accuracy: 0.9940 loss: 0.0\n",
      "Epoch 96/100\n",
      "34300/34300 [==============================] - 8s 222us/step - loss: 0.0027 - accuracy: 0.9990 - val_loss: 0.0339 - val_accuracy: 0.9933\n",
      "Epoch 97/100\n",
      "34300/34300 [==============================] - 8s 224us/step - loss: 0.0034 - accuracy: 0.9990 - val_loss: 0.0353 - val_accuracy: 0.9935\n",
      "Epoch 98/100\n",
      "34300/34300 [==============================] - 8s 232us/step - loss: 0.0022 - accuracy: 0.9992 - val_loss: 0.0346 - val_accuracy: 0.9934\n",
      "Epoch 99/100\n",
      "34300/34300 [==============================] - 8s 225us/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.0456 - val_accuracy: 0.9919\n",
      "Epoch 100/100\n",
      "34300/34300 [==============================] - 8s 220us/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 0.0312 - val_accuracy: 0.9939\n",
      "Wall time: 12min 46s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x213bec79d88>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(X_train, y_train, batch_size=128, epochs=100, verbose=1,\n",
    "         validation_split = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(X_test_sub)\n",
    "y_pred = predicted.argmax(axis=1)\n",
    "\n",
    "Out = pd.DataFrame([test['filename'],y_pred]).T\n",
    "Out.rename(columns = {0:'filename', 1:'label'})\n",
    "#Out\n",
    "Out.to_csv('MNIST_DropOut_2LayerCNN_02he_normal_Adam11.csv', header =  ['filename', 'label' ], index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_cnn_batch_03():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3,3), kernel_initializer='he_normal', input_shape=(28,28,3), strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(64, (3,3), kernel_initializer='he_normal', strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(2,2))    \n",
    "    \n",
    "    model.add(Conv2D(32, (3,3), kernel_initializer='he_normal', input_shape=(28,28,3), strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(64, (3,3), kernel_initializer='he_normal', strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(2,2)) \n",
    "    \n",
    "    model.add(Conv2D(32, (3,3), kernel_initializer='he_normal', input_shape=(28,28,3), strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(64, (3,3), kernel_initializer='he_normal', strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(2,2)) \n",
    "    \n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(128, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(64, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    adam = tf.keras.optimizers.Adam(lr = 0.001) \n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model_cnn_batch_03()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34300 samples, validate on 14700 samples\n",
      "Epoch 1/100\n",
      "34300/34300 [==============================] - 10s 290us/step - loss: 0.8151 - accuracy: 0.7638 - val_loss: 0.1313 - val_accuracy: 0.9653\n",
      "Epoch 2/100\n",
      "34300/34300 [==============================] - 8s 240us/step - loss: 0.2035 - accuracy: 0.9517 - val_loss: 0.1234 - val_accuracy: 0.9628\n",
      "Epoch 3/100\n",
      "34300/34300 [==============================] - 8s 241us/step - loss: 0.1265 - accuracy: 0.9694 - val_loss: 0.0751 - val_accuracy: 0.9773\n",
      "Epoch 4/100\n",
      "34300/34300 [==============================] - 8s 241us/step - loss: 0.0946 - accuracy: 0.9769 - val_loss: 0.0519 - val_accuracy: 0.9854\n",
      "Epoch 5/100\n",
      "34300/34300 [==============================] - 8s 241us/step - loss: 0.0771 - accuracy: 0.9813 - val_loss: 0.0440 - val_accuracy: 0.9875\n",
      "Epoch 6/100\n",
      "34300/34300 [==============================] - 8s 242us/step - loss: 0.0642 - accuracy: 0.9836 - val_loss: 0.0520 - val_accuracy: 0.9858\n",
      "Epoch 7/100\n",
      "34300/34300 [==============================] - 8s 242us/step - loss: 0.0550 - accuracy: 0.9859 - val_loss: 0.0498 - val_accuracy: 0.9852\n",
      "Epoch 8/100\n",
      "34300/34300 [==============================] - 8s 242us/step - loss: 0.0518 - accuracy: 0.9863 - val_loss: 0.0487 - val_accuracy: 0.9861\n",
      "Epoch 9/100\n",
      "34300/34300 [==============================] - 8s 243us/step - loss: 0.0466 - accuracy: 0.9892 - val_loss: 0.0384 - val_accuracy: 0.9901\n",
      "Epoch 10/100\n",
      "34300/34300 [==============================] - 9s 251us/step - loss: 0.0405 - accuracy: 0.9900 - val_loss: 0.0306 - val_accuracy: 0.9926\n",
      "Epoch 11/100\n",
      "34300/34300 [==============================] - 9s 249us/step - loss: 0.0357 - accuracy: 0.9909 - val_loss: 0.0796 - val_accuracy: 0.9790\n",
      "Epoch 12/100\n",
      "34300/34300 [==============================] - 8s 244us/step - loss: 0.0403 - accuracy: 0.9897 - val_loss: 0.0337 - val_accuracy: 0.9915\n",
      "Epoch 13/100\n",
      "34300/34300 [==============================] - 8s 246us/step - loss: 0.0303 - accuracy: 0.9929 - val_loss: 0.0577 - val_accuracy: 0.9855\n",
      "Epoch 14/100\n",
      "34300/34300 [==============================] - 9s 253us/step - loss: 0.0325 - accuracy: 0.9920 - val_loss: 0.0431 - val_accuracy: 0.9892\n",
      "Epoch 15/100\n",
      "34300/34300 [==============================] - 9s 253us/step - loss: 0.0288 - accuracy: 0.9934 - val_loss: 0.0342 - val_accuracy: 0.9917\n",
      "Epoch 16/100\n",
      "34300/34300 [==============================] - 9s 248us/step - loss: 0.0282 - accuracy: 0.9927 - val_loss: 0.0457 - val_accuracy: 0.9897\n",
      "Epoch 17/100\n",
      "34300/34300 [==============================] - 8s 247us/step - loss: 0.0266 - accuracy: 0.9934 - val_loss: 0.0361 - val_accuracy: 0.9909\n",
      "Epoch 18/100\n",
      "34300/34300 [==============================] - 9s 252us/step - loss: 0.0275 - accuracy: 0.9928 - val_loss: 0.0409 - val_accuracy: 0.9912\n",
      "Epoch 19/100\n",
      "34300/34300 [==============================] - 9s 253us/step - loss: 0.0227 - accuracy: 0.9943 - val_loss: 0.0350 - val_accuracy: 0.9922\n",
      "Epoch 20/100\n",
      "34300/34300 [==============================] - 9s 258us/step - loss: 0.0233 - accuracy: 0.9947 - val_loss: 0.0324 - val_accuracy: 0.9927\n",
      "Epoch 21/100\n",
      "34300/34300 [==============================] - 9s 255us/step - loss: 0.0180 - accuracy: 0.9955 - val_loss: 0.0361 - val_accuracy: 0.9926\n",
      "Epoch 22/100\n",
      "34300/34300 [==============================] - 9s 256us/step - loss: 0.0160 - accuracy: 0.9957 - val_loss: 0.0399 - val_accuracy: 0.9907\n",
      "Epoch 23/100\n",
      "34300/34300 [==============================] - 8s 245us/step - loss: 0.0169 - accuracy: 0.9956 - val_loss: 0.0575 - val_accuracy: 0.9882\n",
      "Epoch 24/100\n",
      "34300/34300 [==============================] - 8s 246us/step - loss: 0.0194 - accuracy: 0.9951 - val_loss: 0.0579 - val_accuracy: 0.9876\n",
      "Epoch 25/100\n",
      "34300/34300 [==============================] - 9s 249us/step - loss: 0.0178 - accuracy: 0.9956 - val_loss: 0.0401 - val_accuracy: 0.9928\n",
      "Epoch 26/100\n",
      "34300/34300 [==============================] - 9s 248us/step - loss: 0.0214 - accuracy: 0.9945 - val_loss: 0.0648 - val_accuracy: 0.9876\n",
      "Epoch 27/100\n",
      "34300/34300 [==============================] - 9s 254us/step - loss: 0.0165 - accuracy: 0.9956 - val_loss: 0.0354 - val_accuracy: 0.9927\n",
      "Epoch 28/100\n",
      "34300/34300 [==============================] - 8s 245us/step - loss: 0.0129 - accuracy: 0.9966 - val_loss: 0.0480 - val_accuracy: 0.9905\n",
      "Epoch 29/100\n",
      "34300/34300 [==============================] - 9s 253us/step - loss: 0.0157 - accuracy: 0.9958 - val_loss: 0.0360 - val_accuracy: 0.9933\n",
      "Epoch 30/100\n",
      "34300/34300 [==============================] - 9s 253us/step - loss: 0.0157 - accuracy: 0.9956 - val_loss: 0.0867 - val_accuracy: 0.9833\n",
      "Epoch 31/100\n",
      "34300/34300 [==============================] - 9s 252us/step - loss: 0.0161 - accuracy: 0.9952 - val_loss: 0.0460 - val_accuracy: 0.9914\n",
      "Epoch 32/100\n",
      "34300/34300 [==============================] - 8s 246us/step - loss: 0.0126 - accuracy: 0.9969 - val_loss: 0.0434 - val_accuracy: 0.9917\n",
      "Epoch 33/100\n",
      "34300/34300 [==============================] - 9s 253us/step - loss: 0.0116 - accuracy: 0.9971 - val_loss: 0.0543 - val_accuracy: 0.9894\n",
      "Epoch 34/100\n",
      "34300/34300 [==============================] - 9s 255us/step - loss: 0.0131 - accuracy: 0.9965 - val_loss: 0.0380 - val_accuracy: 0.9929\n",
      "Epoch 35/100\n",
      "34300/34300 [==============================] - 9s 276us/step - loss: 0.0125 - accuracy: 0.9965 - val_loss: 0.0386 - val_accuracy: 0.9930\n",
      "Epoch 36/100\n",
      "34300/34300 [==============================] - 9s 269us/step - loss: 0.0113 - accuracy: 0.9968 - val_loss: 0.0461 - val_accuracy: 0.9908\n",
      "Epoch 37/100\n",
      "34300/34300 [==============================] - 9s 250us/step - loss: 0.0098 - accuracy: 0.9978 - val_loss: 0.0374 - val_accuracy: 0.9933\n",
      "Epoch 38/100\n",
      "34300/34300 [==============================] - 9s 256us/step - loss: 0.0138 - accuracy: 0.9967 - val_loss: 0.0529 - val_accuracy: 0.9904\n",
      "Epoch 39/100\n",
      "34300/34300 [==============================] - 9s 264us/step - loss: 0.0149 - accuracy: 0.9963 - val_loss: 0.0444 - val_accuracy: 0.9929\n",
      "Epoch 40/100\n",
      "34300/34300 [==============================] - 9s 257us/step - loss: 0.0089 - accuracy: 0.9980 - val_loss: 0.0432 - val_accuracy: 0.9921\n",
      "Epoch 41/100\n",
      "34300/34300 [==============================] - 9s 256us/step - loss: 0.0097 - accuracy: 0.9973 - val_loss: 0.0446 - val_accuracy: 0.9929\n",
      "Epoch 42/100\n",
      "34300/34300 [==============================] - 8s 247us/step - loss: 0.0085 - accuracy: 0.9979 - val_loss: 0.0423 - val_accuracy: 0.9923\n",
      "Epoch 43/100\n",
      "34300/34300 [==============================] - 8s 246us/step - loss: 0.0111 - accuracy: 0.9972 - val_loss: 0.0422 - val_accuracy: 0.9926\n",
      "Epoch 44/100\n",
      "34300/34300 [==============================] - 8s 243us/step - loss: 0.0097 - accuracy: 0.9976 - val_loss: 0.0401 - val_accuracy: 0.9925\n",
      "Epoch 45/100\n",
      "34300/34300 [==============================] - 9s 254us/step - loss: 0.0112 - accuracy: 0.9971 - val_loss: 0.0518 - val_accuracy: 0.9911\n",
      "Epoch 46/100\n",
      "34300/34300 [==============================] - 9s 250us/step - loss: 0.0127 - accuracy: 0.9969 - val_loss: 0.0443 - val_accuracy: 0.9925\n",
      "Epoch 47/100\n",
      "34300/34300 [==============================] - 9s 252us/step - loss: 0.0080 - accuracy: 0.9979 - val_loss: 0.0493 - val_accuracy: 0.9914\n",
      "Epoch 48/100\n",
      "34300/34300 [==============================] - 9s 265us/step - loss: 0.0073 - accuracy: 0.9983 - val_loss: 0.0414 - val_accuracy: 0.9929\n",
      "Epoch 49/100\n",
      "34300/34300 [==============================] - 9s 262us/step - loss: 0.0118 - accuracy: 0.9971 - val_loss: 0.0429 - val_accuracy: 0.9930\n",
      "Epoch 50/100\n",
      "34300/34300 [==============================] - 8s 246us/step - loss: 0.0102 - accuracy: 0.9975 - val_loss: 0.0523 - val_accuracy: 0.9918\n",
      "Epoch 51/100\n",
      "34300/34300 [==============================] - 9s 250us/step - loss: 0.0075 - accuracy: 0.9980 - val_loss: 0.0587 - val_accuracy: 0.9915\n",
      "Epoch 52/100\n",
      "34300/34300 [==============================] - 8s 247us/step - loss: 0.0098 - accuracy: 0.9978 - val_loss: 0.0470 - val_accuracy: 0.9914\n",
      "Epoch 53/100\n",
      "34300/34300 [==============================] - 8s 244us/step - loss: 0.0090 - accuracy: 0.9978 - val_loss: 0.0440 - val_accuracy: 0.9931\n",
      "Epoch 54/100\n",
      "34300/34300 [==============================] - 8s 243us/step - loss: 0.0069 - accuracy: 0.9983 - val_loss: 0.0518 - val_accuracy: 0.9903\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34300/34300 [==============================] - 10s 294us/step - loss: 0.0098 - accuracy: 0.9977 - val_loss: 0.0383 - val_accuracy: 0.9939\n",
      "Epoch 56/100\n",
      "34300/34300 [==============================] - 8s 243us/step - loss: 0.0110 - accuracy: 0.9976 - val_loss: 0.0367 - val_accuracy: 0.9942\n",
      "Epoch 57/100\n",
      "34300/34300 [==============================] - 8s 245us/step - loss: 0.0058 - accuracy: 0.9984 - val_loss: 0.0502 - val_accuracy: 0.9922\n",
      "Epoch 58/100\n",
      "34300/34300 [==============================] - 8s 243us/step - loss: 0.0101 - accuracy: 0.9977 - val_loss: 0.0408 - val_accuracy: 0.9931\n",
      "Epoch 59/100\n",
      "34300/34300 [==============================] - 8s 245us/step - loss: 0.0048 - accuracy: 0.9988 - val_loss: 0.0403 - val_accuracy: 0.9941\n",
      "Epoch 60/100\n",
      "34300/34300 [==============================] - 8s 242us/step - loss: 0.0079 - accuracy: 0.9980 - val_loss: 0.0451 - val_accuracy: 0.9937\n",
      "Epoch 61/100\n",
      "34300/34300 [==============================] - 8s 243us/step - loss: 0.0067 - accuracy: 0.9982 - val_loss: 0.0487 - val_accuracy: 0.9927\n",
      "Epoch 62/100\n",
      "34300/34300 [==============================] - 8s 243us/step - loss: 0.0043 - accuracy: 0.9989 - val_loss: 0.0398 - val_accuracy: 0.9937\n",
      "Epoch 63/100\n",
      "34300/34300 [==============================] - 9s 258us/step - loss: 0.0103 - accuracy: 0.9973 - val_loss: 0.0500 - val_accuracy: 0.9912\n",
      "Epoch 64/100\n",
      "34300/34300 [==============================] - 9s 250us/step - loss: 0.0089 - accuracy: 0.9978 - val_loss: 0.0425 - val_accuracy: 0.9936\n",
      "Epoch 65/100\n",
      "34300/34300 [==============================] - 9s 251us/step - loss: 0.0067 - accuracy: 0.9984 - val_loss: 0.0397 - val_accuracy: 0.9941\n",
      "Epoch 66/100\n",
      "34300/34300 [==============================] - 9s 259us/step - loss: 0.0050 - accuracy: 0.9985 - val_loss: 0.0499 - val_accuracy: 0.9922\n",
      "Epoch 67/100\n",
      "34300/34300 [==============================] - 9s 249us/step - loss: 0.0046 - accuracy: 0.9985 - val_loss: 0.0450 - val_accuracy: 0.9935\n",
      "Epoch 68/100\n",
      "34300/34300 [==============================] - 9s 249us/step - loss: 0.0078 - accuracy: 0.9980 - val_loss: 0.0374 - val_accuracy: 0.9941\n",
      "Epoch 69/100\n",
      "34300/34300 [==============================] - 9s 268us/step - loss: 0.0071 - accuracy: 0.9980 - val_loss: 0.0481 - val_accuracy: 0.9926\n",
      "Epoch 70/100\n",
      "34300/34300 [==============================] - 9s 261us/step - loss: 0.0080 - accuracy: 0.9983 - val_loss: 0.0471 - val_accuracy: 0.9923\n",
      "Epoch 71/100\n",
      "34300/34300 [==============================] - 9s 268us/step - loss: 0.0070 - accuracy: 0.9982 - val_loss: 0.0463 - val_accuracy: 0.9922\n",
      "Epoch 72/100\n",
      "34300/34300 [==============================] - 9s 262us/step - loss: 0.0042 - accuracy: 0.9988 - val_loss: 0.0467 - val_accuracy: 0.9939\n",
      "Epoch 73/100\n",
      "34300/34300 [==============================] - 9s 258us/step - loss: 0.0040 - accuracy: 0.9992 - val_loss: 0.0532 - val_accuracy: 0.9927\n",
      "Epoch 74/100\n",
      "34300/34300 [==============================] - 9s 262us/step - loss: 0.0080 - accuracy: 0.9978 - val_loss: 0.0565 - val_accuracy: 0.9918\n",
      "Epoch 75/100\n",
      "34300/34300 [==============================] - 9s 249us/step - loss: 0.0081 - accuracy: 0.9980 - val_loss: 0.0405 - val_accuracy: 0.9939\n",
      "Epoch 76/100\n",
      "34300/34300 [==============================] - 9s 252us/step - loss: 0.0068 - accuracy: 0.9983 - val_loss: 0.0513 - val_accuracy: 0.9923\n",
      "Epoch 77/100\n",
      "34300/34300 [==============================] - 9s 258us/step - loss: 0.0050 - accuracy: 0.9986 - val_loss: 0.0461 - val_accuracy: 0.9929\n",
      "Epoch 78/100\n",
      "34300/34300 [==============================] - 9s 254us/step - loss: 0.0065 - accuracy: 0.9983 - val_loss: 0.0600 - val_accuracy: 0.9917\n",
      "Epoch 79/100\n",
      "34300/34300 [==============================] - 9s 254us/step - loss: 0.0056 - accuracy: 0.9987 - val_loss: 0.0443 - val_accuracy: 0.9928\n",
      "Epoch 80/100\n",
      "34300/34300 [==============================] - 9s 250us/step - loss: 0.0053 - accuracy: 0.9985 - val_loss: 0.0518 - val_accuracy: 0.9931\n",
      "Epoch 81/100\n",
      "34300/34300 [==============================] - 9s 249us/step - loss: 0.0036 - accuracy: 0.9990 - val_loss: 0.0423 - val_accuracy: 0.9948\n",
      "Epoch 82/100\n",
      "34300/34300 [==============================] - 9s 248us/step - loss: 0.0040 - accuracy: 0.9989 - val_loss: 0.0486 - val_accuracy: 0.9933\n",
      "Epoch 83/100\n",
      "34300/34300 [==============================] - 9s 249us/step - loss: 0.0063 - accuracy: 0.9984 - val_loss: 0.0596 - val_accuracy: 0.9922\n",
      "Epoch 84/100\n",
      "34300/34300 [==============================] - 9s 249us/step - loss: 0.0077 - accuracy: 0.9980 - val_loss: 0.0597 - val_accuracy: 0.9919\n",
      "Epoch 85/100\n",
      "34300/34300 [==============================] - 9s 249us/step - loss: 0.0046 - accuracy: 0.9989 - val_loss: 0.0465 - val_accuracy: 0.9935\n",
      "Epoch 86/100\n",
      "34300/34300 [==============================] - 9s 250us/step - loss: 0.0050 - accuracy: 0.9987 - val_loss: 0.0625 - val_accuracy: 0.9921\n",
      "Epoch 87/100\n",
      "34300/34300 [==============================] - 9s 264us/step - loss: 0.0037 - accuracy: 0.9991 - val_loss: 0.0498 - val_accuracy: 0.9941\n",
      "Epoch 88/100\n",
      "34300/34300 [==============================] - 9s 266us/step - loss: 0.0050 - accuracy: 0.9985 - val_loss: 0.0521 - val_accuracy: 0.9929\n",
      "Epoch 89/100\n",
      "34300/34300 [==============================] - 9s 249us/step - loss: 0.0050 - accuracy: 0.9987 - val_loss: 0.0517 - val_accuracy: 0.9932\n",
      "Epoch 90/100\n",
      "34300/34300 [==============================] - 9s 250us/step - loss: 0.0070 - accuracy: 0.9983 - val_loss: 0.0534 - val_accuracy: 0.9927\n",
      "Epoch 91/100\n",
      "34300/34300 [==============================] - 9s 253us/step - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.0475 - val_accuracy: 0.9930\n",
      "Epoch 92/100\n",
      "34300/34300 [==============================] - 9s 257us/step - loss: 0.0058 - accuracy: 0.9986 - val_loss: 0.0488 - val_accuracy: 0.9926\n",
      "Epoch 93/100\n",
      "34300/34300 [==============================] - 9s 249us/step - loss: 0.0027 - accuracy: 0.9992 - val_loss: 0.0492 - val_accuracy: 0.9935\n",
      "Epoch 94/100\n",
      "34300/34300 [==============================] - 8s 248us/step - loss: 0.0034 - accuracy: 0.9994 - val_loss: 0.0494 - val_accuracy: 0.9935\n",
      "Epoch 95/100\n",
      "34300/34300 [==============================] - 8s 248us/step - loss: 0.0057 - accuracy: 0.9986 - val_loss: 0.0609 - val_accuracy: 0.9910\n",
      "Epoch 96/100\n",
      "34300/34300 [==============================] - 8s 247us/step - loss: 0.0072 - accuracy: 0.9984 - val_loss: 0.0529 - val_accuracy: 0.9921\n",
      "Epoch 97/100\n",
      "34300/34300 [==============================] - 9s 254us/step - loss: 0.0045 - accuracy: 0.9988 - val_loss: 0.0421 - val_accuracy: 0.9934\n",
      "Epoch 98/100\n",
      "34300/34300 [==============================] - 9s 267us/step - loss: 0.0036 - accuracy: 0.9990 - val_loss: 0.0492 - val_accuracy: 0.9939\n",
      "Epoch 99/100\n",
      "34300/34300 [==============================] - 9s 268us/step - loss: 0.0033 - accuracy: 0.9990 - val_loss: 0.0479 - val_accuracy: 0.9937\n",
      "Epoch 100/100\n",
      "34300/34300 [==============================] - 9s 256us/step - loss: 0.0040 - accuracy: 0.9990 - val_loss: 0.0453 - val_accuracy: 0.9937\n",
      "Wall time: 14min 28s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x213d7a48b88>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(X_train, y_train, batch_size=128, epochs=100, verbose=1,\n",
    "         validation_split = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(X_test_sub)\n",
    "y_pred = predicted.argmax(axis=1)\n",
    "\n",
    "Out = pd.DataFrame([test['filename'],y_pred]).T\n",
    "Out.rename(columns = {0:'filename', 1:'label'})\n",
    "#Out\n",
    "Out.to_csv('MNIST_DropOut_2LayerCNN_02he_normal_Adam13.csv', header =  ['filename', 'label' ], index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_cnn_batch_04():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3,3), kernel_initializer='he_normal', input_shape=(28,28,3), strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(64, (3,3), kernel_initializer='he_normal', strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(2,2))    \n",
    "    \n",
    "    model.add(Conv2D(32, (3,3), kernel_initializer='he_normal', input_shape=(28,28,3), strides = (2,2), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(64, (3,3), kernel_initializer='he_normal', strides = (2,2), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(2,2)) \n",
    "    \n",
    "    model.add(Conv2D(32, (3,3), kernel_initializer='he_normal', input_shape=(28,28,3), strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(64, (3,3), kernel_initializer='he_normal', strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(2,2)) \n",
    "    \n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(128, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(64, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))    \n",
    "        \n",
    "    model.add(Dense(32, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    adam = tf.keras.optimizers.Adam(lr = 0.001) \n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model_cnn_batch_04()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34300 samples, validate on 14700 samples\n",
      "Epoch 1/100\n",
      "34300/34300 [==============================] - 9s 271us/step - loss: 1.9887 - accuracy: 0.3116 - val_loss: 1.1212 - val_accuracy: 0.7563\n",
      "Epoch 2/100\n",
      "34300/34300 [==============================] - 7s 217us/step - loss: 1.0336 - accuracy: 0.6706 - val_loss: 0.3087 - val_accuracy: 0.9493\n",
      "Epoch 3/100\n",
      "34300/34300 [==============================] - 7s 217us/step - loss: 0.5860 - accuracy: 0.8297 - val_loss: 0.1516 - val_accuracy: 0.9648\n",
      "Epoch 4/100\n",
      "34300/34300 [==============================] - 8s 219us/step - loss: 0.3919 - accuracy: 0.8952 - val_loss: 0.1394 - val_accuracy: 0.9652\n",
      "Epoch 5/100\n",
      "34300/34300 [==============================] - 7s 218us/step - loss: 0.2965 - accuracy: 0.9232 - val_loss: 0.0912 - val_accuracy: 0.9790\n",
      "Epoch 6/100\n",
      "34300/34300 [==============================] - 8s 232us/step - loss: 0.2442 - accuracy: 0.9363 - val_loss: 0.1080 - val_accuracy: 0.9752\n",
      "Epoch 7/100\n",
      "34300/34300 [==============================] - 8s 241us/step - loss: 0.2057 - accuracy: 0.9478 - val_loss: 0.0885 - val_accuracy: 0.9810\n",
      "Epoch 8/100\n",
      "34300/34300 [==============================] - 8s 231us/step - loss: 0.1885 - accuracy: 0.9540 - val_loss: 0.0964 - val_accuracy: 0.9790\n",
      "Epoch 9/100\n",
      "34300/34300 [==============================] - 8s 229us/step - loss: 0.1681 - accuracy: 0.9586 - val_loss: 0.0711 - val_accuracy: 0.9854\n",
      "Epoch 10/100\n",
      "34300/34300 [==============================] - 8s 224us/step - loss: 0.1507 - accuracy: 0.9618 - val_loss: 0.0825 - val_accuracy: 0.9839 - loss: 0.1517 - \n",
      "Epoch 11/100\n",
      "34300/34300 [==============================] - 8s 224us/step - loss: 0.1367 - accuracy: 0.9645 - val_loss: 0.0714 - val_accuracy: 0.9852\n",
      "Epoch 12/100\n",
      "34300/34300 [==============================] - 8s 223us/step - loss: 0.1358 - accuracy: 0.9646 - val_loss: 0.0817 - val_accuracy: 0.9833\n",
      "Epoch 13/100\n",
      "34300/34300 [==============================] - 8s 229us/step - loss: 0.1224 - accuracy: 0.9680 - val_loss: 0.0707 - val_accuracy: 0.9874\n",
      "Epoch 14/100\n",
      "34300/34300 [==============================] - 8s 223us/step - loss: 0.1101 - accuracy: 0.9695 - val_loss: 0.0838 - val_accuracy: 0.9860\n",
      "Epoch 15/100\n",
      "34300/34300 [==============================] - 8s 225us/step - loss: 0.1079 - accuracy: 0.9718 - val_loss: 0.1009 - val_accuracy: 0.9812\n",
      "Epoch 16/100\n",
      "34300/34300 [==============================] - 8s 223us/step - loss: 0.1048 - accuracy: 0.9728 - val_loss: 0.0671 - val_accuracy: 0.9879\n",
      "Epoch 17/100\n",
      "34300/34300 [==============================] - 8s 225us/step - loss: 0.0951 - accuracy: 0.9732 - val_loss: 0.0722 - val_accuracy: 0.9876\n",
      "Epoch 18/100\n",
      "34300/34300 [==============================] - 8s 224us/step - loss: 0.0933 - accuracy: 0.9743 - val_loss: 0.1506 - val_accuracy: 0.9780\n",
      "Epoch 19/100\n",
      "34300/34300 [==============================] - 8s 224us/step - loss: 0.0966 - accuracy: 0.9728 - val_loss: 0.0893 - val_accuracy: 0.9875\n",
      "Epoch 20/100\n",
      "34300/34300 [==============================] - 8s 226us/step - loss: 0.0863 - accuracy: 0.9766 - val_loss: 0.0853 - val_accuracy: 0.9873cura - ETA: 0s - loss: 0.0887 - accuracy:  - ETA: 0s - loss: 0\n",
      "Epoch 21/100\n",
      "34300/34300 [==============================] - 8s 225us/step - loss: 0.0846 - accuracy: 0.9764 - val_loss: 0.0792 - val_accuracy: 0.9871\n",
      "Epoch 22/100\n",
      "34300/34300 [==============================] - 8s 225us/step - loss: 0.0891 - accuracy: 0.9758 - val_loss: 0.0876 - val_accuracy: 0.9863\n",
      "Epoch 23/100\n",
      "34300/34300 [==============================] - 8s 225us/step - loss: 0.0824 - accuracy: 0.9774 - val_loss: 0.1459 - val_accuracy: 0.9771\n",
      "Epoch 24/100\n",
      "34300/34300 [==============================] - 8s 227us/step - loss: 0.0758 - accuracy: 0.9773 - val_loss: 0.0774 - val_accuracy: 0.9873\n",
      "Epoch 25/100\n",
      "34300/34300 [==============================] - 8s 225us/step - loss: 0.0735 - accuracy: 0.9791 - val_loss: 0.0941 - val_accuracy: 0.9880\n",
      "Epoch 26/100\n",
      "34300/34300 [==============================] - 8s 225us/step - loss: 0.0731 - accuracy: 0.9796 - val_loss: 0.0928 - val_accuracy: 0.9880\n",
      "Epoch 27/100\n",
      "34300/34300 [==============================] - 8s 226us/step - loss: 0.0722 - accuracy: 0.9789 - val_loss: 0.0816 - val_accuracy: 0.9886\n",
      "Epoch 28/100\n",
      "34300/34300 [==============================] - 8s 226us/step - loss: 0.0720 - accuracy: 0.9795 - val_loss: 0.1696 - val_accuracy: 0.9794\n",
      "Epoch 29/100\n",
      "34300/34300 [==============================] - 8s 226us/step - loss: 0.0799 - accuracy: 0.9780 - val_loss: 0.0880 - val_accuracy: 0.9886\n",
      "Epoch 30/100\n",
      "34300/34300 [==============================] - 8s 226us/step - loss: 0.0692 - accuracy: 0.9798 - val_loss: 0.0879 - val_accuracy: 0.9859\n",
      "Epoch 31/100\n",
      "34300/34300 [==============================] - 8s 226us/step - loss: 0.0694 - accuracy: 0.9813 - val_loss: 0.1482 - val_accuracy: 0.9808 loss: 0.0\n",
      "Epoch 32/100\n",
      "34300/34300 [==============================] - 8s 225us/step - loss: 0.0595 - accuracy: 0.9820 - val_loss: 0.0878 - val_accuracy: 0.9880\n",
      "Epoch 33/100\n",
      "34300/34300 [==============================] - 8s 225us/step - loss: 0.0688 - accuracy: 0.9813 - val_loss: 0.1023 - val_accuracy: 0.9871\n",
      "Epoch 34/100\n",
      "34300/34300 [==============================] - 8s 226us/step - loss: 0.0666 - accuracy: 0.9807 - val_loss: 0.0963 - val_accuracy: 0.9876\n",
      "Epoch 35/100\n",
      "34300/34300 [==============================] - 8s 226us/step - loss: 0.0651 - accuracy: 0.9812 - val_loss: 0.0914 - val_accuracy: 0.9897\n",
      "Epoch 36/100\n",
      "34300/34300 [==============================] - 8s 225us/step - loss: 0.0617 - accuracy: 0.9820 - val_loss: 0.0731 - val_accuracy: 0.9907\n",
      "Epoch 37/100\n",
      "34300/34300 [==============================] - 8s 224us/step - loss: 0.0514 - accuracy: 0.9837 - val_loss: 0.1058 - val_accuracy: 0.9880\n",
      "Epoch 38/100\n",
      "34300/34300 [==============================] - 8s 224us/step - loss: 0.0585 - accuracy: 0.9825 - val_loss: 0.0874 - val_accuracy: 0.9907\n",
      "Epoch 39/100\n",
      "34300/34300 [==============================] - 8s 223us/step - loss: 0.0575 - accuracy: 0.9831 - val_loss: 0.1021 - val_accuracy: 0.9874\n",
      "Epoch 40/100\n",
      "34300/34300 [==============================] - 8s 238us/step - loss: 0.0643 - accuracy: 0.9817 - val_loss: 0.0976 - val_accuracy: 0.9879\n",
      "Epoch 41/100\n",
      "34300/34300 [==============================] - 8s 236us/step - loss: 0.0684 - accuracy: 0.9812 - val_loss: 0.0837 - val_accuracy: 0.9894\n",
      "Epoch 42/100\n",
      "34300/34300 [==============================] - 8s 228us/step - loss: 0.0493 - accuracy: 0.9838 - val_loss: 0.0890 - val_accuracy: 0.9888\n",
      "Epoch 43/100\n",
      "34300/34300 [==============================] - 8s 228us/step - loss: 0.0584 - accuracy: 0.9819 - val_loss: 0.0859 - val_accuracy: 0.9896\n",
      "Epoch 44/100\n",
      "34300/34300 [==============================] - 8s 227us/step - loss: 0.0491 - accuracy: 0.9843 - val_loss: 0.0799 - val_accuracy: 0.9899\n",
      "Epoch 45/100\n",
      "34300/34300 [==============================] - 8s 227us/step - loss: 0.0498 - accuracy: 0.9847 - val_loss: 0.1032 - val_accuracy: 0.9882\n",
      "Epoch 46/100\n",
      "34300/34300 [==============================] - 8s 227us/step - loss: 0.0522 - accuracy: 0.9843 - val_loss: 0.1162 - val_accuracy: 0.9856\n",
      "Epoch 47/100\n",
      "34300/34300 [==============================] - 8s 227us/step - loss: 0.0513 - accuracy: 0.9840 - val_loss: 0.1631 - val_accuracy: 0.9829 - loss: 0.0513 - accuracy: \n",
      "Epoch 48/100\n",
      "34300/34300 [==============================] - 8s 228us/step - loss: 0.0622 - accuracy: 0.9825 - val_loss: 0.0759 - val_accuracy: 0.9907\n",
      "Epoch 49/100\n",
      "34300/34300 [==============================] - 8s 228us/step - loss: 0.0536 - accuracy: 0.9839 - val_loss: 0.0895 - val_accuracy: 0.9885\n",
      "Epoch 50/100\n",
      "34300/34300 [==============================] - 8s 228us/step - loss: 0.0500 - accuracy: 0.9840 - val_loss: 0.0971 - val_accuracy: 0.9899\n",
      "Epoch 51/100\n",
      "34300/34300 [==============================] - 8s 227us/step - loss: 0.0508 - accuracy: 0.9850 - val_loss: 0.0834 - val_accuracy: 0.9906\n",
      "Epoch 52/100\n",
      "34300/34300 [==============================] - 8s 226us/step - loss: 0.0502 - accuracy: 0.9846 - val_loss: 0.1044 - val_accuracy: 0.9875\n",
      "Epoch 53/100\n",
      "34300/34300 [==============================] - 8s 227us/step - loss: 0.0551 - accuracy: 0.9841 - val_loss: 0.0802 - val_accuracy: 0.9907\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34300/34300 [==============================] - 8s 224us/step - loss: 0.0505 - accuracy: 0.9839 - val_loss: 0.0879 - val_accuracy: 0.9907\n",
      "Epoch 55/100\n",
      "34300/34300 [==============================] - 8s 223us/step - loss: 0.0525 - accuracy: 0.9841 - val_loss: 0.0956 - val_accuracy: 0.9899\n",
      "Epoch 56/100\n",
      "34300/34300 [==============================] - 8s 228us/step - loss: 0.0443 - accuracy: 0.9851 - val_loss: 0.0957 - val_accuracy: 0.9901\n",
      "Epoch 57/100\n",
      "34300/34300 [==============================] - 8s 224us/step - loss: 0.0491 - accuracy: 0.9849 - val_loss: 0.0991 - val_accuracy: 0.9882\n",
      "Epoch 58/100\n",
      "34300/34300 [==============================] - 8s 227us/step - loss: 0.0439 - accuracy: 0.9865 - val_loss: 0.1133 - val_accuracy: 0.9874\n",
      "Epoch 59/100\n",
      "34300/34300 [==============================] - 8s 234us/step - loss: 0.0508 - accuracy: 0.9836 - val_loss: 0.1084 - val_accuracy: 0.9889\n",
      "Epoch 60/100\n",
      "34300/34300 [==============================] - 8s 241us/step - loss: 0.0487 - accuracy: 0.9854 - val_loss: 0.0904 - val_accuracy: 0.9894\n",
      "Epoch 61/100\n",
      "34300/34300 [==============================] - 8s 242us/step - loss: 0.0460 - accuracy: 0.9849 - val_loss: 0.0965 - val_accuracy: 0.9912\n",
      "Epoch 62/100\n",
      "34300/34300 [==============================] - 8s 241us/step - loss: 0.0468 - accuracy: 0.9861 - val_loss: 0.1007 - val_accuracy: 0.9903\n",
      "Epoch 63/100\n",
      "34300/34300 [==============================] - 8s 241us/step - loss: 0.0438 - accuracy: 0.9863 - val_loss: 0.1000 - val_accuracy: 0.9897\n",
      "Epoch 64/100\n",
      "34300/34300 [==============================] - 8s 236us/step - loss: 0.0429 - accuracy: 0.9873 - val_loss: 0.1136 - val_accuracy: 0.9894\n",
      "Epoch 65/100\n",
      "34300/34300 [==============================] - 8s 236us/step - loss: 0.0474 - accuracy: 0.9855 - val_loss: 0.1176 - val_accuracy: 0.9899\n",
      "Epoch 66/100\n",
      "34300/34300 [==============================] - 8s 240us/step - loss: 0.0392 - accuracy: 0.9866 - val_loss: 0.0877 - val_accuracy: 0.9918\n",
      "Epoch 67/100\n",
      "34300/34300 [==============================] - 8s 242us/step - loss: 0.0465 - accuracy: 0.9856 - val_loss: 0.1554 - val_accuracy: 0.9871\n",
      "Epoch 68/100\n",
      "34300/34300 [==============================] - 8s 234us/step - loss: 0.0486 - accuracy: 0.9856 - val_loss: 0.1258 - val_accuracy: 0.9865\n",
      "Epoch 69/100\n",
      "34300/34300 [==============================] - 8s 232us/step - loss: 0.0413 - accuracy: 0.9864 - val_loss: 0.1006 - val_accuracy: 0.9897\n",
      "Epoch 70/100\n",
      "34300/34300 [==============================] - 8s 231us/step - loss: 0.0518 - accuracy: 0.9842 - val_loss: 0.1329 - val_accuracy: 0.9872\n",
      "Epoch 71/100\n",
      "34300/34300 [==============================] - 8s 235us/step - loss: 0.0444 - accuracy: 0.9861 - val_loss: 0.1216 - val_accuracy: 0.9872\n",
      "Epoch 72/100\n",
      "34300/34300 [==============================] - 8s 228us/step - loss: 0.0456 - accuracy: 0.9862 - val_loss: 0.1087 - val_accuracy: 0.9893\n",
      "Epoch 73/100\n",
      "34300/34300 [==============================] - 8s 233us/step - loss: 0.0524 - accuracy: 0.9847 - val_loss: 0.1361 - val_accuracy: 0.9876\n",
      "Epoch 74/100\n",
      "34300/34300 [==============================] - 9s 252us/step - loss: 0.0483 - accuracy: 0.9851 - val_loss: 0.1409 - val_accuracy: 0.9862\n",
      "Epoch 75/100\n",
      "34300/34300 [==============================] - 8s 246us/step - loss: 0.0460 - accuracy: 0.9852 - val_loss: 0.1083 - val_accuracy: 0.9905\n",
      "Epoch 76/100\n",
      "34300/34300 [==============================] - 8s 242us/step - loss: 0.0426 - accuracy: 0.9870 - val_loss: 0.1219 - val_accuracy: 0.9883\n",
      "Epoch 77/100\n",
      "34300/34300 [==============================] - 8s 233us/step - loss: 0.0405 - accuracy: 0.9867 - val_loss: 0.1020 - val_accuracy: 0.9914\n",
      "Epoch 78/100\n",
      "34300/34300 [==============================] - 8s 239us/step - loss: 0.0471 - accuracy: 0.9855 - val_loss: 0.1438 - val_accuracy: 0.9864\n",
      "Epoch 79/100\n",
      "34300/34300 [==============================] - 8s 234us/step - loss: 0.0403 - accuracy: 0.9876 - val_loss: 0.1105 - val_accuracy: 0.9888\n",
      "Epoch 80/100\n",
      "34300/34300 [==============================] - 8s 236us/step - loss: 0.0462 - accuracy: 0.9848 - val_loss: 0.1075 - val_accuracy: 0.9899\n",
      "Epoch 81/100\n",
      "34300/34300 [==============================] - 8s 236us/step - loss: 0.0416 - accuracy: 0.9864 - val_loss: 0.1037 - val_accuracy: 0.9906\n",
      "Epoch 82/100\n",
      "34300/34300 [==============================] - 8s 243us/step - loss: 0.0398 - accuracy: 0.9866 - val_loss: 0.1008 - val_accuracy: 0.9915\n",
      "Epoch 83/100\n",
      "34300/34300 [==============================] - 8s 239us/step - loss: 0.0303 - accuracy: 0.9893 - val_loss: 0.1045 - val_accuracy: 0.9914\n",
      "Epoch 84/100\n",
      "34300/34300 [==============================] - 8s 236us/step - loss: 0.0429 - accuracy: 0.9864 - val_loss: 0.1427 - val_accuracy: 0.9891\n",
      "Epoch 85/100\n",
      "34300/34300 [==============================] - 8s 229us/step - loss: 0.0400 - accuracy: 0.9862 - val_loss: 0.1319 - val_accuracy: 0.9888\n",
      "Epoch 86/100\n",
      "34300/34300 [==============================] - 8s 229us/step - loss: 0.0454 - accuracy: 0.9868 - val_loss: 0.1331 - val_accuracy: 0.9896\n",
      "Epoch 87/100\n",
      "34300/34300 [==============================] - 8s 235us/step - loss: 0.0406 - accuracy: 0.9870 - val_loss: 0.2518 - val_accuracy: 0.9795\n",
      "Epoch 88/100\n",
      "34300/34300 [==============================] - 8s 229us/step - loss: 0.0415 - accuracy: 0.9863 - val_loss: 0.1199 - val_accuracy: 0.9895\n",
      "Epoch 89/100\n",
      "34300/34300 [==============================] - 8s 233us/step - loss: 0.0417 - accuracy: 0.9867 - val_loss: 0.1461 - val_accuracy: 0.9888\n",
      "Epoch 90/100\n",
      "34300/34300 [==============================] - 8s 232us/step - loss: 0.0417 - accuracy: 0.9876 - val_loss: 0.1235 - val_accuracy: 0.9904\n",
      "Epoch 91/100\n",
      "34300/34300 [==============================] - 8s 240us/step - loss: 0.0325 - accuracy: 0.9883 - val_loss: 0.1219 - val_accuracy: 0.9910\n",
      "Epoch 92/100\n",
      "34300/34300 [==============================] - 8s 226us/step - loss: 0.0387 - accuracy: 0.9874 - val_loss: 0.1149 - val_accuracy: 0.9903\n",
      "Epoch 93/100\n",
      "34300/34300 [==============================] - 8s 227us/step - loss: 0.0386 - accuracy: 0.9870 - val_loss: 0.1012 - val_accuracy: 0.9915\n",
      "Epoch 94/100\n",
      "34300/34300 [==============================] - 8s 226us/step - loss: 0.0384 - accuracy: 0.9876 - val_loss: 0.1095 - val_accuracy: 0.9909\n",
      "Epoch 95/100\n",
      "34300/34300 [==============================] - 8s 228us/step - loss: 0.0454 - accuracy: 0.9863 - val_loss: 0.1033 - val_accuracy: 0.9901\n",
      "Epoch 96/100\n",
      "34300/34300 [==============================] - 8s 227us/step - loss: 0.0385 - accuracy: 0.9879 - val_loss: 0.1110 - val_accuracy: 0.9903\n",
      "Epoch 97/100\n",
      "34300/34300 [==============================] - 8s 228us/step - loss: 0.0376 - accuracy: 0.9879 - val_loss: 0.1193 - val_accuracy: 0.9909\n",
      "Epoch 98/100\n",
      "34300/34300 [==============================] - 8s 227us/step - loss: 0.0444 - accuracy: 0.9872 - val_loss: 0.1209 - val_accuracy: 0.9898\n",
      "Epoch 99/100\n",
      "34300/34300 [==============================] - 8s 228us/step - loss: 0.0358 - accuracy: 0.9880 - val_loss: 0.1138 - val_accuracy: 0.9913\n",
      "Epoch 100/100\n",
      "34300/34300 [==============================] - 8s 227us/step - loss: 0.0321 - accuracy: 0.9890 - val_loss: 0.1206 - val_accuracy: 0.9910\n",
      "Wall time: 13min 11s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x213dd92aac8>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(X_train, y_train, batch_size=128, epochs=100, verbose=1,\n",
    "         validation_split = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(X_test_sub)\n",
    "y_pred = predicted.argmax(axis=1)\n",
    "\n",
    "Out = pd.DataFrame([test['filename'],y_pred]).T\n",
    "Out.rename(columns = {0:'filename', 1:'label'})\n",
    "#Out\n",
    "Out.to_csv('MNIST_DropOut_2LayerCNN_02he_normal_Adam14.csv', header =  ['filename', 'label' ], index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_cnn_batch_05():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3,3), kernel_initializer='he_normal', input_shape=(28,28,3), strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(64, (3,3), kernel_initializer='he_normal', strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(2,2))    \n",
    "    \n",
    "    model.add(Conv2D(32, (3,3), kernel_initializer='he_normal', input_shape=(28,28,3), strides = (2,2), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(64, (3,3), kernel_initializer='he_normal', strides = (2,2), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(2,2)) \n",
    "    \n",
    "    model.add(Conv2D(32, (3,3), kernel_initializer='he_normal', input_shape=(28,28,3), strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(64, (3,3), kernel_initializer='he_normal', strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(2,2)) \n",
    "    \n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(128, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(64, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))    \n",
    "\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    adam = tf.keras.optimizers.Adam(lr = 0.001) \n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model_cnn_batch_05()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34300 samples, validate on 14700 samples\n",
      "Epoch 1/150\n",
      "34300/34300 [==============================] - 8s 246us/step - loss: 1.2052 - accuracy: 0.6177 - val_loss: 0.2857 - val_accuracy: 0.9219\n",
      "Epoch 2/150\n",
      "34300/34300 [==============================] - 7s 212us/step - loss: 0.2946 - accuracy: 0.9327 - val_loss: 0.0968 - val_accuracy: 0.9722\n",
      "Epoch 3/150\n",
      "34300/34300 [==============================] - 7s 213us/step - loss: 0.1717 - accuracy: 0.9611 - val_loss: 0.0739 - val_accuracy: 0.9799\n",
      "Epoch 4/150\n",
      "34300/34300 [==============================] - 7s 213us/step - loss: 0.1245 - accuracy: 0.9719 - val_loss: 0.0636 - val_accuracy: 0.9837\n",
      "Epoch 5/150\n",
      "34300/34300 [==============================] - 7s 214us/step - loss: 0.0989 - accuracy: 0.9773 - val_loss: 0.0740 - val_accuracy: 0.9813\n",
      "Epoch 6/150\n",
      "34300/34300 [==============================] - 7s 214us/step - loss: 0.0814 - accuracy: 0.9813 - val_loss: 0.0837 - val_accuracy: 0.9797\n",
      "Epoch 7/150\n",
      "34300/34300 [==============================] - 7s 215us/step - loss: 0.0721 - accuracy: 0.9843 - val_loss: 0.0625 - val_accuracy: 0.9852\n",
      "Epoch 8/150\n",
      "34300/34300 [==============================] - 7s 216us/step - loss: 0.0657 - accuracy: 0.9853 - val_loss: 0.0560 - val_accuracy: 0.9863\n",
      "Epoch 9/150\n",
      "34300/34300 [==============================] - 7s 215us/step - loss: 0.0533 - accuracy: 0.9885 - val_loss: 0.0733 - val_accuracy: 0.9832\n",
      "Epoch 10/150\n",
      "34300/34300 [==============================] - 7s 216us/step - loss: 0.0516 - accuracy: 0.9888 - val_loss: 0.0649 - val_accuracy: 0.9842\n",
      "Epoch 11/150\n",
      "34300/34300 [==============================] - 7s 216us/step - loss: 0.0444 - accuracy: 0.9904 - val_loss: 0.0790 - val_accuracy: 0.9816\n",
      "Epoch 12/150\n",
      "34300/34300 [==============================] - 7s 217us/step - loss: 0.0415 - accuracy: 0.9905 - val_loss: 0.0583 - val_accuracy: 0.9870\n",
      "Epoch 13/150\n",
      "34300/34300 [==============================] - 7s 216us/step - loss: 0.0416 - accuracy: 0.9912 - val_loss: 0.0732 - val_accuracy: 0.9859\n",
      "Epoch 14/150\n",
      "34300/34300 [==============================] - 7s 217us/step - loss: 0.0390 - accuracy: 0.9921 - val_loss: 0.0818 - val_accuracy: 0.9838\n",
      "Epoch 15/150\n",
      "34300/34300 [==============================] - 7s 217us/step - loss: 0.0342 - accuracy: 0.9925 - val_loss: 0.0679 - val_accuracy: 0.9844\n",
      "Epoch 16/150\n",
      "34300/34300 [==============================] - 7s 217us/step - loss: 0.0332 - accuracy: 0.9929 - val_loss: 0.0855 - val_accuracy: 0.9834\n",
      "Epoch 17/150\n",
      "34300/34300 [==============================] - 7s 217us/step - loss: 0.0317 - accuracy: 0.9937 - val_loss: 0.0633 - val_accuracy: 0.9882\n",
      "Epoch 18/150\n",
      "34300/34300 [==============================] - 7s 218us/step - loss: 0.0235 - accuracy: 0.9941 - val_loss: 0.0554 - val_accuracy: 0.9889\n",
      "Epoch 19/150\n",
      "34300/34300 [==============================] - 7s 218us/step - loss: 0.0270 - accuracy: 0.9938 - val_loss: 0.0693 - val_accuracy: 0.9869\n",
      "Epoch 20/150\n",
      "34300/34300 [==============================] - 7s 217us/step - loss: 0.0251 - accuracy: 0.9941 - val_loss: 0.0547 - val_accuracy: 0.9897\n",
      "Epoch 21/150\n",
      "34300/34300 [==============================] - 7s 219us/step - loss: 0.0166 - accuracy: 0.9962 - val_loss: 0.0872 - val_accuracy: 0.9863\n",
      "Epoch 22/150\n",
      "34300/34300 [==============================] - 7s 218us/step - loss: 0.0235 - accuracy: 0.9951 - val_loss: 0.0998 - val_accuracy: 0.9812\n",
      "Epoch 23/150\n",
      "34300/34300 [==============================] - 7s 217us/step - loss: 0.0252 - accuracy: 0.9943 - val_loss: 0.0987 - val_accuracy: 0.9826\n",
      "Epoch 24/150\n",
      "34300/34300 [==============================] - 7s 219us/step - loss: 0.0230 - accuracy: 0.9947 - val_loss: 0.0623 - val_accuracy: 0.9884\n",
      "Epoch 25/150\n",
      "34300/34300 [==============================] - 8s 219us/step - loss: 0.0168 - accuracy: 0.9963 - val_loss: 0.0736 - val_accuracy: 0.9884\n",
      "Epoch 26/150\n",
      "34300/34300 [==============================] - 8s 219us/step - loss: 0.0166 - accuracy: 0.9964 - val_loss: 0.0716 - val_accuracy: 0.9876\n",
      "Epoch 27/150\n",
      "34300/34300 [==============================] - 7s 218us/step - loss: 0.0187 - accuracy: 0.9958 - val_loss: 0.0912 - val_accuracy: 0.9858\n",
      "Epoch 28/150\n",
      "34300/34300 [==============================] - 7s 219us/step - loss: 0.0180 - accuracy: 0.9961 - val_loss: 0.0677 - val_accuracy: 0.9881\n",
      "Epoch 29/150\n",
      "34300/34300 [==============================] - 8s 220us/step - loss: 0.0166 - accuracy: 0.9963 - val_loss: 0.0688 - val_accuracy: 0.9886\n",
      "Epoch 30/150\n",
      "34300/34300 [==============================] - 8s 219us/step - loss: 0.0185 - accuracy: 0.9961 - val_loss: 0.0748 - val_accuracy: 0.9886\n",
      "Epoch 31/150\n",
      "34300/34300 [==============================] - 7s 218us/step - loss: 0.0189 - accuracy: 0.9962 - val_loss: 0.0678 - val_accuracy: 0.9886\n",
      "Epoch 32/150\n",
      "34300/34300 [==============================] - 8s 219us/step - loss: 0.0138 - accuracy: 0.9969 - val_loss: 0.0708 - val_accuracy: 0.9876\n",
      "Epoch 33/150\n",
      "34300/34300 [==============================] - 9s 260us/step - loss: 0.0168 - accuracy: 0.9963 - val_loss: 0.0739 - val_accuracy: 0.9880\n",
      "Epoch 34/150\n",
      "34300/34300 [==============================] - 9s 267us/step - loss: 0.0122 - accuracy: 0.9974 - val_loss: 0.0641 - val_accuracy: 0.9900\n",
      "Epoch 35/150\n",
      "34300/34300 [==============================] - 11s 314us/step - loss: 0.0153 - accuracy: 0.9966 - val_loss: 0.0666 - val_accuracy: 0.9901\n",
      "Epoch 36/150\n",
      "34300/34300 [==============================] - 8s 246us/step - loss: 0.0158 - accuracy: 0.9966 - val_loss: 0.0677 - val_accuracy: 0.9894\n",
      "Epoch 37/150\n",
      "34300/34300 [==============================] - 8s 242us/step - loss: 0.0125 - accuracy: 0.9975 - val_loss: 0.0834 - val_accuracy: 0.9881\n",
      "Epoch 38/150\n",
      "34300/34300 [==============================] - 8s 243us/step - loss: 0.0101 - accuracy: 0.9976 - val_loss: 0.0860 - val_accuracy: 0.9882\n",
      "Epoch 39/150\n",
      "34300/34300 [==============================] - 8s 240us/step - loss: 0.0125 - accuracy: 0.9971 - val_loss: 0.0882 - val_accuracy: 0.9865\n",
      "Epoch 40/150\n",
      "34300/34300 [==============================] - 8s 238us/step - loss: 0.0125 - accuracy: 0.9972 - val_loss: 0.0803 - val_accuracy: 0.9885\n",
      "Epoch 41/150\n",
      "34300/34300 [==============================] - 8s 239us/step - loss: 0.0162 - accuracy: 0.9962 - val_loss: 0.0699 - val_accuracy: 0.9886\n",
      "Epoch 42/150\n",
      "34300/34300 [==============================] - 8s 243us/step - loss: 0.0135 - accuracy: 0.9972 - val_loss: 0.0715 - val_accuracy: 0.9890\n",
      "Epoch 43/150\n",
      "34300/34300 [==============================] - 9s 251us/step - loss: 0.0102 - accuracy: 0.9980 - val_loss: 0.0725 - val_accuracy: 0.9883\n",
      "Epoch 44/150\n",
      "34300/34300 [==============================] - 8s 242us/step - loss: 0.0077 - accuracy: 0.9982 - val_loss: 0.0769 - val_accuracy: 0.9892\n",
      "Epoch 45/150\n",
      "34300/34300 [==============================] - 8s 235us/step - loss: 0.0103 - accuracy: 0.9978 - val_loss: 0.0765 - val_accuracy: 0.9896\n",
      "Epoch 46/150\n",
      "34300/34300 [==============================] - 8s 238us/step - loss: 0.0145 - accuracy: 0.9973 - val_loss: 0.0794 - val_accuracy: 0.9890\n",
      "Epoch 47/150\n",
      "34300/34300 [==============================] - 8s 238us/step - loss: 0.0116 - accuracy: 0.9976 - val_loss: 0.0664 - val_accuracy: 0.9904\n",
      "Epoch 48/150\n",
      "34300/34300 [==============================] - 8s 236us/step - loss: 0.0118 - accuracy: 0.9978 - val_loss: 0.0807 - val_accuracy: 0.9884\n",
      "Epoch 49/150\n",
      "34300/34300 [==============================] - 8s 238us/step - loss: 0.0101 - accuracy: 0.9976 - val_loss: 0.0749 - val_accuracy: 0.9890\n",
      "Epoch 50/150\n",
      "34300/34300 [==============================] - 8s 237us/step - loss: 0.0075 - accuracy: 0.9985 - val_loss: 0.0858 - val_accuracy: 0.9882\n",
      "Epoch 51/150\n",
      "34300/34300 [==============================] - 8s 238us/step - loss: 0.0097 - accuracy: 0.9980 - val_loss: 0.1187 - val_accuracy: 0.9845\n",
      "Epoch 52/150\n",
      "34300/34300 [==============================] - 8s 238us/step - loss: 0.0135 - accuracy: 0.9971 - val_loss: 0.0803 - val_accuracy: 0.9889\n",
      "Epoch 53/150\n",
      "34300/34300 [==============================] - 8s 238us/step - loss: 0.0106 - accuracy: 0.9976 - val_loss: 0.0774 - val_accuracy: 0.9884\n",
      "Epoch 54/150\n",
      "34300/34300 [==============================] - 8s 237us/step - loss: 0.0058 - accuracy: 0.9987 - val_loss: 0.0679 - val_accuracy: 0.9904\n",
      "Epoch 55/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34300/34300 [==============================] - 8s 237us/step - loss: 0.0113 - accuracy: 0.9974 - val_loss: 0.1062 - val_accuracy: 0.9859\n",
      "Epoch 56/150\n",
      "34300/34300 [==============================] - 8s 236us/step - loss: 0.0092 - accuracy: 0.9979 - val_loss: 0.0664 - val_accuracy: 0.9906\n",
      "Epoch 57/150\n",
      "34300/34300 [==============================] - 8s 235us/step - loss: 0.0069 - accuracy: 0.9983 - val_loss: 0.0805 - val_accuracy: 0.9886\n",
      "Epoch 58/150\n",
      "34300/34300 [==============================] - 8s 237us/step - loss: 0.0092 - accuracy: 0.9983 - val_loss: 0.1348 - val_accuracy: 0.9841\n",
      "Epoch 59/150\n",
      "34300/34300 [==============================] - 8s 238us/step - loss: 0.0111 - accuracy: 0.9975 - val_loss: 0.0688 - val_accuracy: 0.9904\n",
      "Epoch 60/150\n",
      "34300/34300 [==============================] - 8s 236us/step - loss: 0.0075 - accuracy: 0.9984 - val_loss: 0.0790 - val_accuracy: 0.9899\n",
      "Epoch 61/150\n",
      "34300/34300 [==============================] - 8s 237us/step - loss: 0.0036 - accuracy: 0.9992 - val_loss: 0.0841 - val_accuracy: 0.9888\n",
      "Epoch 62/150\n",
      "34300/34300 [==============================] - 8s 237us/step - loss: 0.0070 - accuracy: 0.9987 - val_loss: 0.0952 - val_accuracy: 0.9883\n",
      "Epoch 63/150\n",
      "34300/34300 [==============================] - 8s 235us/step - loss: 0.0075 - accuracy: 0.9983 - val_loss: 0.0881 - val_accuracy: 0.9884\n",
      "Epoch 64/150\n",
      "34300/34300 [==============================] - 8s 232us/step - loss: 0.0121 - accuracy: 0.9975 - val_loss: 0.0773 - val_accuracy: 0.9903\n",
      "Epoch 65/150\n",
      "34300/34300 [==============================] - 8s 236us/step - loss: 0.0072 - accuracy: 0.9986 - val_loss: 0.0710 - val_accuracy: 0.9904\n",
      "Epoch 66/150\n",
      "34300/34300 [==============================] - 8s 235us/step - loss: 0.0084 - accuracy: 0.9984 - val_loss: 0.0631 - val_accuracy: 0.9908\n",
      "Epoch 67/150\n",
      "34300/34300 [==============================] - 8s 234us/step - loss: 0.0082 - accuracy: 0.9986 - val_loss: 0.0719 - val_accuracy: 0.9909\n",
      "Epoch 68/150\n",
      "34300/34300 [==============================] - 8s 234us/step - loss: 0.0070 - accuracy: 0.9986 - val_loss: 0.0882 - val_accuracy: 0.9884\n",
      "Epoch 69/150\n",
      "34300/34300 [==============================] - 8s 232us/step - loss: 0.0048 - accuracy: 0.9990 - val_loss: 0.1017 - val_accuracy: 0.9867\n",
      "Epoch 70/150\n",
      "34300/34300 [==============================] - 8s 234us/step - loss: 0.0097 - accuracy: 0.9980 - val_loss: 0.0987 - val_accuracy: 0.9880\n",
      "Epoch 71/150\n",
      "34300/34300 [==============================] - 8s 237us/step - loss: 0.0065 - accuracy: 0.9984 - val_loss: 0.0758 - val_accuracy: 0.9894\n",
      "Epoch 72/150\n",
      "34300/34300 [==============================] - 8s 236us/step - loss: 0.0097 - accuracy: 0.9981 - val_loss: 0.0916 - val_accuracy: 0.9871\n",
      "Epoch 73/150\n",
      "34300/34300 [==============================] - 8s 235us/step - loss: 0.0082 - accuracy: 0.9980 - val_loss: 0.0801 - val_accuracy: 0.9897\n",
      "Epoch 74/150\n",
      "34300/34300 [==============================] - 8s 238us/step - loss: 0.0075 - accuracy: 0.9987 - val_loss: 0.0687 - val_accuracy: 0.9912\n",
      "Epoch 75/150\n",
      "34300/34300 [==============================] - 8s 240us/step - loss: 0.0049 - accuracy: 0.9990 - val_loss: 0.0766 - val_accuracy: 0.9897\n",
      "Epoch 76/150\n",
      "34300/34300 [==============================] - 8s 240us/step - loss: 0.0057 - accuracy: 0.9987 - val_loss: 0.0770 - val_accuracy: 0.9903\n",
      "Epoch 77/150\n",
      "34300/34300 [==============================] - 8s 233us/step - loss: 0.0052 - accuracy: 0.9987 - val_loss: 0.0832 - val_accuracy: 0.9905\n",
      "Epoch 78/150\n",
      "34300/34300 [==============================] - 8s 232us/step - loss: 0.0047 - accuracy: 0.9990 - val_loss: 0.0817 - val_accuracy: 0.9908\n",
      "Epoch 79/150\n",
      "34300/34300 [==============================] - 8s 229us/step - loss: 0.0056 - accuracy: 0.9988 - val_loss: 0.0884 - val_accuracy: 0.9885\n",
      "Epoch 80/150\n",
      "34300/34300 [==============================] - 8s 230us/step - loss: 0.0048 - accuracy: 0.9988 - val_loss: 0.0887 - val_accuracy: 0.9899\n",
      "Epoch 81/150\n",
      "34300/34300 [==============================] - 8s 226us/step - loss: 0.0064 - accuracy: 0.9988 - val_loss: 0.0892 - val_accuracy: 0.9898\n",
      "Epoch 82/150\n",
      "34300/34300 [==============================] - 8s 230us/step - loss: 0.0095 - accuracy: 0.9982 - val_loss: 0.0858 - val_accuracy: 0.9904\n",
      "Epoch 83/150\n",
      "34300/34300 [==============================] - 8s 227us/step - loss: 0.0036 - accuracy: 0.9992 - val_loss: 0.0877 - val_accuracy: 0.9901\n",
      "Epoch 84/150\n",
      "34300/34300 [==============================] - 8s 233us/step - loss: 0.0054 - accuracy: 0.9990 - val_loss: 0.0869 - val_accuracy: 0.9899\n",
      "Epoch 85/150\n",
      "34300/34300 [==============================] - 8s 232us/step - loss: 0.0040 - accuracy: 0.9991 - val_loss: 0.0842 - val_accuracy: 0.9901\n",
      "Epoch 86/150\n",
      "34300/34300 [==============================] - 8s 229us/step - loss: 0.0069 - accuracy: 0.9988 - val_loss: 0.1126 - val_accuracy: 0.9873\n",
      "Epoch 87/150\n",
      "34300/34300 [==============================] - 8s 227us/step - loss: 0.0112 - accuracy: 0.9977 - val_loss: 0.0929 - val_accuracy: 0.9891\n",
      "Epoch 88/150\n",
      "34300/34300 [==============================] - 8s 228us/step - loss: 0.0051 - accuracy: 0.9986 - val_loss: 0.0959 - val_accuracy: 0.9893\n",
      "Epoch 89/150\n",
      "34300/34300 [==============================] - 8s 229us/step - loss: 0.0043 - accuracy: 0.9991 - val_loss: 0.0852 - val_accuracy: 0.9902\n",
      "Epoch 90/150\n",
      "34300/34300 [==============================] - 8s 228us/step - loss: 0.0080 - accuracy: 0.9985 - val_loss: 0.0854 - val_accuracy: 0.9903\n",
      "Epoch 91/150\n",
      "34300/34300 [==============================] - 8s 229us/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0838 - val_accuracy: 0.9912\n",
      "Epoch 92/150\n",
      "34300/34300 [==============================] - 8s 227us/step - loss: 0.0045 - accuracy: 0.9992 - val_loss: 0.0914 - val_accuracy: 0.9903\n",
      "Epoch 93/150\n",
      "34300/34300 [==============================] - 8s 227us/step - loss: 0.0037 - accuracy: 0.9990 - val_loss: 0.1022 - val_accuracy: 0.9878\n",
      "Epoch 94/150\n",
      "34300/34300 [==============================] - 8s 230us/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.1119 - val_accuracy: 0.9879\n",
      "Epoch 95/150\n",
      "34300/34300 [==============================] - 8s 229us/step - loss: 0.0074 - accuracy: 0.9984 - val_loss: 0.0862 - val_accuracy: 0.9905\n",
      "Epoch 96/150\n",
      "34300/34300 [==============================] - 8s 231us/step - loss: 0.0058 - accuracy: 0.9987 - val_loss: 0.0741 - val_accuracy: 0.9906\n",
      "Epoch 97/150\n",
      "34300/34300 [==============================] - 8s 225us/step - loss: 0.0044 - accuracy: 0.9990 - val_loss: 0.0843 - val_accuracy: 0.9891\n",
      "Epoch 98/150\n",
      "34300/34300 [==============================] - 8s 231us/step - loss: 0.0032 - accuracy: 0.9994 - val_loss: 0.0826 - val_accuracy: 0.9901\n",
      "Epoch 99/150\n",
      "34300/34300 [==============================] - 8s 230us/step - loss: 0.0035 - accuracy: 0.9993 - val_loss: 0.0960 - val_accuracy: 0.9901\n",
      "Epoch 100/150\n",
      "34300/34300 [==============================] - 8s 229us/step - loss: 0.0066 - accuracy: 0.9987 - val_loss: 0.1086 - val_accuracy: 0.9884\n",
      "Epoch 101/150\n",
      "34300/34300 [==============================] - 9s 248us/step - loss: 0.0059 - accuracy: 0.9990 - val_loss: 0.0900 - val_accuracy: 0.9895\n",
      "Epoch 102/150\n",
      "34300/34300 [==============================] - 9s 252us/step - loss: 0.0077 - accuracy: 0.9981 - val_loss: 0.0944 - val_accuracy: 0.9889\n",
      "Epoch 103/150\n",
      "34300/34300 [==============================] - 8s 244us/step - loss: 0.0075 - accuracy: 0.9987 - val_loss: 0.0728 - val_accuracy: 0.9914\n",
      "Epoch 104/150\n",
      "34300/34300 [==============================] - 8s 243us/step - loss: 0.0038 - accuracy: 0.9992 - val_loss: 0.0771 - val_accuracy: 0.9909\n",
      "Epoch 105/150\n",
      "34300/34300 [==============================] - 8s 236us/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.1011 - val_accuracy: 0.9886\n",
      "Epoch 106/150\n",
      "34300/34300 [==============================] - 8s 236us/step - loss: 0.0040 - accuracy: 0.9990 - val_loss: 0.0818 - val_accuracy: 0.9909\n",
      "Epoch 107/150\n",
      "34300/34300 [==============================] - 8s 238us/step - loss: 0.0054 - accuracy: 0.9990 - val_loss: 0.0963 - val_accuracy: 0.9899\n",
      "Epoch 108/150\n",
      "34300/34300 [==============================] - 8s 235us/step - loss: 0.0029 - accuracy: 0.9992 - val_loss: 0.1028 - val_accuracy: 0.9890\n",
      "Epoch 109/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34300/34300 [==============================] - 8s 234us/step - loss: 0.0035 - accuracy: 0.9991 - val_loss: 0.0939 - val_accuracy: 0.9904\n",
      "Epoch 110/150\n",
      "34300/34300 [==============================] - 8s 234us/step - loss: 0.0037 - accuracy: 0.9992 - val_loss: 0.0944 - val_accuracy: 0.9895\n",
      "Epoch 111/150\n",
      "34300/34300 [==============================] - 8s 235us/step - loss: 0.0061 - accuracy: 0.9987 - val_loss: 0.1135 - val_accuracy: 0.9884\n",
      "Epoch 112/150\n",
      "34300/34300 [==============================] - 8s 232us/step - loss: 0.0049 - accuracy: 0.9990 - val_loss: 0.0842 - val_accuracy: 0.9902\n",
      "Epoch 113/150\n",
      "34300/34300 [==============================] - 8s 233us/step - loss: 0.0032 - accuracy: 0.9991 - val_loss: 0.0811 - val_accuracy: 0.9915\n",
      "Epoch 114/150\n",
      "34300/34300 [==============================] - 8s 229us/step - loss: 0.0050 - accuracy: 0.9991 - val_loss: 0.0937 - val_accuracy: 0.9898\n",
      "Epoch 115/150\n",
      "34300/34300 [==============================] - 8s 234us/step - loss: 0.0062 - accuracy: 0.9985 - val_loss: 0.0858 - val_accuracy: 0.9897\n",
      "Epoch 116/150\n",
      "34300/34300 [==============================] - 8s 233us/step - loss: 0.0057 - accuracy: 0.9986 - val_loss: 0.1185 - val_accuracy: 0.9865\n",
      "Epoch 117/150\n",
      "34300/34300 [==============================] - 8s 238us/step - loss: 0.0068 - accuracy: 0.9988 - val_loss: 0.0922 - val_accuracy: 0.9897\n",
      "Epoch 118/150\n",
      "34300/34300 [==============================] - 8s 233us/step - loss: 0.0043 - accuracy: 0.9991 - val_loss: 0.1113 - val_accuracy: 0.9887\n",
      "Epoch 119/150\n",
      "34300/34300 [==============================] - 8s 231us/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 0.0849 - val_accuracy: 0.9912\n",
      "Epoch 120/150\n",
      "34300/34300 [==============================] - 8s 228us/step - loss: 9.9503e-04 - accuracy: 0.9998 - val_loss: 0.0903 - val_accuracy: 0.9907\n",
      "Epoch 121/150\n",
      "34300/34300 [==============================] - 8s 232us/step - loss: 9.7899e-04 - accuracy: 0.9997 - val_loss: 0.0884 - val_accuracy: 0.9912\n",
      "Epoch 122/150\n",
      "34300/34300 [==============================] - 8s 228us/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.1193 - val_accuracy: 0.9893\n",
      "Epoch 123/150\n",
      "34300/34300 [==============================] - 8s 219us/step - loss: 0.0082 - accuracy: 0.9985 - val_loss: 0.1079 - val_accuracy: 0.9893\n",
      "Epoch 124/150\n",
      "34300/34300 [==============================] - 8s 219us/step - loss: 0.0086 - accuracy: 0.9978 - val_loss: 0.1111 - val_accuracy: 0.9871\n",
      "Epoch 125/150\n",
      "34300/34300 [==============================] - 7s 218us/step - loss: 0.0036 - accuracy: 0.9992 - val_loss: 0.0899 - val_accuracy: 0.9895 loss: 0.0035 - accura\n",
      "Epoch 126/150\n",
      "34300/34300 [==============================] - 7s 218us/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.1019 - val_accuracy: 0.9885\n",
      "Epoch 127/150\n",
      "34300/34300 [==============================] - 7s 218us/step - loss: 0.0035 - accuracy: 0.9993 - val_loss: 0.0957 - val_accuracy: 0.9908\n",
      "Epoch 128/150\n",
      "34300/34300 [==============================] - 7s 217us/step - loss: 0.0042 - accuracy: 0.9991 - val_loss: 0.1081 - val_accuracy: 0.9887\n",
      "Epoch 129/150\n",
      "34300/34300 [==============================] - 7s 218us/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0936 - val_accuracy: 0.9897\n",
      "Epoch 130/150\n",
      "34300/34300 [==============================] - 7s 218us/step - loss: 0.0042 - accuracy: 0.9990 - val_loss: 0.0982 - val_accuracy: 0.9902\n",
      "Epoch 131/150\n",
      "34300/34300 [==============================] - 8s 224us/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.1104 - val_accuracy: 0.9885\n",
      "Epoch 132/150\n",
      "34300/34300 [==============================] - 8s 234us/step - loss: 0.0035 - accuracy: 0.9994 - val_loss: 0.1151 - val_accuracy: 0.9882\n",
      "Epoch 133/150\n",
      "34300/34300 [==============================] - 8s 236us/step - loss: 0.0038 - accuracy: 0.9992 - val_loss: 0.1080 - val_accuracy: 0.9901\n",
      "Epoch 134/150\n",
      "34300/34300 [==============================] - 8s 225us/step - loss: 0.0068 - accuracy: 0.9988 - val_loss: 0.1017 - val_accuracy: 0.9898\n",
      "Epoch 135/150\n",
      "34300/34300 [==============================] - 8s 219us/step - loss: 0.0076 - accuracy: 0.9986 - val_loss: 0.0803 - val_accuracy: 0.9903\n",
      "Epoch 136/150\n",
      "34300/34300 [==============================] - 8s 220us/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0867 - val_accuracy: 0.9907\n",
      "Epoch 137/150\n",
      "34300/34300 [==============================] - 8s 220us/step - loss: 0.0042 - accuracy: 0.9991 - val_loss: 0.0987 - val_accuracy: 0.9899\n",
      "Epoch 138/150\n",
      "34300/34300 [==============================] - 8s 220us/step - loss: 0.0038 - accuracy: 0.9992 - val_loss: 0.0928 - val_accuracy: 0.9897\n",
      "Epoch 139/150\n",
      "34300/34300 [==============================] - 8s 220us/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.0979 - val_accuracy: 0.9901\n",
      "Epoch 140/150\n",
      "34300/34300 [==============================] - 8s 221us/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.1006 - val_accuracy: 0.9904\n",
      "Epoch 141/150\n",
      "34300/34300 [==============================] - 8s 221us/step - loss: 0.0116 - accuracy: 0.9985 - val_loss: 0.0871 - val_accuracy: 0.9910\n",
      "Epoch 142/150\n",
      "34300/34300 [==============================] - 8s 221us/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 0.0841 - val_accuracy: 0.9910\n",
      "Epoch 143/150\n",
      "34300/34300 [==============================] - 8s 220us/step - loss: 0.0027 - accuracy: 0.9996 - val_loss: 0.0857 - val_accuracy: 0.9906\n",
      "Epoch 144/150\n",
      "34300/34300 [==============================] - 8s 220us/step - loss: 0.0057 - accuracy: 0.9990 - val_loss: 0.0865 - val_accuracy: 0.9904\n",
      "Epoch 145/150\n",
      "34300/34300 [==============================] - 8s 219us/step - loss: 0.0037 - accuracy: 0.9992 - val_loss: 0.0903 - val_accuracy: 0.9902\n",
      "Epoch 146/150\n",
      "34300/34300 [==============================] - 8s 220us/step - loss: 0.0042 - accuracy: 0.9993 - val_loss: 0.0811 - val_accuracy: 0.9905\n",
      "Epoch 147/150\n",
      "34300/34300 [==============================] - 8s 221us/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.0949 - val_accuracy: 0.9898\n",
      "Epoch 148/150\n",
      "34300/34300 [==============================] - 8s 221us/step - loss: 0.0029 - accuracy: 0.9992 - val_loss: 0.1233 - val_accuracy: 0.9869\n",
      "Epoch 149/150\n",
      "34300/34300 [==============================] - 8s 220us/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0909 - val_accuracy: 0.9901\n",
      "Epoch 150/150\n",
      "34300/34300 [==============================] - 8s 221us/step - loss: 0.0034 - accuracy: 0.9993 - val_loss: 0.0963 - val_accuracy: 0.9903\n",
      "Wall time: 19min 43s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x213e3605908>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(X_train, y_train, batch_size=128, epochs=150, verbose=1,\n",
    "         validation_split = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(X_test_sub)\n",
    "y_pred = predicted.argmax(axis=1)\n",
    "\n",
    "Out = pd.DataFrame([test['filename'],y_pred]).T\n",
    "Out.rename(columns = {0:'filename', 1:'label'})\n",
    "#Out\n",
    "Out.to_csv('MNIST_DropOut_2LayerCNN_02he_normal_Adam15.csv', header =  ['filename', 'label' ], index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_cnn_batch_06():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3,3), kernel_initializer='he_normal', input_shape=(28,28,3), strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(64, (3,3), kernel_initializer='he_normal', strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(2,2))    \n",
    "    \n",
    "    model.add(Conv2D(32, (3,3), kernel_initializer='he_normal', input_shape=(28,28,3), strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(64, (3,3), kernel_initializer='he_normal', strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(2,2)) \n",
    "    \n",
    "    model.add(Conv2D(32, (3,3), kernel_initializer='he_normal', input_shape=(28,28,3), strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(64, (3,3), kernel_initializer='he_normal', strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(2,2)) \n",
    "    \n",
    "    model.add(Conv2D(32, (3,3), kernel_initializer='he_normal', input_shape=(28,28,3), strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(64, (3,3), kernel_initializer='he_normal', strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(2,2)) \n",
    "    \n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(128, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(64, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))    \n",
    "    \n",
    "    model.add(Dense(32, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    adam = tf.keras.optimizers.Adam(lr = 0.001) \n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model_cnn_batch_06()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34300 samples, validate on 14700 samples\n",
      "Epoch 1/150\n",
      "34300/34300 [==============================] - 11s 320us/step - loss: 1.8130 - accuracy: 0.3831 - val_loss: 0.6084 - val_accuracy: 0.9204\n",
      "Epoch 2/150\n",
      "34300/34300 [==============================] - 9s 260us/step - loss: 0.8013 - accuracy: 0.7588 - val_loss: 0.1801 - val_accuracy: 0.9683\n",
      "Epoch 3/150\n",
      "34300/34300 [==============================] - 9s 261us/step - loss: 0.4592 - accuracy: 0.8711 - val_loss: 0.0946 - val_accuracy: 0.9767\n",
      "Epoch 4/150\n",
      "34300/34300 [==============================] - 9s 265us/step - loss: 0.3352 - accuracy: 0.9063 - val_loss: 0.1215 - val_accuracy: 0.9694\n",
      "Epoch 5/150\n",
      "34300/34300 [==============================] - 9s 263us/step - loss: 0.2561 - accuracy: 0.9316 - val_loss: 0.1154 - val_accuracy: 0.9693\n",
      "Epoch 6/150\n",
      "34300/34300 [==============================] - 9s 265us/step - loss: 0.2160 - accuracy: 0.9407 - val_loss: 0.1041 - val_accuracy: 0.9750\n",
      "Epoch 7/150\n",
      "34300/34300 [==============================] - 9s 263us/step - loss: 0.1926 - accuracy: 0.9487 - val_loss: 0.0760 - val_accuracy: 0.9828\n",
      "Epoch 8/150\n",
      "34300/34300 [==============================] - 9s 266us/step - loss: 0.1634 - accuracy: 0.9564 - val_loss: 0.1240 - val_accuracy: 0.9728\n",
      "Epoch 9/150\n",
      "34300/34300 [==============================] - 9s 264us/step - loss: 0.1538 - accuracy: 0.9571 - val_loss: 0.0804 - val_accuracy: 0.9832\n",
      "Epoch 10/150\n",
      "34300/34300 [==============================] - 9s 264us/step - loss: 0.1422 - accuracy: 0.9627 - val_loss: 0.1139 - val_accuracy: 0.9784\n",
      "Epoch 11/150\n",
      "34300/34300 [==============================] - 9s 265us/step - loss: 0.1244 - accuracy: 0.9657 - val_loss: 0.0610 - val_accuracy: 0.9883\n",
      "Epoch 12/150\n",
      "34300/34300 [==============================] - 9s 265us/step - loss: 0.1205 - accuracy: 0.9655 - val_loss: 0.1024 - val_accuracy: 0.9791\n",
      "Epoch 13/150\n",
      "34300/34300 [==============================] - 9s 265us/step - loss: 0.1226 - accuracy: 0.9675 - val_loss: 0.0621 - val_accuracy: 0.9882\n",
      "Epoch 14/150\n",
      "34300/34300 [==============================] - 9s 265us/step - loss: 0.1106 - accuracy: 0.9700 - val_loss: 0.0607 - val_accuracy: 0.9886\n",
      "Epoch 15/150\n",
      "34300/34300 [==============================] - 9s 265us/step - loss: 0.1071 - accuracy: 0.9718 - val_loss: 0.0733 - val_accuracy: 0.9863\n",
      "Epoch 16/150\n",
      "34300/34300 [==============================] - 9s 266us/step - loss: 0.1006 - accuracy: 0.9736 - val_loss: 0.0489 - val_accuracy: 0.9912\n",
      "Epoch 17/150\n",
      "34300/34300 [==============================] - 9s 265us/step - loss: 0.0835 - accuracy: 0.9761 - val_loss: 0.0656 - val_accuracy: 0.9895\n",
      "Epoch 18/150\n",
      "34300/34300 [==============================] - 9s 267us/step - loss: 0.0960 - accuracy: 0.9751 - val_loss: 0.1081 - val_accuracy: 0.9838\n",
      "Epoch 19/150\n",
      "34300/34300 [==============================] - 9s 268us/step - loss: 0.0845 - accuracy: 0.9771 - val_loss: 0.0888 - val_accuracy: 0.9879\n",
      "Epoch 20/150\n",
      "34300/34300 [==============================] - 9s 267us/step - loss: 0.0816 - accuracy: 0.9779 - val_loss: 0.0684 - val_accuracy: 0.9886\n",
      "Epoch 21/150\n",
      "34300/34300 [==============================] - 9s 266us/step - loss: 0.0857 - accuracy: 0.9763 - val_loss: 0.0637 - val_accuracy: 0.9903\n",
      "Epoch 22/150\n",
      "34300/34300 [==============================] - 9s 268us/step - loss: 0.0785 - accuracy: 0.9782 - val_loss: 0.0592 - val_accuracy: 0.9898\n",
      "Epoch 23/150\n",
      "34300/34300 [==============================] - 9s 266us/step - loss: 0.0759 - accuracy: 0.9795 - val_loss: 0.0768 - val_accuracy: 0.9880\n",
      "Epoch 24/150\n",
      "34300/34300 [==============================] - 9s 267us/step - loss: 0.0682 - accuracy: 0.9792 - val_loss: 0.0883 - val_accuracy: 0.9873\n",
      "Epoch 25/150\n",
      "34300/34300 [==============================] - 9s 267us/step - loss: 0.0718 - accuracy: 0.9800 - val_loss: 0.3979 - val_accuracy: 0.9507\n",
      "Epoch 26/150\n",
      "34300/34300 [==============================] - 9s 268us/step - loss: 0.0693 - accuracy: 0.9787 - val_loss: 0.0612 - val_accuracy: 0.9910\n",
      "Epoch 27/150\n",
      "34300/34300 [==============================] - 9s 267us/step - loss: 0.0707 - accuracy: 0.9794 - val_loss: 0.0712 - val_accuracy: 0.9908\n",
      "Epoch 28/150\n",
      "34300/34300 [==============================] - 10s 280us/step - loss: 0.0735 - accuracy: 0.9799 - val_loss: 0.0705 - val_accuracy: 0.9896\n",
      "Epoch 29/150\n",
      "34300/34300 [==============================] - 9s 271us/step - loss: 0.0639 - accuracy: 0.9823 - val_loss: 0.0572 - val_accuracy: 0.9921\n",
      "Epoch 30/150\n",
      "34300/34300 [==============================] - 9s 271us/step - loss: 0.0668 - accuracy: 0.9808 - val_loss: 0.0916 - val_accuracy: 0.9871\n",
      "Epoch 31/150\n",
      "34300/34300 [==============================] - 9s 274us/step - loss: 0.0610 - accuracy: 0.9814 - val_loss: 0.0672 - val_accuracy: 0.9901\n",
      "Epoch 32/150\n",
      "34300/34300 [==============================] - 9s 273us/step - loss: 0.0578 - accuracy: 0.9820 - val_loss: 0.0569 - val_accuracy: 0.9914\n",
      "Epoch 33/150\n",
      "34300/34300 [==============================] - 9s 271us/step - loss: 0.0639 - accuracy: 0.9814 - val_loss: 0.0789 - val_accuracy: 0.9908\n",
      "Epoch 34/150\n",
      "34300/34300 [==============================] - 9s 272us/step - loss: 0.0595 - accuracy: 0.9815 - val_loss: 0.2460 - val_accuracy: 0.9655\n",
      "Epoch 35/150\n",
      "34300/34300 [==============================] - 9s 272us/step - loss: 0.0602 - accuracy: 0.9825 - val_loss: 0.0670 - val_accuracy: 0.9913\n",
      "Epoch 36/150\n",
      "34300/34300 [==============================] - 9s 273us/step - loss: 0.0632 - accuracy: 0.9820 - val_loss: 0.0758 - val_accuracy: 0.9907\n",
      "Epoch 37/150\n",
      "34300/34300 [==============================] - 9s 273us/step - loss: 0.0582 - accuracy: 0.9820 - val_loss: 0.0830 - val_accuracy: 0.9905\n",
      "Epoch 38/150\n",
      "34300/34300 [==============================] - 9s 273us/step - loss: 0.0556 - accuracy: 0.9822 - val_loss: 0.0561 - val_accuracy: 0.9927\n",
      "Epoch 39/150\n",
      "34300/34300 [==============================] - 9s 272us/step - loss: 0.0552 - accuracy: 0.9834 - val_loss: 0.0701 - val_accuracy: 0.9914\n",
      "Epoch 40/150\n",
      "34300/34300 [==============================] - 9s 272us/step - loss: 0.0575 - accuracy: 0.9821 - val_loss: 0.0755 - val_accuracy: 0.9910\n",
      "Epoch 41/150\n",
      "34300/34300 [==============================] - 9s 272us/step - loss: 0.0543 - accuracy: 0.9834 - val_loss: 0.0906 - val_accuracy: 0.9884\n",
      "Epoch 42/150\n",
      "34300/34300 [==============================] - 9s 272us/step - loss: 0.0537 - accuracy: 0.9830 - val_loss: 0.0878 - val_accuracy: 0.9899\n",
      "Epoch 43/150\n",
      "34300/34300 [==============================] - 9s 273us/step - loss: 0.0445 - accuracy: 0.9863 - val_loss: 0.0944 - val_accuracy: 0.9901\n",
      "Epoch 44/150\n",
      "34300/34300 [==============================] - 9s 272us/step - loss: 0.0481 - accuracy: 0.9845 - val_loss: 0.1365 - val_accuracy: 0.9848\n",
      "Epoch 45/150\n",
      "34300/34300 [==============================] - 9s 273us/step - loss: 0.0534 - accuracy: 0.9828 - val_loss: 0.0822 - val_accuracy: 0.9912\n",
      "Epoch 46/150\n",
      "34300/34300 [==============================] - 9s 273us/step - loss: 0.0566 - accuracy: 0.9834 - val_loss: 0.0718 - val_accuracy: 0.9919\n",
      "Epoch 47/150\n",
      "34300/34300 [==============================] - 9s 272us/step - loss: 0.0467 - accuracy: 0.9842 - val_loss: 0.0805 - val_accuracy: 0.9918\n",
      "Epoch 48/150\n",
      "34300/34300 [==============================] - 9s 273us/step - loss: 0.0507 - accuracy: 0.9837 - val_loss: 0.1040 - val_accuracy: 0.9890\n",
      "Epoch 49/150\n",
      "34300/34300 [==============================] - 9s 272us/step - loss: 0.0504 - accuracy: 0.9843 - val_loss: 0.0801 - val_accuracy: 0.9912\n",
      "Epoch 50/150\n",
      "34300/34300 [==============================] - 9s 273us/step - loss: 0.0591 - accuracy: 0.9816 - val_loss: 0.0955 - val_accuracy: 0.9899\n",
      "Epoch 51/150\n",
      "34300/34300 [==============================] - 9s 274us/step - loss: 0.0456 - accuracy: 0.9856 - val_loss: 0.0804 - val_accuracy: 0.9912\n",
      "Epoch 52/150\n",
      "34300/34300 [==============================] - 9s 272us/step - loss: 0.0477 - accuracy: 0.9835 - val_loss: 0.0732 - val_accuracy: 0.9926\n",
      "Epoch 53/150\n",
      "34300/34300 [==============================] - 9s 275us/step - loss: 0.0417 - accuracy: 0.9864 - val_loss: 0.1005 - val_accuracy: 0.9891\n",
      "Epoch 54/150\n",
      "34300/34300 [==============================] - 9s 273us/step - loss: 0.0460 - accuracy: 0.9856 - val_loss: 0.1075 - val_accuracy: 0.9899\n",
      "Epoch 55/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34300/34300 [==============================] - 9s 270us/step - loss: 0.0478 - accuracy: 0.9853 - val_loss: 0.1071 - val_accuracy: 0.9893\n",
      "Epoch 56/150\n",
      "34300/34300 [==============================] - 9s 272us/step - loss: 0.0431 - accuracy: 0.9854 - val_loss: 0.0874 - val_accuracy: 0.9909\n",
      "Epoch 57/150\n",
      "34300/34300 [==============================] - 9s 270us/step - loss: 0.0427 - accuracy: 0.9856 - val_loss: 0.1403 - val_accuracy: 0.9882\n",
      "Epoch 58/150\n",
      "34300/34300 [==============================] - 9s 270us/step - loss: 0.0497 - accuracy: 0.9842 - val_loss: 0.0826 - val_accuracy: 0.9912\n",
      "Epoch 59/150\n",
      "34300/34300 [==============================] - 9s 271us/step - loss: 0.0402 - accuracy: 0.9851 - val_loss: 0.0944 - val_accuracy: 0.9910\n",
      "Epoch 60/150\n",
      "34300/34300 [==============================] - 9s 271us/step - loss: 0.0461 - accuracy: 0.9856 - val_loss: 0.0922 - val_accuracy: 0.9918\n",
      "Epoch 61/150\n",
      "34300/34300 [==============================] - 9s 271us/step - loss: 0.0447 - accuracy: 0.9861 - val_loss: 0.0930 - val_accuracy: 0.9918\n",
      "Epoch 62/150\n",
      "34300/34300 [==============================] - 9s 271us/step - loss: 0.0427 - accuracy: 0.9866 - val_loss: 0.0898 - val_accuracy: 0.9918\n",
      "Epoch 63/150\n",
      "34300/34300 [==============================] - 9s 271us/step - loss: 0.0399 - accuracy: 0.9854 - val_loss: 0.0829 - val_accuracy: 0.9918\n",
      "Epoch 64/150\n",
      "34300/34300 [==============================] - 9s 273us/step - loss: 0.0431 - accuracy: 0.9860 - val_loss: 0.1167 - val_accuracy: 0.9897\n",
      "Epoch 65/150\n",
      "34300/34300 [==============================] - 9s 273us/step - loss: 0.0372 - accuracy: 0.9867 - val_loss: 0.0850 - val_accuracy: 0.9929\n",
      "Epoch 66/150\n",
      "34300/34300 [==============================] - 9s 272us/step - loss: 0.0405 - accuracy: 0.9859 - val_loss: 0.1028 - val_accuracy: 0.9916\n",
      "Epoch 67/150\n",
      "34300/34300 [==============================] - 9s 266us/step - loss: 0.0398 - accuracy: 0.9864 - val_loss: 0.1096 - val_accuracy: 0.9903\n",
      "Epoch 68/150\n",
      "34300/34300 [==============================] - 9s 265us/step - loss: 0.0456 - accuracy: 0.9853 - val_loss: 0.0818 - val_accuracy: 0.9921\n",
      "Epoch 69/150\n",
      "34300/34300 [==============================] - 9s 267us/step - loss: 0.0347 - accuracy: 0.9871 - val_loss: 0.0784 - val_accuracy: 0.9925\n",
      "Epoch 70/150\n",
      "34300/34300 [==============================] - 9s 266us/step - loss: 0.0349 - accuracy: 0.9869 - val_loss: 0.0835 - val_accuracy: 0.9921\n",
      "Epoch 71/150\n",
      "34300/34300 [==============================] - 9s 266us/step - loss: 0.0553 - accuracy: 0.9848 - val_loss: 0.1150 - val_accuracy: 0.9890\n",
      "Epoch 72/150\n",
      "34300/34300 [==============================] - 9s 265us/step - loss: 0.0468 - accuracy: 0.9857 - val_loss: 0.0867 - val_accuracy: 0.9907\n",
      "Epoch 73/150\n",
      "34300/34300 [==============================] - 9s 267us/step - loss: 0.0394 - accuracy: 0.9876 - val_loss: 0.0805 - val_accuracy: 0.9926\n",
      "Epoch 74/150\n",
      "34300/34300 [==============================] - 9s 266us/step - loss: 0.0351 - accuracy: 0.9874 - val_loss: 0.1025 - val_accuracy: 0.9916\n",
      "Epoch 75/150\n",
      "34300/34300 [==============================] - 9s 266us/step - loss: 0.0320 - accuracy: 0.9878 - val_loss: 0.0955 - val_accuracy: 0.9921\n",
      "Epoch 76/150\n",
      "34300/34300 [==============================] - 9s 265us/step - loss: 0.0421 - accuracy: 0.9862 - val_loss: 0.1224 - val_accuracy: 0.9894\n",
      "Epoch 77/150\n",
      "34300/34300 [==============================] - 9s 266us/step - loss: 0.0421 - accuracy: 0.9845 - val_loss: 0.0815 - val_accuracy: 0.9933\n",
      "Epoch 78/150\n",
      "34300/34300 [==============================] - 9s 266us/step - loss: 0.0395 - accuracy: 0.9862 - val_loss: 0.1317 - val_accuracy: 0.9883\n",
      "Epoch 79/150\n",
      "34300/34300 [==============================] - 9s 266us/step - loss: 0.0460 - accuracy: 0.9857 - val_loss: 0.1093 - val_accuracy: 0.9897\n",
      "Epoch 80/150\n",
      "34300/34300 [==============================] - 9s 266us/step - loss: 0.0398 - accuracy: 0.9877 - val_loss: 0.0927 - val_accuracy: 0.9910\n",
      "Epoch 81/150\n",
      "34300/34300 [==============================] - 9s 266us/step - loss: 0.0420 - accuracy: 0.9868 - val_loss: 0.0836 - val_accuracy: 0.9927\n",
      "Epoch 82/150\n",
      "34300/34300 [==============================] - 9s 266us/step - loss: 0.0357 - accuracy: 0.9869 - val_loss: 0.0819 - val_accuracy: 0.9935\n",
      "Epoch 83/150\n",
      "34300/34300 [==============================] - 9s 266us/step - loss: 0.0347 - accuracy: 0.9875 - val_loss: 0.0917 - val_accuracy: 0.9920\n",
      "Epoch 84/150\n",
      "34300/34300 [==============================] - 10s 283us/step - loss: 0.0472 - accuracy: 0.9847 - val_loss: 0.0846 - val_accuracy: 0.9916\n",
      "Epoch 85/150\n",
      "34300/34300 [==============================] - 10s 281us/step - loss: 0.0432 - accuracy: 0.9862 - val_loss: 0.1507 - val_accuracy: 0.9843\n",
      "Epoch 86/150\n",
      "34300/34300 [==============================] - 9s 271us/step - loss: 0.0444 - accuracy: 0.9855 - val_loss: 0.0924 - val_accuracy: 0.9915\n",
      "Epoch 87/150\n",
      "34300/34300 [==============================] - 10s 278us/step - loss: 0.0377 - accuracy: 0.9875 - val_loss: 0.0720 - val_accuracy: 0.9929\n",
      "Epoch 88/150\n",
      "34300/34300 [==============================] - 9s 274us/step - loss: 0.0437 - accuracy: 0.9861 - val_loss: 0.0861 - val_accuracy: 0.9928\n",
      "Epoch 89/150\n",
      "34300/34300 [==============================] - 9s 270us/step - loss: 0.0379 - accuracy: 0.9862 - val_loss: 0.0877 - val_accuracy: 0.9927\n",
      "Epoch 90/150\n",
      "34300/34300 [==============================] - 9s 270us/step - loss: 0.0339 - accuracy: 0.9882 - val_loss: 0.0776 - val_accuracy: 0.9939\n",
      "Epoch 91/150\n",
      "34300/34300 [==============================] - 10s 288us/step - loss: 0.0310 - accuracy: 0.9883 - val_loss: 0.0876 - val_accuracy: 0.9929\n",
      "Epoch 92/150\n",
      "34300/34300 [==============================] - 10s 277us/step - loss: 0.0284 - accuracy: 0.9885 - val_loss: 0.0969 - val_accuracy: 0.9931\n",
      "Epoch 93/150\n",
      "34300/34300 [==============================] - 10s 286us/step - loss: 0.0332 - accuracy: 0.9877 - val_loss: 0.0925 - val_accuracy: 0.9930\n",
      "Epoch 94/150\n",
      "34300/34300 [==============================] - 9s 277us/step - loss: 0.0425 - accuracy: 0.9868 - val_loss: 0.0847 - val_accuracy: 0.9935\n",
      "Epoch 95/150\n",
      "34300/34300 [==============================] - 9s 270us/step - loss: 0.0437 - accuracy: 0.9860 - val_loss: 0.0822 - val_accuracy: 0.9928\n",
      "Epoch 96/150\n",
      "34300/34300 [==============================] - 10s 288us/step - loss: 0.0395 - accuracy: 0.9872 - val_loss: 0.0841 - val_accuracy: 0.9927\n",
      "Epoch 97/150\n",
      "34300/34300 [==============================] - 9s 275us/step - loss: 0.0314 - accuracy: 0.9878 - val_loss: 0.0840 - val_accuracy: 0.9935\n",
      "Epoch 98/150\n",
      "34300/34300 [==============================] - 10s 288us/step - loss: 0.0353 - accuracy: 0.9883 - val_loss: 0.0845 - val_accuracy: 0.9934\n",
      "Epoch 99/150\n",
      "34300/34300 [==============================] - 9s 272us/step - loss: 0.0312 - accuracy: 0.9888 - val_loss: 0.0843 - val_accuracy: 0.99330s - los\n",
      "Epoch 100/150\n",
      "34300/34300 [==============================] - 9s 271us/step - loss: 0.0395 - accuracy: 0.9873 - val_loss: 0.0922 - val_accuracy: 0.9917\n",
      "Epoch 101/150\n",
      "34300/34300 [==============================] - 10s 301us/step - loss: 0.0440 - accuracy: 0.9860 - val_loss: 0.0852 - val_accuracy: 0.9926\n",
      "Epoch 102/150\n",
      "34300/34300 [==============================] - 10s 289us/step - loss: 0.0301 - accuracy: 0.9887 - val_loss: 0.1139 - val_accuracy: 0.9908\n",
      "Epoch 103/150\n",
      "34300/34300 [==============================] - 10s 279us/step - loss: 0.0391 - accuracy: 0.9865 - val_loss: 0.1143 - val_accuracy: 0.9894\n",
      "Epoch 104/150\n",
      "34300/34300 [==============================] - 10s 281us/step - loss: 0.0338 - accuracy: 0.9874 - val_loss: 0.0990 - val_accuracy: 0.9927\n",
      "Epoch 105/150\n",
      "34300/34300 [==============================] - 10s 281us/step - loss: 0.0332 - accuracy: 0.9881 - val_loss: 0.1085 - val_accuracy: 0.9922\n",
      "Epoch 106/150\n",
      "34300/34300 [==============================] - 10s 279us/step - loss: 0.0321 - accuracy: 0.9887 - val_loss: 0.1112 - val_accuracy: 0.9916\n",
      "Epoch 107/150\n",
      "34300/34300 [==============================] - 9s 276us/step - loss: 0.0510 - accuracy: 0.9859 - val_loss: 0.0897 - val_accuracy: 0.9924\n",
      "Epoch 108/150\n",
      "34300/34300 [==============================] - 9s 273us/step - loss: 0.0336 - accuracy: 0.9871 - val_loss: 0.0905 - val_accuracy: 0.9929\n",
      "Epoch 109/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34300/34300 [==============================] - 9s 272us/step - loss: 0.0308 - accuracy: 0.9889 - val_loss: 0.1268 - val_accuracy: 0.9911\n",
      "Epoch 110/150\n",
      "34300/34300 [==============================] - 9s 272us/step - loss: 0.0344 - accuracy: 0.9887 - val_loss: 0.0889 - val_accuracy: 0.9933\n",
      "Epoch 111/150\n",
      "34300/34300 [==============================] - 9s 271us/step - loss: 0.0331 - accuracy: 0.9878 - val_loss: 0.0857 - val_accuracy: 0.9935\n",
      "Epoch 112/150\n",
      "34300/34300 [==============================] - 9s 272us/step - loss: 0.0309 - accuracy: 0.9883 - val_loss: 0.1057 - val_accuracy: 0.9926\n",
      "Epoch 113/150\n",
      "34300/34300 [==============================] - 9s 277us/step - loss: 0.0328 - accuracy: 0.9882 - val_loss: 0.0969 - val_accuracy: 0.9931\n",
      "Epoch 114/150\n",
      "34300/34300 [==============================] - 9s 271us/step - loss: 0.0301 - accuracy: 0.9891 - val_loss: 0.1133 - val_accuracy: 0.9920\n",
      "Epoch 115/150\n",
      "34300/34300 [==============================] - 9s 271us/step - loss: 0.0302 - accuracy: 0.9891 - val_loss: 0.1259 - val_accuracy: 0.9922\n",
      "Epoch 116/150\n",
      "34300/34300 [==============================] - 10s 280us/step - loss: 0.0367 - accuracy: 0.9882 - val_loss: 0.0984 - val_accuracy: 0.9928\n",
      "Epoch 117/150\n",
      "34300/34300 [==============================] - 10s 288us/step - loss: 0.0340 - accuracy: 0.9883 - val_loss: 0.1024 - val_accuracy: 0.9924\n",
      "Epoch 118/150\n",
      "34300/34300 [==============================] - 10s 280us/step - loss: 0.0420 - accuracy: 0.9879 - val_loss: 0.0994 - val_accuracy: 0.9924\n",
      "Epoch 119/150\n",
      "34300/34300 [==============================] - 10s 289us/step - loss: 0.0309 - accuracy: 0.9891 - val_loss: 0.0783 - val_accuracy: 0.9940\n",
      "Epoch 120/150\n",
      "34300/34300 [==============================] - 9s 275us/step - loss: 0.0292 - accuracy: 0.9890 - val_loss: 0.1020 - val_accuracy: 0.9930\n",
      "Epoch 121/150\n",
      "34300/34300 [==============================] - 10s 278us/step - loss: 0.0369 - accuracy: 0.9876 - val_loss: 0.1096 - val_accuracy: 0.9934\n",
      "Epoch 122/150\n",
      "34300/34300 [==============================] - 10s 281us/step - loss: 0.0328 - accuracy: 0.9878 - val_loss: 0.1090 - val_accuracy: 0.9928\n",
      "Epoch 123/150\n",
      "34300/34300 [==============================] - 10s 289us/step - loss: 0.0307 - accuracy: 0.9888 - val_loss: 0.1059 - val_accuracy: 0.9933\n",
      "Epoch 124/150\n",
      "34300/34300 [==============================] - 9s 275us/step - loss: 0.0421 - accuracy: 0.9873 - val_loss: 0.1051 - val_accuracy: 0.9929\n",
      "Epoch 125/150\n",
      "34300/34300 [==============================] - 10s 287us/step - loss: 0.0344 - accuracy: 0.9876 - val_loss: 0.0968 - val_accuracy: 0.9929\n",
      "Epoch 126/150\n",
      "34300/34300 [==============================] - 9s 273us/step - loss: 0.0387 - accuracy: 0.9870 - val_loss: 0.1019 - val_accuracy: 0.9927\n",
      "Epoch 127/150\n",
      "34300/34300 [==============================] - 10s 281us/step - loss: 0.0298 - accuracy: 0.9900 - val_loss: 0.0899 - val_accuracy: 0.9939\n",
      "Epoch 128/150\n",
      "34300/34300 [==============================] - 9s 275us/step - loss: 0.0329 - accuracy: 0.9887 - val_loss: 0.1343 - val_accuracy: 0.9918\n",
      "Epoch 129/150\n",
      "34300/34300 [==============================] - 10s 278us/step - loss: 0.0368 - accuracy: 0.9877 - val_loss: 0.1011 - val_accuracy: 0.9933\n",
      "Epoch 130/150\n",
      "34300/34300 [==============================] - 10s 278us/step - loss: 0.0281 - accuracy: 0.9890 - val_loss: 0.1136 - val_accuracy: 0.9927\n",
      "Epoch 131/150\n",
      "34300/34300 [==============================] - 10s 286us/step - loss: 0.0313 - accuracy: 0.9883 - val_loss: 0.1094 - val_accuracy: 0.9925\n",
      "Epoch 132/150\n",
      "34300/34300 [==============================] - 9s 277us/step - loss: 0.0338 - accuracy: 0.9884 - val_loss: 0.1514 - val_accuracy: 0.9866\n",
      "Epoch 133/150\n",
      "34300/34300 [==============================] - 10s 277us/step - loss: 0.0353 - accuracy: 0.9883 - val_loss: 0.0984 - val_accuracy: 0.9932\n",
      "Epoch 134/150\n",
      "34300/34300 [==============================] - 9s 273us/step - loss: 0.0340 - accuracy: 0.9883 - val_loss: 0.0786 - val_accuracy: 0.9939\n",
      "Epoch 135/150\n",
      "34300/34300 [==============================] - 9s 274us/step - loss: 0.0294 - accuracy: 0.9894 - val_loss: 0.0937 - val_accuracy: 0.9936\n",
      "Epoch 136/150\n",
      "34300/34300 [==============================] - 10s 292us/step - loss: 0.0342 - accuracy: 0.9880 - val_loss: 0.0906 - val_accuracy: 0.9935\n",
      "Epoch 137/150\n",
      "34300/34300 [==============================] - 9s 271us/step - loss: 0.0324 - accuracy: 0.9886 - val_loss: 0.0905 - val_accuracy: 0.9934\n",
      "Epoch 138/150\n",
      "34300/34300 [==============================] - 9s 270us/step - loss: 0.0302 - accuracy: 0.9892 - val_loss: 0.1013 - val_accuracy: 0.9924\n",
      "Epoch 139/150\n",
      "34300/34300 [==============================] - 9s 272us/step - loss: 0.0329 - accuracy: 0.9892 - val_loss: 0.0991 - val_accuracy: 0.9931\n",
      "Epoch 140/150\n",
      "34300/34300 [==============================] - 9s 272us/step - loss: 0.0265 - accuracy: 0.9902 - val_loss: 0.0996 - val_accuracy: 0.9939\n",
      "Epoch 141/150\n",
      "34300/34300 [==============================] - 9s 274us/step - loss: 0.0333 - accuracy: 0.9885 - val_loss: 0.1048 - val_accuracy: 0.9935\n",
      "Epoch 142/150\n",
      "34300/34300 [==============================] - 9s 271us/step - loss: 0.0324 - accuracy: 0.9888 - val_loss: 0.1253 - val_accuracy: 0.9911\n",
      "Epoch 143/150\n",
      "34300/34300 [==============================] - 10s 283us/step - loss: 0.0331 - accuracy: 0.9885 - val_loss: 0.0903 - val_accuracy: 0.9939\n",
      "Epoch 144/150\n",
      "34300/34300 [==============================] - 9s 273us/step - loss: 0.0267 - accuracy: 0.9898 - val_loss: 0.1315 - val_accuracy: 0.9923\n",
      "Epoch 145/150\n",
      "34300/34300 [==============================] - 9s 269us/step - loss: 0.0353 - accuracy: 0.9882 - val_loss: 0.0966 - val_accuracy: 0.9929\n",
      "Epoch 146/150\n",
      "34300/34300 [==============================] - 9s 270us/step - loss: 0.0295 - accuracy: 0.9888 - val_loss: 0.0978 - val_accuracy: 0.9940\n",
      "Epoch 147/150\n",
      "34300/34300 [==============================] - 9s 269us/step - loss: 0.0288 - accuracy: 0.9893 - val_loss: 0.0992 - val_accuracy: 0.9938\n",
      "Epoch 148/150\n",
      "34300/34300 [==============================] - 9s 270us/step - loss: 0.0297 - accuracy: 0.9891 - val_loss: 0.1668 - val_accuracy: 0.9903\n",
      "Epoch 149/150\n",
      "34300/34300 [==============================] - 10s 279us/step - loss: 0.0403 - accuracy: 0.9889 - val_loss: 0.0984 - val_accuracy: 0.9935\n",
      "Epoch 150/150\n",
      "34300/34300 [==============================] - 10s 279us/step - loss: 0.0345 - accuracy: 0.9890 - val_loss: 0.1067 - val_accuracy: 0.9920\n",
      "Wall time: 23min 27s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x213e8151b88>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(X_train, y_train, batch_size=128, epochs=150, verbose=1,\n",
    "         validation_split = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(X_test_sub)\n",
    "y_pred = predicted.argmax(axis=1)\n",
    "\n",
    "Out = pd.DataFrame([test['filename'],y_pred]).T\n",
    "Out.rename(columns = {0:'filename', 1:'label'})\n",
    "#Out\n",
    "Out.to_csv('MNIST_DropOut_2LayerCNN_02he_normal_Adam16.csv', header =  ['filename', 'label' ], index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_cnn_batch_07():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (4,4), kernel_initializer='he_normal', input_shape=(28,28,3), strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(64, (4,4), kernel_initializer='he_normal', strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(128, (4,4), kernel_initializer='he_normal', input_shape=(28,28,3), strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(256, (4,4), kernel_initializer='he_normal', strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(2,2))    \n",
    "    \n",
    "    model.add(Conv2D(128, (3,3), kernel_initializer='he_normal', input_shape=(28,28,3), strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(256, (3,3), kernel_initializer='he_normal', strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(2,2)) \n",
    "    \n",
    "    model.add(Conv2D(32, (3,3), kernel_initializer='he_normal', input_shape=(28,28,3), strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(64, (3,3), kernel_initializer='he_normal', strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(2,2)) \n",
    "    \n",
    "    model.add(Conv2D(32, (3,3), kernel_initializer='he_normal', input_shape=(28,28,3), strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(64, (3,3), kernel_initializer='he_normal', strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(2,2)) \n",
    "    \n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(128, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(64, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))    \n",
    "    \n",
    "    model.add(Dense(32, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    adam = tf.keras.optimizers.Adam(lr = 0.001) \n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model_cnn_batch_07()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34300 samples, validate on 14700 samples\n",
      "Epoch 1/150\n",
      "34300/34300 [==============================] - 49s 1ms/step - loss: 2.1509 - accuracy: 0.2273 - val_loss: 1.5043 - val_accuracy: 0.6087\n",
      "Epoch 2/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 1.5659 - accuracy: 0.4225 - val_loss: 0.9803 - val_accuracy: 0.7497\n",
      "Epoch 3/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 1.2198 - accuracy: 0.5373 - val_loss: 0.7239 - val_accuracy: 0.7422\n",
      "Epoch 4/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 1.0228 - accuracy: 0.6045 - val_loss: 0.4428 - val_accuracy: 0.8832\n",
      "Epoch 5/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.8961 - accuracy: 0.6489 - val_loss: 2.0536 - val_accuracy: 0.5308\n",
      "Epoch 6/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.8088 - accuracy: 0.6875 - val_loss: 0.3826 - val_accuracy: 0.8640\n",
      "Epoch 7/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.7463 - accuracy: 0.7083 - val_loss: 0.3369 - val_accuracy: 0.8696\n",
      "Epoch 8/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.6996 - accuracy: 0.7228 - val_loss: 0.2173 - val_accuracy: 0.8918\n",
      "Epoch 9/150\n",
      "34300/34300 [==============================] - 45s 1ms/step - loss: 0.6614 - accuracy: 0.7382 - val_loss: 0.2277 - val_accuracy: 0.8848\n",
      "Epoch 10/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.6488 - accuracy: 0.7396 - val_loss: 0.2754 - val_accuracy: 0.8761\n",
      "Epoch 11/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.6158 - accuracy: 0.7523 - val_loss: 0.2016 - val_accuracy: 0.8918\n",
      "Epoch 12/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.6065 - accuracy: 0.7503 - val_loss: 0.2003 - val_accuracy: 0.8895\n",
      "Epoch 13/150\n",
      "34300/34300 [==============================] - 45s 1ms/step - loss: 0.5891 - accuracy: 0.7555 - val_loss: 0.2962 - val_accuracy: 0.8718\n",
      "Epoch 14/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.5795 - accuracy: 0.7628 - val_loss: 0.2248 - val_accuracy: 0.8943\n",
      "Epoch 15/150\n",
      "34300/34300 [==============================] - 47s 1ms/step - loss: 0.5682 - accuracy: 0.7640 - val_loss: 0.3440 - val_accuracy: 0.8675\n",
      "Epoch 16/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.5572 - accuracy: 0.7683 - val_loss: 0.2160 - val_accuracy: 0.8895\n",
      "Epoch 17/150\n",
      "34300/34300 [==============================] - 45s 1ms/step - loss: 0.5656 - accuracy: 0.7640 - val_loss: 0.1948 - val_accuracy: 0.9393\n",
      "Epoch 18/150\n",
      "34300/34300 [==============================] - 45s 1ms/step - loss: 0.5594 - accuracy: 0.7694 - val_loss: 0.2400 - val_accuracy: 0.9323\n",
      "Epoch 19/150\n",
      "34300/34300 [==============================] - 45s 1ms/step - loss: 0.5511 - accuracy: 0.7740 - val_loss: 0.1968 - val_accuracy: 0.8947\n",
      "Epoch 20/150\n",
      "34300/34300 [==============================] - 45s 1ms/step - loss: 0.5337 - accuracy: 0.7737 - val_loss: 0.1829 - val_accuracy: 0.9309\n",
      "Epoch 21/150\n",
      "34300/34300 [==============================] - 45s 1ms/step - loss: 0.5333 - accuracy: 0.7826 - val_loss: 0.2478 - val_accuracy: 0.9084\n",
      "Epoch 22/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.5256 - accuracy: 0.7913 - val_loss: 0.2009 - val_accuracy: 0.9752\n",
      "Epoch 23/150\n",
      "21888/34300 [==================>...........] - ETA: 14s - loss: 0.5182 - accuracy: 0.7987"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3733\u001b[0m     return nest.pack_sequence_as(\n\u001b[0;32m   3734\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_outputs_structure\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3735\u001b[1;33m         \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3736\u001b[0m         expand_composites=True)\n\u001b[0;32m   3737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   3733\u001b[0m     return nest.pack_sequence_as(\n\u001b[0;32m   3734\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_outputs_structure\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3735\u001b[1;33m         \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3736\u001b[0m         expand_composites=True)\n\u001b[0;32m   3737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    906\u001b[0m     \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 908\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    909\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m       \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history7 = model.fit(X_train, y_train, batch_size=128, epochs=150, verbose=1,\n",
    "         validation_split = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(X_test_sub)\n",
    "y_pred = predicted.argmax(axis=1)\n",
    "\n",
    "Out = pd.DataFrame([test['filename'],y_pred]).T\n",
    "Out.rename(columns = {0:'filename', 1:'label'})\n",
    "#Out\n",
    "Out.to_csv('MNIST_DropOut_2LayerCNN_02he_normal_Adam17.csv', header =  ['filename', 'label' ], index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_cnn_batch_08():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (4,4), kernel_initializer='he_normal', input_shape=(28,28,3), strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(64, (4,4), kernel_initializer='he_normal', strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(128, (4,4), kernel_initializer='he_normal', input_shape=(28,28,3), strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(256, (4,4), kernel_initializer='he_normal', strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(2,2))    \n",
    "    \n",
    "    model.add(Conv2D(32, (4,4), kernel_initializer='he_normal', input_shape=(28,28,3), strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(64, (4,4), kernel_initializer='he_normal', strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(128, (4,4), kernel_initializer='he_normal', input_shape=(28,28,3), strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(256, (4,4), kernel_initializer='he_normal', strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(2,2)) \n",
    "    \n",
    "    model.add(Conv2D(32, (3,3), kernel_initializer='he_normal', input_shape=(28,28,3), strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(64, (3,3), kernel_initializer='he_normal', strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(2,2)) \n",
    "    \n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(64, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(32, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    adam = tf.keras.optimizers.Adam(lr = 0.001) \n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_cnn_batch_09():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (4,4), kernel_initializer='he_normal', input_shape=(28,28,3), strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(64, (4,4), kernel_initializer='he_normal', strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(128, (4,4), kernel_initializer='he_normal', input_shape=(28,28,3), strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(256, (4,4), kernel_initializer='he_normal', strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(2,2))    \n",
    "    \n",
    "    model.add(Conv2D(32, (4,4), kernel_initializer='he_normal', input_shape=(28,28,3), strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(64, (4,4), kernel_initializer='he_normal', strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(128, (4,4), kernel_initializer='he_normal', input_shape=(28,28,3), strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(256, (4,4), kernel_initializer='he_normal', strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(2,2)) \n",
    "    \n",
    "    model.add(Conv2D(32, (3,3), kernel_initializer='he_normal', input_shape=(28,28,3), strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(64, (3,3), kernel_initializer='he_normal', strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(2,2)) \n",
    "    \n",
    "    model.add(Conv2D(32, (3,3), kernel_initializer='he_normal', input_shape=(28,28,3), strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(64, (3,3), kernel_initializer='he_normal', strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(2,2)) \n",
    "    \n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(128, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(64, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))    \n",
    "    \n",
    "    model.add(Dense(32, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    adam = tf.keras.optimizers.Adam(lr = 0.001) \n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_model_cnn_batch_08' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-07d224e22800>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_model_cnn_batch_08\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'get_model_cnn_batch_08' is not defined"
     ]
    }
   ],
   "source": [
    "model = get_model_cnn_batch_08()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34300 samples, validate on 14700 samples\n",
      "Epoch 1/150\n",
      "34300/34300 [==============================] - 48s 1ms/step - loss: 1.0478 - accuracy: 0.6930 - val_loss: 0.2148 - val_accuracy: 0.9669\n",
      "Epoch 2/150\n",
      "34300/34300 [==============================] - 42s 1ms/step - loss: 0.3824 - accuracy: 0.9035 - val_loss: 0.0893 - val_accuracy: 0.9757\n",
      "Epoch 3/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.2474 - accuracy: 0.9355 - val_loss: 0.0716 - val_accuracy: 0.9803\n",
      "Epoch 4/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.2042 - accuracy: 0.9466 - val_loss: 0.0869 - val_accuracy: 0.9764\n",
      "Epoch 5/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.1690 - accuracy: 0.9552 - val_loss: 0.0922 - val_accuracy: 0.9776\n",
      "Epoch 6/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.1574 - accuracy: 0.9586 - val_loss: 0.0480 - val_accuracy: 0.9890\n",
      "Epoch 7/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.1368 - accuracy: 0.9642 - val_loss: 0.0517 - val_accuracy: 0.9876\n",
      "Epoch 8/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.1245 - accuracy: 0.9661 - val_loss: 0.0719 - val_accuracy: 0.9837\n",
      "Epoch 9/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.1142 - accuracy: 0.9699 - val_loss: 0.0755 - val_accuracy: 0.9807\n",
      "Epoch 10/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.1106 - accuracy: 0.9711 - val_loss: 0.0513 - val_accuracy: 0.9895\n",
      "Epoch 11/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0992 - accuracy: 0.9731 - val_loss: 0.0508 - val_accuracy: 0.9897\n",
      "Epoch 12/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0941 - accuracy: 0.9748 - val_loss: 0.2151 - val_accuracy: 0.9589\n",
      "Epoch 13/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.1009 - accuracy: 0.9723 - val_loss: 0.0735 - val_accuracy: 0.9863\n",
      "Epoch 14/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0871 - accuracy: 0.9776 - val_loss: 0.0849 - val_accuracy: 0.9844\n",
      "Epoch 15/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0863 - accuracy: 0.9764 - val_loss: 0.0360 - val_accuracy: 0.9927\n",
      "Epoch 16/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0816 - accuracy: 0.9783 - val_loss: 0.0695 - val_accuracy: 0.9871\n",
      "Epoch 17/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0809 - accuracy: 0.9773 - val_loss: 0.0597 - val_accuracy: 0.9872\n",
      "Epoch 18/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0735 - accuracy: 0.9792 - val_loss: 0.0432 - val_accuracy: 0.9931\n",
      "Epoch 19/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0755 - accuracy: 0.9798 - val_loss: 0.0583 - val_accuracy: 0.9900\n",
      "Epoch 20/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0709 - accuracy: 0.9799 - val_loss: 0.0727 - val_accuracy: 0.9839\n",
      "Epoch 21/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0673 - accuracy: 0.9808 - val_loss: 0.0635 - val_accuracy: 0.9885\n",
      "Epoch 22/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0679 - accuracy: 0.9801 - val_loss: 0.0478 - val_accuracy: 0.9917\n",
      "Epoch 23/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0577 - accuracy: 0.9820 - val_loss: 0.0531 - val_accuracy: 0.9914\n",
      "Epoch 24/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0622 - accuracy: 0.9811 - val_loss: 0.0584 - val_accuracy: 0.9924\n",
      "Epoch 25/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0591 - accuracy: 0.9823 - val_loss: 0.0917 - val_accuracy: 0.9852\n",
      "Epoch 26/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0562 - accuracy: 0.9831 - val_loss: 0.0439 - val_accuracy: 0.9933\n",
      "Epoch 27/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0662 - accuracy: 0.9813 - val_loss: 0.0432 - val_accuracy: 0.9920\n",
      "Epoch 28/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0539 - accuracy: 0.9838 - val_loss: 0.0616 - val_accuracy: 0.9903\n",
      "Epoch 29/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0604 - accuracy: 0.9820 - val_loss: 0.0431 - val_accuracy: 0.9933\n",
      "Epoch 30/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0539 - accuracy: 0.9838 - val_loss: 0.0539 - val_accuracy: 0.9912\n",
      "Epoch 31/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0539 - accuracy: 0.9836 - val_loss: 0.0552 - val_accuracy: 0.9916\n",
      "Epoch 32/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0512 - accuracy: 0.9834 - val_loss: 0.0500 - val_accuracy: 0.9924\n",
      "Epoch 33/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0555 - accuracy: 0.9838 - val_loss: 0.0478 - val_accuracy: 0.9923\n",
      "Epoch 34/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0468 - accuracy: 0.9845 - val_loss: 0.0740 - val_accuracy: 0.9875\n",
      "Epoch 35/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0458 - accuracy: 0.9848 - val_loss: 0.0970 - val_accuracy: 0.9871\n",
      "Epoch 36/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0566 - accuracy: 0.9834 - val_loss: 0.0521 - val_accuracy: 0.9921\n",
      "Epoch 37/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0537 - accuracy: 0.9844 - val_loss: 0.0389 - val_accuracy: 0.9939\n",
      "Epoch 38/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0442 - accuracy: 0.9860 - val_loss: 0.0687 - val_accuracy: 0.9907\n",
      "Epoch 39/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0428 - accuracy: 0.9857 - val_loss: 0.0772 - val_accuracy: 0.9905\n",
      "Epoch 40/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0528 - accuracy: 0.9845 - val_loss: 0.0732 - val_accuracy: 0.9889\n",
      "Epoch 41/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0417 - accuracy: 0.9856 - val_loss: 0.0500 - val_accuracy: 0.9929\n",
      "Epoch 42/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0503 - accuracy: 0.9851 - val_loss: 0.0971 - val_accuracy: 0.9858\n",
      "Epoch 43/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0478 - accuracy: 0.9858 - val_loss: 0.0661 - val_accuracy: 0.9890\n",
      "Epoch 44/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0441 - accuracy: 0.9863 - val_loss: 0.0643 - val_accuracy: 0.9922\n",
      "Epoch 45/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0418 - accuracy: 0.9866 - val_loss: 0.0509 - val_accuracy: 0.9931\n",
      "Epoch 46/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0399 - accuracy: 0.9873 - val_loss: 0.0780 - val_accuracy: 0.9910\n",
      "Epoch 47/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0463 - accuracy: 0.9854 - val_loss: 0.0538 - val_accuracy: 0.9928\n",
      "Epoch 48/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0438 - accuracy: 0.9871 - val_loss: 0.0471 - val_accuracy: 0.9937\n",
      "Epoch 49/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0381 - accuracy: 0.9875 - val_loss: 0.0567 - val_accuracy: 0.9921\n",
      "Epoch 50/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0376 - accuracy: 0.9875 - val_loss: 0.0735 - val_accuracy: 0.9914\n",
      "Epoch 51/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0394 - accuracy: 0.9869 - val_loss: 0.0682 - val_accuracy: 0.9928\n",
      "Epoch 52/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0397 - accuracy: 0.9873 - val_loss: 0.0505 - val_accuracy: 0.9938\n",
      "Epoch 53/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0395 - accuracy: 0.9875 - val_loss: 0.0652 - val_accuracy: 0.9926\n",
      "Epoch 54/150\n",
      "34300/34300 [==============================] - 1230s 36ms/step - loss: 0.0388 - accuracy: 0.9872 - val_loss: 0.0971 - val_accuracy: 0.9897\n",
      "Epoch 55/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0413 - accuracy: 0.9868 - val_loss: 0.0702 - val_accuracy: 0.9924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0395 - accuracy: 0.9873 - val_loss: 0.0521 - val_accuracy: 0.9943\n",
      "Epoch 57/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0313 - accuracy: 0.9892 - val_loss: 0.0674 - val_accuracy: 0.9920\n",
      "Epoch 58/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0358 - accuracy: 0.9876 - val_loss: 0.0691 - val_accuracy: 0.9932\n",
      "Epoch 59/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0319 - accuracy: 0.9880 - val_loss: 0.0735 - val_accuracy: 0.9917\n",
      "Epoch 60/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0386 - accuracy: 0.9875 - val_loss: 0.0725 - val_accuracy: 0.9932\n",
      "Epoch 61/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0423 - accuracy: 0.9874 - val_loss: 0.0604 - val_accuracy: 0.9916\n",
      "Epoch 62/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0376 - accuracy: 0.9879 - val_loss: 0.0573 - val_accuracy: 0.9925\n",
      "Epoch 63/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0386 - accuracy: 0.9874 - val_loss: 0.1103 - val_accuracy: 0.9871\n",
      "Epoch 64/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0412 - accuracy: 0.9873 - val_loss: 0.0778 - val_accuracy: 0.9901\n",
      "Epoch 65/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0354 - accuracy: 0.9876 - val_loss: 0.0853 - val_accuracy: 0.9899\n",
      "Epoch 66/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0372 - accuracy: 0.9881 - val_loss: 0.0691 - val_accuracy: 0.9925\n",
      "Epoch 67/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0344 - accuracy: 0.9883 - val_loss: 0.0574 - val_accuracy: 0.9939\n",
      "Epoch 68/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0285 - accuracy: 0.9896 - val_loss: 0.0541 - val_accuracy: 0.9938\n",
      "Epoch 69/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0337 - accuracy: 0.9887 - val_loss: 0.0550 - val_accuracy: 0.9942\n",
      "Epoch 70/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0301 - accuracy: 0.9894 - val_loss: 0.0534 - val_accuracy: 0.9945\n",
      "Epoch 71/150\n",
      "34300/34300 [==============================] - 45s 1ms/step - loss: 0.0317 - accuracy: 0.9883 - val_loss: 0.0653 - val_accuracy: 0.9929\n",
      "Epoch 72/150\n",
      "34300/34300 [==============================] - 45s 1ms/step - loss: 0.0425 - accuracy: 0.9873 - val_loss: 0.1233 - val_accuracy: 0.9868\n",
      "Epoch 73/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0365 - accuracy: 0.9878 - val_loss: 0.0601 - val_accuracy: 0.9929\n",
      "Epoch 74/150\n",
      "34300/34300 [==============================] - 50s 1ms/step - loss: 0.0343 - accuracy: 0.9883 - val_loss: 0.0680 - val_accuracy: 0.9927\n",
      "Epoch 75/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0320 - accuracy: 0.9892 - val_loss: 0.0562 - val_accuracy: 0.9945\n",
      "Epoch 76/150\n",
      "34300/34300 [==============================] - 45s 1ms/step - loss: 0.0298 - accuracy: 0.9897 - val_loss: 0.0596 - val_accuracy: 0.9941\n",
      "Epoch 77/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0274 - accuracy: 0.9897 - val_loss: 0.0600 - val_accuracy: 0.9952\n",
      "Epoch 78/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0265 - accuracy: 0.9903 - val_loss: 0.0573 - val_accuracy: 0.9953\n",
      "Epoch 79/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0354 - accuracy: 0.9877 - val_loss: 0.0633 - val_accuracy: 0.9940\n",
      "Epoch 80/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0397 - accuracy: 0.9878 - val_loss: 0.0696 - val_accuracy: 0.9920\n",
      "Epoch 81/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0354 - accuracy: 0.9876 - val_loss: 0.0618 - val_accuracy: 0.9922\n",
      "Epoch 82/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0352 - accuracy: 0.9886 - val_loss: 0.0519 - val_accuracy: 0.9937\n",
      "Epoch 83/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0261 - accuracy: 0.9906 - val_loss: 0.0529 - val_accuracy: 0.9950\n",
      "Epoch 84/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0298 - accuracy: 0.9888 - val_loss: 0.0591 - val_accuracy: 0.9947\n",
      "Epoch 85/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0291 - accuracy: 0.9892 - val_loss: 0.0738 - val_accuracy: 0.9929\n",
      "Epoch 86/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0260 - accuracy: 0.9908 - val_loss: 0.0574 - val_accuracy: 0.9950\n",
      "Epoch 87/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0320 - accuracy: 0.9888 - val_loss: 0.0941 - val_accuracy: 0.9910\n",
      "Epoch 88/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0353 - accuracy: 0.9884 - val_loss: 0.0622 - val_accuracy: 0.9935\n",
      "Epoch 89/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0270 - accuracy: 0.9898 - val_loss: 0.0724 - val_accuracy: 0.9933\n",
      "Epoch 90/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0346 - accuracy: 0.9892 - val_loss: 0.0779 - val_accuracy: 0.9931\n",
      "Epoch 91/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0335 - accuracy: 0.9887 - val_loss: 0.0992 - val_accuracy: 0.9907\n",
      "Epoch 92/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0303 - accuracy: 0.9890 - val_loss: 0.0610 - val_accuracy: 0.9941\n",
      "Epoch 93/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0309 - accuracy: 0.9894 - val_loss: 0.0661 - val_accuracy: 0.9943\n",
      "Epoch 94/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0270 - accuracy: 0.9897 - val_loss: 0.0718 - val_accuracy: 0.9939\n",
      "Epoch 95/150\n",
      "34300/34300 [==============================] - 45s 1ms/step - loss: 0.0281 - accuracy: 0.9908 - val_loss: 0.0653 - val_accuracy: 0.9940\n",
      "Epoch 96/150\n",
      "34300/34300 [==============================] - 45s 1ms/step - loss: 0.0265 - accuracy: 0.9897 - val_loss: 0.0854 - val_accuracy: 0.9931\n",
      "Epoch 97/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0258 - accuracy: 0.9907 - val_loss: 0.0733 - val_accuracy: 0.9946\n",
      "Epoch 98/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0266 - accuracy: 0.9902 - val_loss: 0.0751 - val_accuracy: 0.9944\n",
      "Epoch 99/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0333 - accuracy: 0.9892 - val_loss: 0.1052 - val_accuracy: 0.9927\n",
      "Epoch 100/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0274 - accuracy: 0.9911 - val_loss: 0.0764 - val_accuracy: 0.9936\n",
      "Epoch 101/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0336 - accuracy: 0.9899 - val_loss: 0.0765 - val_accuracy: 0.9935\n",
      "Epoch 102/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0348 - accuracy: 0.9886 - val_loss: 0.0780 - val_accuracy: 0.9929\n",
      "Epoch 103/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0300 - accuracy: 0.9897 - val_loss: 0.0712 - val_accuracy: 0.9937\n",
      "Epoch 104/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0274 - accuracy: 0.9905 - val_loss: 0.0869 - val_accuracy: 0.9938\n",
      "Epoch 105/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0259 - accuracy: 0.9904 - val_loss: 0.0652 - val_accuracy: 0.9948\n",
      "Epoch 106/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0242 - accuracy: 0.9915 - val_loss: 0.0708 - val_accuracy: 0.9941\n",
      "Epoch 107/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0238 - accuracy: 0.9914 - val_loss: 0.0743 - val_accuracy: 0.9945\n",
      "Epoch 108/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0231 - accuracy: 0.9914 - val_loss: 0.0721 - val_accuracy: 0.9950\n",
      "Epoch 109/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0254 - accuracy: 0.9913 - val_loss: 0.0936 - val_accuracy: 0.9929\n",
      "Epoch 110/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0425 - accuracy: 0.9886 - val_loss: 0.0733 - val_accuracy: 0.9935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0364 - accuracy: 0.9889 - val_loss: 0.0674 - val_accuracy: 0.9936\n",
      "Epoch 112/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0319 - accuracy: 0.9891 - val_loss: 0.0708 - val_accuracy: 0.9937\n",
      "Epoch 113/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0287 - accuracy: 0.9899 - val_loss: 0.0664 - val_accuracy: 0.9931\n",
      "Epoch 114/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0257 - accuracy: 0.9912 - val_loss: 0.0731 - val_accuracy: 0.9938\n",
      "Epoch 115/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0243 - accuracy: 0.9912 - val_loss: 0.0862 - val_accuracy: 0.9927\n",
      "Epoch 116/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0276 - accuracy: 0.9898 - val_loss: 0.0689 - val_accuracy: 0.9948\n",
      "Epoch 117/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0266 - accuracy: 0.9898 - val_loss: 0.0731 - val_accuracy: 0.9946\n",
      "Epoch 118/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0247 - accuracy: 0.9906 - val_loss: 0.0701 - val_accuracy: 0.9945\n",
      "Epoch 119/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0228 - accuracy: 0.9908 - val_loss: 0.0701 - val_accuracy: 0.9949\n",
      "Epoch 120/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0234 - accuracy: 0.9908 - val_loss: 0.0737 - val_accuracy: 0.9943\n",
      "Epoch 121/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0327 - accuracy: 0.9890 - val_loss: 0.1022 - val_accuracy: 0.9919\n",
      "Epoch 122/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0316 - accuracy: 0.9898 - val_loss: 0.0813 - val_accuracy: 0.9924\n",
      "Epoch 123/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0300 - accuracy: 0.9898 - val_loss: 0.0799 - val_accuracy: 0.9934\n",
      "Epoch 124/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0254 - accuracy: 0.9910 - val_loss: 0.0947 - val_accuracy: 0.9922\n",
      "Epoch 125/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0253 - accuracy: 0.9907 - val_loss: 0.0826 - val_accuracy: 0.9937\n",
      "Epoch 126/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0336 - accuracy: 0.9894 - val_loss: 0.0890 - val_accuracy: 0.9924\n",
      "Epoch 127/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0282 - accuracy: 0.9899 - val_loss: 0.0687 - val_accuracy: 0.9945\n",
      "Epoch 128/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0253 - accuracy: 0.9911 - val_loss: 0.0780 - val_accuracy: 0.9944\n",
      "Epoch 129/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0228 - accuracy: 0.9921 - val_loss: 0.0783 - val_accuracy: 0.9944\n",
      "Epoch 130/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0242 - accuracy: 0.9910 - val_loss: 0.0749 - val_accuracy: 0.9942\n",
      "Epoch 131/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0317 - accuracy: 0.9903 - val_loss: 0.1111 - val_accuracy: 0.9920\n",
      "Epoch 132/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0239 - accuracy: 0.9913 - val_loss: 0.0771 - val_accuracy: 0.9947\n",
      "Epoch 133/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0247 - accuracy: 0.9908 - val_loss: 0.0823 - val_accuracy: 0.9941\n",
      "Epoch 134/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0223 - accuracy: 0.9917 - val_loss: 0.0827 - val_accuracy: 0.9937\n",
      "Epoch 135/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0263 - accuracy: 0.9903 - val_loss: 0.0991 - val_accuracy: 0.9935\n",
      "Epoch 136/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0360 - accuracy: 0.9896 - val_loss: 0.1011 - val_accuracy: 0.9928\n",
      "Epoch 137/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0274 - accuracy: 0.9910 - val_loss: 0.0787 - val_accuracy: 0.9939\n",
      "Epoch 138/150\n",
      "34300/34300 [==============================] - 45s 1ms/step - loss: 0.0221 - accuracy: 0.9918 - val_loss: 0.0758 - val_accuracy: 0.9944\n",
      "Epoch 139/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0218 - accuracy: 0.9917 - val_loss: 0.0834 - val_accuracy: 0.9944\n",
      "Epoch 140/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0232 - accuracy: 0.9913 - val_loss: 0.0795 - val_accuracy: 0.9950\n",
      "Epoch 141/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0286 - accuracy: 0.9911 - val_loss: 0.1332 - val_accuracy: 0.9886\n",
      "Epoch 142/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0327 - accuracy: 0.9900 - val_loss: 0.1090 - val_accuracy: 0.9880\n",
      "Epoch 143/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0311 - accuracy: 0.9895 - val_loss: 0.0875 - val_accuracy: 0.9940\n",
      "Epoch 144/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0208 - accuracy: 0.9925 - val_loss: 0.0760 - val_accuracy: 0.9950\n",
      "Epoch 145/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0237 - accuracy: 0.9912 - val_loss: 0.0819 - val_accuracy: 0.9949\n",
      "Epoch 146/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0247 - accuracy: 0.9912 - val_loss: 0.0786 - val_accuracy: 0.9945\n",
      "Epoch 147/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0244 - accuracy: 0.9911 - val_loss: 0.0764 - val_accuracy: 0.9947\n",
      "Epoch 148/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0258 - accuracy: 0.9918 - val_loss: 0.0776 - val_accuracy: 0.9941\n",
      "Epoch 149/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0200 - accuracy: 0.9926 - val_loss: 0.0820 - val_accuracy: 0.9946\n",
      "Epoch 150/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.0224 - accuracy: 0.9910 - val_loss: 0.0802 - val_accuracy: 0.9941\n",
      "Wall time: 2h 8min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history7 = model.fit(X_train, y_train, batch_size=128, epochs=150, verbose=1,\n",
    "         validation_split = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aasha\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\keras\\engine\\saving.py:165: UserWarning: TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "  'TensorFlow optimizers do not '\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "#Step 1: Create or open a file with write-binary mode and save the model to it\n",
    "filenm = 'MNIST_DropOut_2LayerCNN_02he_normal_Adam18.pickle'\n",
    "\n",
    "#Step 2: Open the saved file with read-binary mode\n",
    "pickle = pkl.dump(model, open(filenm, 'wb'))\n",
    "\n",
    "predicted = model.predict(X_test_sub)\n",
    "y_pred = predicted.argmax(axis=1)\n",
    "\n",
    "Out = pd.DataFrame([test['filename'],y_pred]).T\n",
    "Out.rename(columns = {0:'filename', 1:'label'})\n",
    "\n",
    "#Out\n",
    "Out.to_csv('MNIST_DropOut_2LayerCNN_02he_normal_Adam18.csv', header =  ['filename', 'label' ], index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model_cnn_batch_09()\n",
    "#with 09 its 99.45 in AV Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34300 samples, validate on 14700 samples\n",
      "Epoch 1/150\n",
      "34300/34300 [==============================] - 52s 2ms/step - loss: 1.8705 - accuracy: 0.3723 - val_loss: 1.9328 - val_accuracy: 0.3402\n",
      "Epoch 2/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.8061 - accuracy: 0.7670 - val_loss: 0.7687 - val_accuracy: 0.8028\n",
      "Epoch 3/150\n",
      "34300/34300 [==============================] - 45s 1ms/step - loss: 0.4718 - accuracy: 0.8725 - val_loss: 0.1165 - val_accuracy: 0.9750\n",
      "Epoch 4/150\n",
      "34300/34300 [==============================] - 45s 1ms/step - loss: 0.3433 - accuracy: 0.9090 - val_loss: 0.3700 - val_accuracy: 0.9188\n",
      "Epoch 5/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.2709 - accuracy: 0.9309 - val_loss: 0.2476 - val_accuracy: 0.9476\n",
      "Epoch 6/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.2391 - accuracy: 0.9395 - val_loss: 0.2289 - val_accuracy: 0.9564\n",
      "Epoch 7/150\n",
      "34300/34300 [==============================] - 43s 1ms/step - loss: 0.1956 - accuracy: 0.9506 - val_loss: 0.2050 - val_accuracy: 0.9629\n",
      "Epoch 8/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.1780 - accuracy: 0.9566 - val_loss: 0.1094 - val_accuracy: 0.9768\n",
      "Epoch 9/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.1657 - accuracy: 0.9615 - val_loss: 0.1631 - val_accuracy: 0.9725\n",
      "Epoch 10/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.1520 - accuracy: 0.9631 - val_loss: 0.0637 - val_accuracy: 0.9878\n",
      "Epoch 11/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.1429 - accuracy: 0.9656 - val_loss: 0.0766 - val_accuracy: 0.9859\n",
      "Epoch 12/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.1375 - accuracy: 0.9685 - val_loss: 0.0657 - val_accuracy: 0.9880\n",
      "Epoch 13/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.1269 - accuracy: 0.9706 - val_loss: 0.1166 - val_accuracy: 0.9796\n",
      "Epoch 14/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.1178 - accuracy: 0.9710 - val_loss: 0.0849 - val_accuracy: 0.9862\n",
      "Epoch 15/150\n",
      "34300/34300 [==============================] - 45s 1ms/step - loss: 0.1131 - accuracy: 0.9722 - val_loss: 0.0657 - val_accuracy: 0.9878\n",
      "Epoch 16/150\n",
      "34300/34300 [==============================] - 45s 1ms/step - loss: 0.1070 - accuracy: 0.9744 - val_loss: 0.0825 - val_accuracy: 0.9869\n",
      "Epoch 17/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.1064 - accuracy: 0.9753 - val_loss: 0.0985 - val_accuracy: 0.9852\n",
      "Epoch 18/150\n",
      "34300/34300 [==============================] - 45s 1ms/step - loss: 0.1026 - accuracy: 0.9752 - val_loss: 0.0800 - val_accuracy: 0.9873\n",
      "Epoch 19/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0916 - accuracy: 0.9772 - val_loss: 0.0722 - val_accuracy: 0.9890\n",
      "Epoch 20/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0912 - accuracy: 0.9779 - val_loss: 0.0656 - val_accuracy: 0.9906\n",
      "Epoch 21/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0912 - accuracy: 0.9783 - val_loss: 0.0801 - val_accuracy: 0.9858\n",
      "Epoch 22/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0874 - accuracy: 0.9793 - val_loss: 0.1026 - val_accuracy: 0.9830\n",
      "Epoch 23/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0825 - accuracy: 0.9796 - val_loss: 0.0893 - val_accuracy: 0.9810\n",
      "Epoch 24/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0866 - accuracy: 0.9797 - val_loss: 0.1810 - val_accuracy: 0.9699\n",
      "Epoch 25/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0752 - accuracy: 0.9818 - val_loss: 0.1037 - val_accuracy: 0.9855\n",
      "Epoch 26/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0801 - accuracy: 0.9803 - val_loss: 0.1109 - val_accuracy: 0.9824\n",
      "Epoch 27/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0781 - accuracy: 0.9810 - val_loss: 0.0848 - val_accuracy: 0.9890\n",
      "Epoch 28/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0769 - accuracy: 0.9806 - val_loss: 0.0655 - val_accuracy: 0.9912\n",
      "Epoch 29/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0744 - accuracy: 0.9817 - val_loss: 0.1176 - val_accuracy: 0.9852\n",
      "Epoch 30/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0764 - accuracy: 0.9811 - val_loss: 0.0684 - val_accuracy: 0.9905\n",
      "Epoch 31/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0646 - accuracy: 0.9829 - val_loss: 0.0573 - val_accuracy: 0.9926\n",
      "Epoch 32/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0623 - accuracy: 0.9836 - val_loss: 0.0610 - val_accuracy: 0.9919\n",
      "Epoch 33/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0731 - accuracy: 0.9821 - val_loss: 0.1387 - val_accuracy: 0.9827\n",
      "Epoch 34/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0746 - accuracy: 0.9813 - val_loss: 0.0626 - val_accuracy: 0.9916\n",
      "Epoch 35/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0564 - accuracy: 0.9859 - val_loss: 0.1264 - val_accuracy: 0.9832\n",
      "Epoch 36/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0558 - accuracy: 0.9853 - val_loss: 0.0610 - val_accuracy: 0.9932\n",
      "Epoch 37/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0705 - accuracy: 0.9822 - val_loss: 0.0776 - val_accuracy: 0.9914\n",
      "Epoch 38/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0688 - accuracy: 0.9831 - val_loss: 0.1668 - val_accuracy: 0.9799\n",
      "Epoch 39/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0670 - accuracy: 0.9839 - val_loss: 0.0841 - val_accuracy: 0.9862\n",
      "Epoch 40/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0498 - accuracy: 0.9856 - val_loss: 0.0780 - val_accuracy: 0.9892\n",
      "Epoch 41/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0544 - accuracy: 0.9852 - val_loss: 0.0738 - val_accuracy: 0.9919\n",
      "Epoch 42/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0495 - accuracy: 0.9865 - val_loss: 0.0584 - val_accuracy: 0.9931\n",
      "Epoch 43/150\n",
      "34300/34300 [==============================] - 45s 1ms/step - loss: 0.0543 - accuracy: 0.9859 - val_loss: 0.0839 - val_accuracy: 0.9908\n",
      "Epoch 44/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0544 - accuracy: 0.9844 - val_loss: 0.0562 - val_accuracy: 0.9933\n",
      "Epoch 45/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0577 - accuracy: 0.9851 - val_loss: 0.0875 - val_accuracy: 0.9895\n",
      "Epoch 46/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0755 - accuracy: 0.9822 - val_loss: 0.1071 - val_accuracy: 0.9856\n",
      "Epoch 47/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0532 - accuracy: 0.9855 - val_loss: 0.0549 - val_accuracy: 0.9935\n",
      "Epoch 48/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0455 - accuracy: 0.9879 - val_loss: 0.0661 - val_accuracy: 0.9929\n",
      "Epoch 49/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0524 - accuracy: 0.9863 - val_loss: 0.0996 - val_accuracy: 0.9881\n",
      "Epoch 50/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0586 - accuracy: 0.9852 - val_loss: 0.0560 - val_accuracy: 0.9931\n",
      "Epoch 51/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0435 - accuracy: 0.9881 - val_loss: 0.0838 - val_accuracy: 0.9903\n",
      "Epoch 52/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0472 - accuracy: 0.9869 - val_loss: 0.0671 - val_accuracy: 0.9930\n",
      "Epoch 53/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0494 - accuracy: 0.9868 - val_loss: 0.0726 - val_accuracy: 0.9922\n",
      "Epoch 54/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0493 - accuracy: 0.9866 - val_loss: 0.0664 - val_accuracy: 0.9924\n",
      "Epoch 55/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0483 - accuracy: 0.9861 - val_loss: 0.1246 - val_accuracy: 0.9888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0451 - accuracy: 0.9875 - val_loss: 0.0715 - val_accuracy: 0.9922\n",
      "Epoch 57/150\n",
      "34300/34300 [==============================] - 48s 1ms/step - loss: 0.0668 - accuracy: 0.9839 - val_loss: 0.1289 - val_accuracy: 0.9859\n",
      "Epoch 58/150\n",
      "34300/34300 [==============================] - 50s 1ms/step - loss: 0.0574 - accuracy: 0.9861 - val_loss: 0.0754 - val_accuracy: 0.9927\n",
      "Epoch 59/150\n",
      "34300/34300 [==============================] - 57s 2ms/step - loss: 0.0493 - accuracy: 0.9866 - val_loss: 0.0813 - val_accuracy: 0.9912loss: 0.049 - ETA: 21s - loss: 0.0482 - accuracy: 0. - E - ETA: 2s - los\n",
      "Epoch 60/150\n",
      "34300/34300 [==============================] - 48s 1ms/step - loss: 0.0437 - accuracy: 0.9876 - val_loss: 0.0544 - val_accuracy: 0.9942\n",
      "Epoch 61/150\n",
      "34300/34300 [==============================] - 48s 1ms/step - loss: 0.0394 - accuracy: 0.9877 - val_loss: 0.0786 - val_accuracy: 0.9924\n",
      "Epoch 62/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0444 - accuracy: 0.9879 - val_loss: 0.0902 - val_accuracy: 0.9914\n",
      "Epoch 63/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0560 - accuracy: 0.9847 - val_loss: 0.0829 - val_accuracy: 0.9921\n",
      "Epoch 64/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0452 - accuracy: 0.9880 - val_loss: 0.0833 - val_accuracy: 0.9918\n",
      "Epoch 65/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0465 - accuracy: 0.9868 - val_loss: 0.0797 - val_accuracy: 0.9916\n",
      "Epoch 66/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0429 - accuracy: 0.9876 - val_loss: 0.0604 - val_accuracy: 0.9941\n",
      "Epoch 67/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0456 - accuracy: 0.9869 - val_loss: 0.1016 - val_accuracy: 0.9920\n",
      "Epoch 68/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0422 - accuracy: 0.9885 - val_loss: 0.1015 - val_accuracy: 0.9914\n",
      "Epoch 69/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0389 - accuracy: 0.9890 - val_loss: 0.0800 - val_accuracy: 0.9931\n",
      "Epoch 70/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0341 - accuracy: 0.9899 - val_loss: 0.0776 - val_accuracy: 0.9928\n",
      "Epoch 71/150\n",
      "34300/34300 [==============================] - 47s 1ms/step - loss: 0.0416 - accuracy: 0.9887 - val_loss: 0.0685 - val_accuracy: 0.9931\n",
      "Epoch 72/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0404 - accuracy: 0.9887 - val_loss: 0.0932 - val_accuracy: 0.9923\n",
      "Epoch 73/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0488 - accuracy: 0.9880 - val_loss: 0.0790 - val_accuracy: 0.9924\n",
      "Epoch 74/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0363 - accuracy: 0.9896 - val_loss: 0.0800 - val_accuracy: 0.9930\n",
      "Epoch 75/150\n",
      "34300/34300 [==============================] - 47s 1ms/step - loss: 0.0481 - accuracy: 0.9877 - val_loss: 0.0784 - val_accuracy: 0.9933\n",
      "Epoch 76/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0447 - accuracy: 0.9878 - val_loss: 0.0992 - val_accuracy: 0.9899\n",
      "Epoch 77/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0420 - accuracy: 0.9872 - val_loss: 0.0721 - val_accuracy: 0.9926\n",
      "Epoch 78/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0372 - accuracy: 0.9896 - val_loss: 0.1214 - val_accuracy: 0.9873\n",
      "Epoch 79/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0447 - accuracy: 0.9878 - val_loss: 0.0678 - val_accuracy: 0.9936\n",
      "Epoch 80/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0356 - accuracy: 0.9887 - val_loss: 0.0785 - val_accuracy: 0.9934\n",
      "Epoch 81/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0335 - accuracy: 0.9900 - val_loss: 0.0926 - val_accuracy: 0.9920\n",
      "Epoch 82/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0366 - accuracy: 0.9900 - val_loss: 0.0988 - val_accuracy: 0.9924\n",
      "Epoch 83/150\n",
      "34300/34300 [==============================] - 47s 1ms/step - loss: 0.0382 - accuracy: 0.9888 - val_loss: 0.0877 - val_accuracy: 0.9922\n",
      "Epoch 84/150\n",
      "34300/34300 [==============================] - 47s 1ms/step - loss: 0.0334 - accuracy: 0.9904 - val_loss: 0.0724 - val_accuracy: 0.9945\n",
      "Epoch 85/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0363 - accuracy: 0.9894 - val_loss: 0.0912 - val_accuracy: 0.9915\n",
      "Epoch 86/150\n",
      "34300/34300 [==============================] - 47s 1ms/step - loss: 0.0410 - accuracy: 0.9894 - val_loss: 0.0934 - val_accuracy: 0.9915\n",
      "Epoch 87/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0369 - accuracy: 0.9892 - val_loss: 0.1136 - val_accuracy: 0.9907\n",
      "Epoch 88/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0348 - accuracy: 0.9897 - val_loss: 0.0785 - val_accuracy: 0.9936\n",
      "Epoch 89/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0294 - accuracy: 0.9905 - val_loss: 0.0787 - val_accuracy: 0.9935\n",
      "Epoch 90/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0416 - accuracy: 0.9888 - val_loss: 0.1323 - val_accuracy: 0.9888\n",
      "Epoch 91/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0435 - accuracy: 0.9888 - val_loss: 0.0720 - val_accuracy: 0.9933\n",
      "Epoch 92/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0401 - accuracy: 0.9882 - val_loss: 0.0818 - val_accuracy: 0.9931\n",
      "Epoch 93/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0366 - accuracy: 0.9896 - val_loss: 0.1071 - val_accuracy: 0.9915\n",
      "Epoch 94/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0379 - accuracy: 0.9896 - val_loss: 0.0873 - val_accuracy: 0.9934\n",
      "Epoch 95/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0322 - accuracy: 0.9901 - val_loss: 0.0754 - val_accuracy: 0.9944\n",
      "Epoch 96/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0324 - accuracy: 0.9901 - val_loss: 0.0936 - val_accuracy: 0.9929\n",
      "Epoch 97/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0367 - accuracy: 0.9892 - val_loss: 0.1054 - val_accuracy: 0.9913\n",
      "Epoch 98/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0355 - accuracy: 0.9897 - val_loss: 0.0850 - val_accuracy: 0.9925\n",
      "Epoch 99/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0358 - accuracy: 0.9894 - val_loss: 0.0757 - val_accuracy: 0.9934\n",
      "Epoch 100/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0306 - accuracy: 0.9912 - val_loss: 0.0724 - val_accuracy: 0.9941\n",
      "Epoch 101/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0384 - accuracy: 0.9897 - val_loss: 0.0873 - val_accuracy: 0.9940\n",
      "Epoch 102/150\n",
      "34300/34300 [==============================] - 45s 1ms/step - loss: 0.0409 - accuracy: 0.9891 - val_loss: 0.0750 - val_accuracy: 0.9924\n",
      "Epoch 103/150\n",
      "34300/34300 [==============================] - 45s 1ms/step - loss: 0.0321 - accuracy: 0.9907 - val_loss: 0.0958 - val_accuracy: 0.9934\n",
      "Epoch 104/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0375 - accuracy: 0.9901 - val_loss: 0.1024 - val_accuracy: 0.9913\n",
      "Epoch 105/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0280 - accuracy: 0.9910 - val_loss: 0.0980 - val_accuracy: 0.9935\n",
      "Epoch 106/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0331 - accuracy: 0.9905 - val_loss: 0.0727 - val_accuracy: 0.9937\n",
      "Epoch 107/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0412 - accuracy: 0.9889 - val_loss: 0.1336 - val_accuracy: 0.9885\n",
      "Epoch 108/150\n",
      "34300/34300 [==============================] - 45s 1ms/step - loss: 0.0437 - accuracy: 0.9886 - val_loss: 0.0770 - val_accuracy: 0.9935\n",
      "Epoch 109/150\n",
      "34300/34300 [==============================] - 45s 1ms/step - loss: 0.0323 - accuracy: 0.9902 - val_loss: 0.1947 - val_accuracy: 0.9692\n",
      "Epoch 110/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0370 - accuracy: 0.9897 - val_loss: 0.0865 - val_accuracy: 0.9935\n",
      "Epoch 111/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0284 - accuracy: 0.9915 - val_loss: 0.0955 - val_accuracy: 0.9929\n",
      "Epoch 112/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0297 - accuracy: 0.9910 - val_loss: 0.1088 - val_accuracy: 0.9922\n",
      "Epoch 113/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0262 - accuracy: 0.9918 - val_loss: 0.0801 - val_accuracy: 0.9936\n",
      "Epoch 114/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0295 - accuracy: 0.9907 - val_loss: 0.0938 - val_accuracy: 0.9935\n",
      "Epoch 115/150\n",
      "34300/34300 [==============================] - 48s 1ms/step - loss: 0.0311 - accuracy: 0.9912 - val_loss: 0.1040 - val_accuracy: 0.9928\n",
      "Epoch 116/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0284 - accuracy: 0.9908 - val_loss: 0.1303 - val_accuracy: 0.9919\n",
      "Epoch 117/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0300 - accuracy: 0.9907 - val_loss: 0.1028 - val_accuracy: 0.9920\n",
      "Epoch 118/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0303 - accuracy: 0.9908 - val_loss: 0.0970 - val_accuracy: 0.9942\n",
      "Epoch 119/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0351 - accuracy: 0.9901 - val_loss: 0.1020 - val_accuracy: 0.9927\n",
      "Epoch 120/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0349 - accuracy: 0.9905 - val_loss: 0.1322 - val_accuracy: 0.9922\n",
      "Epoch 121/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0393 - accuracy: 0.9895 - val_loss: 0.1200 - val_accuracy: 0.9924\n",
      "Epoch 122/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0351 - accuracy: 0.9904 - val_loss: 0.1096 - val_accuracy: 0.9922\n",
      "Epoch 123/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0305 - accuracy: 0.9914 - val_loss: 0.1054 - val_accuracy: 0.9922\n",
      "Epoch 124/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0357 - accuracy: 0.9904 - val_loss: 0.1457 - val_accuracy: 0.9886\n",
      "Epoch 125/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0378 - accuracy: 0.9902 - val_loss: 0.0785 - val_accuracy: 0.9944\n",
      "Epoch 126/150\n",
      "34300/34300 [==============================] - 47s 1ms/step - loss: 0.0304 - accuracy: 0.9908 - val_loss: 0.1295 - val_accuracy: 0.9904\n",
      "Epoch 127/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0329 - accuracy: 0.9900 - val_loss: 0.1091 - val_accuracy: 0.9910\n",
      "Epoch 128/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0297 - accuracy: 0.9902 - val_loss: 0.1172 - val_accuracy: 0.9922\n",
      "Epoch 129/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0325 - accuracy: 0.9902 - val_loss: 0.0906 - val_accuracy: 0.9939\n",
      "Epoch 130/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0259 - accuracy: 0.9918 - val_loss: 0.1001 - val_accuracy: 0.9938\n",
      "Epoch 131/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0240 - accuracy: 0.9917 - val_loss: 0.0981 - val_accuracy: 0.9941\n",
      "Epoch 132/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0310 - accuracy: 0.9917 - val_loss: 0.1513 - val_accuracy: 0.9909\n",
      "Epoch 133/150\n",
      "34300/34300 [==============================] - 47s 1ms/step - loss: 0.0272 - accuracy: 0.9911 - val_loss: 0.1117 - val_accuracy: 0.9933\n",
      "Epoch 134/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0316 - accuracy: 0.9906 - val_loss: 0.1341 - val_accuracy: 0.9922\n",
      "Epoch 135/150\n",
      "34300/34300 [==============================] - 46s 1ms/step - loss: 0.0388 - accuracy: 0.9898 - val_loss: 0.0964 - val_accuracy: 0.9929\n",
      "Epoch 136/150\n",
      "34300/34300 [==============================] - 45s 1ms/step - loss: 0.0264 - accuracy: 0.9910 - val_loss: 0.1012 - val_accuracy: 0.9936\n",
      "Epoch 137/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0363 - accuracy: 0.9900 - val_loss: 0.1326 - val_accuracy: 0.9926\n",
      "Epoch 138/150\n",
      "34300/34300 [==============================] - 45s 1ms/step - loss: 0.0309 - accuracy: 0.9911 - val_loss: 0.1094 - val_accuracy: 0.9931\n",
      "Epoch 139/150\n",
      "34300/34300 [==============================] - 45s 1ms/step - loss: 0.0344 - accuracy: 0.9909 - val_loss: 0.1113 - val_accuracy: 0.9915\n",
      "Epoch 140/150\n",
      "34300/34300 [==============================] - 45s 1ms/step - loss: 0.0255 - accuracy: 0.9917 - val_loss: 0.0988 - val_accuracy: 0.9938\n",
      "Epoch 141/150\n",
      "34300/34300 [==============================] - 45s 1ms/step - loss: 0.0268 - accuracy: 0.9915 - val_loss: 0.0961 - val_accuracy: 0.9943\n",
      "Epoch 142/150\n",
      "34300/34300 [==============================] - 48s 1ms/step - loss: 0.0235 - accuracy: 0.9924 - val_loss: 0.1027 - val_accuracy: 0.9941\n",
      "Epoch 143/150\n",
      "34300/34300 [==============================] - 48s 1ms/step - loss: 0.0225 - accuracy: 0.9924 - val_loss: 0.1073 - val_accuracy: 0.9941\n",
      "Epoch 144/150\n",
      "34300/34300 [==============================] - 47s 1ms/step - loss: 0.0220 - accuracy: 0.9926 - val_loss: 0.1098 - val_accuracy: 0.9941\n",
      "Epoch 145/150\n",
      "34300/34300 [==============================] - 47s 1ms/step - loss: 0.0232 - accuracy: 0.9919 - val_loss: 0.1101 - val_accuracy: 0.9942\n",
      "Epoch 146/150\n",
      "34300/34300 [==============================] - 47s 1ms/step - loss: 0.0222 - accuracy: 0.9925 - val_loss: 0.1105 - val_accuracy: 0.9945\n",
      "Epoch 147/150\n",
      "34300/34300 [==============================] - 47s 1ms/step - loss: 0.0258 - accuracy: 0.9915 - val_loss: 0.1219 - val_accuracy: 0.9941\n",
      "Epoch 148/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0433 - accuracy: 0.9894 - val_loss: 0.1098 - val_accuracy: 0.9936\n",
      "Epoch 149/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0328 - accuracy: 0.9903 - val_loss: 0.1430 - val_accuracy: 0.9869\n",
      "Epoch 150/150\n",
      "34300/34300 [==============================] - 44s 1ms/step - loss: 0.0449 - accuracy: 0.9896 - val_loss: 0.0867 - val_accuracy: 0.9941\n",
      "Wall time: 1h 53min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history7 = model.fit(X_train, y_train, batch_size=128, epochs=150, verbose=1,\n",
    "         validation_split = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aasha\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\keras\\engine\\saving.py:165: UserWarning: TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "  'TensorFlow optimizers do not '\n"
     ]
    }
   ],
   "source": [
    "#Step 1: Create or open a file with write-binary mode and save the model to it\n",
    "filenm = 'MNIST_Digit_2LayerCNN_02he_normal_Adam19.pickle'\n",
    "\n",
    "#Step 2: Open the saved file with read-binary mode\n",
    "pickle = pkl.dump(model, open(filenm, 'wb'))\n",
    "\n",
    "predicted = model.predict(X_test_sub)\n",
    "y_pred = predicted.argmax(axis=1)\n",
    "\n",
    "Out = pd.DataFrame([test['filename'],y_pred]).T\n",
    "Out.rename(columns = {0:'filename', 1:'label'})\n",
    "#Out\n",
    "Out.to_csv('MNIST_DropOut_2LayerCNN_02he_normal_Adam19.csv', header =  ['filename', 'label' ], index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/torch/torch.github.io/master/blog/_posts/images/resnets_modelvariants.png\" style=\"width: 500px\"/>\n",
    "\n",
    "<br> Batch normalization layer is usually inserted after dense/convolution and before nonlinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(50, input_shape = (784, )))\n",
    "    model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(50))\n",
    "    model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(50))\n",
    "    model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(50))\n",
    "    model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(50))\n",
    "    model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    adam = optimizers.Adam(lr = 0.01)\n",
    "    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mlp_model()\n",
    "history = model.fit(X_train, y_train, validation_split = 0.4, epochs = 100, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdeVyU1f7A8c9h3xEBFUXFfd8RLXfNXLIy09Kyss2yvX7tt9t261a3bnXbNCs1yzSzRSuX1FwqV9zFfQdBBBRkX8/vjzPAAAMMCmLO9/16zYuZZ5tnBjjf53zPec5RWmuEEEI4HqfaPgEhhBC1QwKAEEI4KAkAQgjhoCQACCGEg5IAIIQQDsqltk+gKoKCgnRYWFhtn4YQQvytbNmyJVFrHVx6+d8qAISFhREZGVnbpyGEEH8rSqnjtpZLCkgIIRyUXQFAKTVDKXVaKbW7nPVKKfWBUuqQUmqnUqq71bo7lFIHLY87rJb3UErtsuzzgVJKXfjHEUIIYS97awCzgOEVrB8BtLI8JgNTAZRSdYGXgF5ABPCSUirAss9Uy7aF+1V0fCGEENXMrjYArfVapVRYBZtcD8zWZlyJDUqpOkqpEGAgsFxrfQZAKbUcGK6UWg34aa3XW5bPBkYDS6r6AXJzc4mJiSErK6uquwobPDw8CA0NxdXVtbZPRQhRw6qrEbgREG31OsayrKLlMTaWl6GUmoypKdCkSZMy62NiYvD19SUsLAzJIl0YrTVJSUnExMTQrFmz2j4dIUQNq65GYFslrz6P5WUXaj1dax2utQ4PDi7Ti4msrCwCAwOl8K8GSikCAwOlNiWEg6iuABADNLZ6HQrEVrI81Mby8yKFf/WR71IIx1FdAWARcLulN1BvIEVrHQcsA65WSgVYGn+vBpZZ1qUqpXpbev/cDiyspnMRQojzkpSWzdcbjpOVm1+r56G15mx6To2/j73dQOcC64E2SqkYpdTdSqn7lVL3WzZZDBwBDgGfAQ8AWBp//wVstjxeLWwQBqYAn1v2Ocx5NABfCpKTk/nkk0+qvN/IkSNJTk6ucJsXX3yRFStWnO+pCeGQzqTnsPXEWfIL7J/rRGvNwu0nGfreWl74aTcz/zpWY+f3/ooDPDBnCwUVnN+bS/bR7V/LeWDOFvadOldj56L+ThPChIeH69J3Au/du5d27drV0hnBsWPHGDVqFLt3l7xFIj8/H2dn51o6qwtT29+pEOdr36lz3P7FJk6nZhPk4841nRowvGMI3ZrUwcPV9v9jalYuj83bzsp9p+nSuA7OCk6cyeDPZwaX2UdrzW974pm9/hjdmwTw4KCWJbZJzshBofD3st2Lbu2BBG6fsQmAf9/QiVt6le3YsnR3HPd/vZWeYQHsjUslLTuP4R0a8MKodoQGeJ3X96KU2qK1Di+9/G81FMSl6Nlnn+Xw4cN07doVV1dXfHx8CAkJYfv27ezZs4fRo0cTHR1NVlYWjz76KJMnTwaKh7VIS0tjxIgR9O3bl3Xr1tGoUSMWLlyIp6cnkyZNYtSoUYwdO5awsDDuuOMOfv75Z3Jzc/nuu+9o27YtCQkJ3HLLLSQlJdGzZ0+WLl3Kli1bCAoKquVvxvHsP5VKA38P/D1rrwttYlo2z36/k1t6NWFw2/o18h4/both6e5TPDqkNe0b+lV5/x3Ryfx1OJEtx86yPz6Vq9rV56HBLQnycaegQLNwx0lm/nWMDg39eXZE2xLf5/boZA7Gp5KVV0B2bj71/Tzo3zoYf09XIo+d4a5Zm/F0c+aNMZ1YeyCBeZuj+XL9cVydFR0a+jOsQwPuH9C8RFvXVxuOs3LfaV64ph139mnGluNnuenT9Xy7OZo7rgwr2m7jkSTeXLqPbSeSqefrzl+Hkvh1Vxxv3dgZgNnrj7N0dxy5+ZpGdTzp0NCPazqHcF2XhiilSM7I4akFO2hZz4e6Xm68tXQfwzrUJ9DHveg9jiSk8eR3O+nauA5f39OLrJwCvvjrKHM3ncDdpfovKC+rAPDKz1Hsia3e6lL7hn68dG2Hcte/+eab7N69m+3bt7N69WquueYadu/eXdSNcsaMGdStW5fMzEx69uzJjTfeSGBgYIljHDx4kLlz5/LZZ59x00038f333zNx4sQy7xUUFMTWrVv55JNPeOedd/j888955ZVXGDx4MM899xxLly5l+vTp1fr5hX2S0rIZ9eEfNK7rxZx7ehHi71kr5/H+igOs2HualftO8/hVrXloUEucnMo27C/YEoOrs2Jg63rlXq3acjI5k3/8uJuMnHx+2xPP+J6NeWJoG4J93SvfGfh9Xzx3zTK1+ObB3rSq58Ps9cf4LjKaCRFN+PNQIvtOpdI8yJtvN59g5d54Xr2+I6CZvvYIW0+UTZu6OCnCwwLYHp1MQ39PvrwrgsZ1vZgQ0YS07DzWH05i64mzrDuUyFtL99G1cR2uaGH+B7XWLIiMoWdYAPf0aw5ARLO69AwL4NM1h5kQ0QQ3Fye+i4zmqQU7aeDnwZtjOjG2RyjrDifx/I+7GDdtPQC+Hi7c1juMYF93omJT2BGTzG974vllZxxvjOnEKz/vISkthy/u6Im7ixMj/vcHby7Zx9vjugCQkZPHlK+34uqs+PjW7ri7OOPu4swTQ1vz8OCWuDpX/8g9l1UAuBRERESU6EP/wQcf8OOPPwIQHR3NwYMHywSAZs2a0bVrVwB69OjBsWPHbB57zJgxRdv88MMPAPz5559Fxx8+fDgBAQE29xU1a/HuU+Tma+KSsxg3bT3f3NObJoHnV10vT0GB5lhSOs2DfWyuP5yQxtxN0Yzv2ZicvALeXX6AXSdTePemLvh6FBfyczYe5x8/mpSls5MiIqwu4yMaF12plkdrzUsLd1OgNb883Jcftp5k9vpjLNl9ill3RtC1cZ0Kzz8vv4A3Fu+jeZA3C6ZcSV1vNwAOnU7j3eX7+fzPo4QFevHhhG5c0ymEqNhzPP39Tu7/egsATep68er1HRjQOhgvNxc8XJ04EJ/Gir3xrNwbT4+mAXwwvluJK2ofdxeGtq/P0Pb1ycrN54o3VjJr3dGiALDl+FmOJKZz/8AWJc71gUEtuXPmZhZuP4mbixNPf7+Tfq2CmH5bOJ5u5kq8f+tglj3Wn9nrj1PHy5XruzbEy624SM0v0Mz48yhvL9vPoLdXk5qdx/8NbU3HRv4A3NOvOdPWHGZgm3rsP3WObzZFk5SezZd3RtCoTskLiJoo/OEyCwAVXalfLN7e3kXPV69ezYoVK1i/fj1eXl4MHDjQZh97d/fiP1hnZ2cyMzNtHrtwO2dnZ/Ly8gDzTylq38/bY2ld34d3xnXh9hmbGPfpOubc04uW9Xzt2l9rTYE2BXJ5Pl51iP8uP0CfloE8M7wtnUNLFrj/WboPT1dnnhzWhkBvNzqF+vPar3sZO3U9n98RTuO6Xmw8ksRLC6MY0DqYR4a04vd98SzZfYpH521n1rpjvDiqPd2a2L6IWBYVz4q9p3luRFs6NvKnYyN/bunVmLtmRXLrZxuYMaknvZoH2twXTK3j4Ok0pk3sUVT4A7Ss58Mnt/bgdGoWAV5uRYVdp1B/Fj3Uhx+2xuDn4crVHRqU+X56NA2gR9MAnhnettLv2MPVmQkRTZi25jDRZzJoXNeL7yJj8HJz5ppOISW2Hdg6mPYhfry1dB9nM3Lp3SywROFfyNvdhSmlgkchZyfFvf2bM6BNME8t2ImXq3OJbR8Z0pKfd8Ty4DdbUQoGtanHPf2acWWLi5e+ldFAL5Cvry+pqak216WkpBAQEICXlxf79u1jw4YN1f7+ffv2Zf78+QD89ttvnD17ttrf43KWkpHLxiNJZZavO5TI0HfXsCO64p5aALHJmWw6doZrOzekc2gdvp18BfkFMHbaeradqPz3kZdfwP1fb6Hbq7/xn6X7OJ1a9iIhKS2baWsO0y7Ej71xqVz30V9M+XoLu0+mABB57AzLouK5f0BzgnzcUUpxZ59mzL4rgriUTEZ//BeLdsQyZc5WmgR68cGEbvRoGsBTw9qy4vEBvD22MzFnM7nhk3UM/u9qrnp3DVe9u4bbvtjIrL+Osv9UKi8viqJtA1/u6ltcw21Zz5f5911BA38P7pi5id/3xdu8KMnIyePd5Qfo0TSAYR1st03U8/Uoc6Xr6uzEzT2bMKJTSIXB0V4TezdFKcVXG46TkZPHLztjuaZTCN7uJa+FlVI8OKgliWk5dGtch8/vKFv426t1fV8WPtiHb+7thYvV5/Nyc+HDW7rx6JBWrHlyEDMm9byohT9cZjWA2hAYGEifPn3o2LEjnp6e1K9f/Mc9fPhwpk2bRufOnWnTpg29e/eu9vd/6aWXmDBhAt9++y0DBgwgJCQEX1/7rjod3Y7oZB6Ys5WTyZn86/oO3HZFGACnz2XxyLxtJKbl8OA3W/n1kX4VNuz+ujMOgGu7NASgTQNfvp9yBbd9sYlbPtvItNt6MKB12bvYwVz5v/xzFMui4ukZFsDUNYf5/I+j3NKrCS9c066owPho1SEyc/P5cEI36vu58/kfR/niz6Ms2X2KPi0DOZOeS30/d+7u27zE8fu0DOLHB/twz5eRPDJ3G74eLnx+e3iJz+PkpBgX3pgRnUL44o+jHIg3FzQazb5Tqbz88x4AlIJPJnYvU0g38Pdg/n3m8941K5IAL1faN/SjYyN/BrauR8+wAD7/4yinU7OZOrF7rd5s2LCOJ8M7NGDephM0rutFek4+48Ib29x2ZKcGzJgUTkSzwDIB4nzY+tzdmwTQvZwa18Ug3UD/5rKzs3F2dsbFxYX169czZcoUtm/ffkHH/Lt9p4/O28aZ9Bz+Oao9retXHvy01sxef5zXft1DPV8PwoK8WH84iU9vC2dw23rc+vkGdkSn8Mp1HXj+x10MaVePaRN7lFtwXfvhnygFix7qW2L56dQsJs3YzIH4VJ4b2Y6JvZuU6cnx6ZrDvLFkH/f1b85zI9txLDGdT9ceZu6maK7r0pD3bu5KbHImg/+7mhu7h/KmpccJQEpmLnM3nWDmX0eJP5fNWzd24uaeZbsVgume+Pay/VzbpSG9K0jT2HI4IY2Ve+MJ8HIrt7AE053yp+2x7IlNISr2HPviUsnJL8Df05XsvHwGtq7HtNt6VOm9a8LmY2cYN209Hq5ONPDzYNWTAy/7O+ClG+hl6sSJE9x0000UFBTg5ubGZ599VtundMES07IJ9Haz659yb9w5Fm6PxUnByP/9wd39mvHI4FblXrHtiE7mraX7WHc4icFt6/HuTV1wc3FiwvQNPDx3K8M6NGDDkTO8M64LY3uEci4rl9d+3cusdceYdGUYiWk5nM3IoUWwD85OiqOJ6ew6mcI/RpYNmPV8PZh3X28enLOVf/2yh8/WHmHKwBb0DKvL0cR0dp5M5tM1R7imc0hRDjssyJs3xnSmSV1v3lq6D1dnJ/IKCnBSiseual3i+P6ertw/oAV39WlGVGxKhY2wdbzceP2GTpV+n7a0CPahRTkNz9Z8PVy5rXfTotfp2Xn8cTCB5XtOszfuHM+OqDxPfzGENw2gYyM/dp88x7jwxpd94V8RqQGIMmrzO10WdYr7v97Ck1e34cFBLSvd/rkfdvHD1hiWPNqPaWsOMz8yBl93F4Z2qM+1XRrSIcSPuJQs4lIyWbQjlsW7TlHX241Hh7Titt5Ni7pIJqRmM2bqX0SfyWRsj1DesXTN01pz7+xIft93GhdnJ3LyCgDo0NCPf45qz6ajZ3h3+QHWPze43K6fWmv+OpTE/1YeYPOxkm0Cg9oEM3ViD5s3Kf1vxUHeW3EAgPsHtLhkCtDLweJdcTzz/U6WPz6ABv4etX06Na68GoAEAFFGdX6nCanZ/Lgthlt7NS1xVa615tS5LBr4eRRdge0+mcK4aevJyS/A1Vmx8v8GlukOZy0lI5feb6zkui4NeWusSY1sPXGWuRtPsDTqFKlZeSW293Zz5p5+zbmnX7MS3SILHUtM57st0Tw4qGWJ7nzJGTl8vOoQTkoR4u+Bs5Ni6urDxKZk4eHqROdGdZh//xWVfhdaazYfO0v8uSyaB3vTLMi7xPvY2v5/Kw/y6844Ftx/ZZX664vKaa0d5upfAoCw2/l+p7b+oV5auJsv1x+nRbA3Uyf2oHV9X2LOZvD8j7tZeyCBro3r8MzwtjQL8ub6j//ExcmJj2/tzs2frmdo+/p8dEv3ct4NPv/jCK/9updfH+lLh4b+JdZl5+Xzx4FEYlMyCfH3JMTfg7Agb3yqoTEPICs3n8/WHmH62iO8dkNHru9qczoLIS4J0gYgztu0NYfRmnL7O59Nz+GpBTs5mpjG4kf7FTV05uQVsGhHLF0a1+Hk2Uyu/+gvbgoP5bstZi6gu/s2Y/GuOCZ8tgE/DxfyCzQLplxJuxA/pgxswfsrDjKxd5LNRsv8AtOQ2zMsoEzhD+Du4sxV7WtmKAQwfcofHtKKh4e0qrH3EKKmSQAQFUrOyOHd3w6Qk19AaIBnUVfHQtujk3lwzlbiUjIp0LBwWyw39TQ9RVbtP83ZjFzevakVHRr68fDcbXy5/jj9Wwfz7xs6EhrgxVPD2vDV+uN8GxnNP65pR7sQM7bMff1b8F1kDC8viuL1Gzrx+754/jiYSOMAL8aFh5KbrzlxJoOnh7e56N+JEJcLuRHsIvPxMb0pYmNjGTt2rM1tBg4cSOlUV2nvv/8+GRkZRa/tGV76fCzcHktOfgHNg7x57oddHE1MB8wV+PS1hxk3bR1KwY8P9KFdiB+f/XGk6Eag77fEEOzrTr9WQdTz82DOPb349ZG+fHlnz6JRDT1cnbm3f3NWPDGAQW3qFb2vp5szz49sx75Tqdw4dR3T1hzBzdmJdYcTmTRzM5O/iqS+nzvDOjSo9s8shKOQGkAtadiwIQsWLDjv/d9//30mTpyIl5cpSBcvXnxB56O1JjM3HzeXktcE8yOj6dDQj+m3h3PNB3/w4JytvDGmEy8u3M2OmBSubl+ft8d2wd/Llcn9m/H4tztYfSCBLqF1WLX/NJOuDCu6mcnF2clmuqY8Izs14OVr21PHy41BbcygZTl5BazcG8/C7bEM79igxsZIEcIRyH/PBXrmmWdKTAjz8ssv88orrzBkyBC6d+9Op06dWLiw7GRnx44do2PHjgBkZmYyfvx4OnfuzM0331xiLKApU6YQHh5Ohw4deOmllwAzwFxsbCyDBg1i0KBBgBleOjExEYB3332Xjh070rFjR95///2i92vXrh333nsvHTp04OqrryYzM5Os3HzikjPZdyqVQ6fTOJ6YQWG/gCjLDT03hTemUR1P/juuC3viznH9x38RczaTDyd049PbehT1ThnVuSEN/Dz4bO0Rft4RS26+Zkx365k/q0YpxaQ+zRjdrVHRe7i5ODGiUwjTbuvB6G7S8CrEhbi8agBLnoVTu6r3mA06wYg3y109fvx4HnvsMR544AEA5s+fz9KlS3n88cfx8/MjMTGR3r17c91115Xb5Wzq1Kl4eXmxc+dOdu7cSffuxT1fXn/9derWrUt+fj5Dhgxh586dPPLII7z77rusWrWqzLj/W7ZsYebMmWzcuBGtNb169WLAgAH4+vmXGXb62/nf0eOq6wHwdXfB39OVxLRssrNN98nvImNwc3bi+q4m7z+kXX1euKYdRxPTefLqNgRYDegFZtyWu/qG8e/F+ziSkE77EL+inL4Q4tIjNYAL1K1bN06fPk1sbCw7duwgICCAkJAQnn/+eTp37sxVV13FyZMniY+PL/cYa9euLRr/v3PnznTuXHy7//z58+nevTvdunUjKiqKPXv2VHg+f/75JzfccAPe3t74+PgwZswYVvy+mkOn02jSNKzEsNO79x9GAW3q+xAW5E2Ivwd+Hq6kZOWy79Q5M0Ve+/rU8Sou6O/p15zXb+hUpvAvND6iCT7uLpw6l8WNPc7/6l8IUfMurxpABVfqNWns2LEsWLCAU6dOMX78eObMmUNCQgJbtmzB1dWVsLAwm8NAW7NVOzh69CjvvPMOmzdvJiAggEmTJlV6nNL3dRRoTXJGLhpwcnUjKzcfD1dn8rUiPctMm+dm6baplKJRgCeHgQnTN3A2I5dx4VUrxP08XLm1dxNm/XWM60r1GBJCXFrsnRR+uFJqv1LqkFLqWRvrmyqlViqldiqlViulQi3LBymltls9spRSoy3rZimljlqt61q9H+3iGT9+PPPmzWPBggWMHTuWlJQU6tWrh6urK6tWreL48eMV7t+/f3/mzJkDwO7du9m5cycA586dw9vbG39/f+Lj41myZEnRPuUNQ92/f39++uknMjIySE9P5/vvf6Rzz940CvBEAdFnMijQmtSsXJyUIti3bBqnjpcrZzNyaeDnQb9WtkexrMiTV7dh5f8NsHuWKCFE7ai0BqCUcgY+BoYCMcBmpdQirbV1LuIdYLbW+kul1GDgDeA2rfUqoKvlOHWBQ8BvVvs9pbU+/64wl4gOHTqQmppKo0aNCAkJ4dZbb+Xaa68lPDycrl270rZtxWO4TJkyhTvvvJPOnTvTtWtXIiIiAOjSpQvdunWjQ4cONG/enD59+hTtM3nyZEaMGEFISAirVq0qWt69e3cmTZpEREQEBRquu3ki/Xr3JCc5HldnRWZuPseTMsjJK8Db3Rlnp7LXAF5uLjwwsAWt6vuc1xjsrs5O5z15tRDi4ql0KAil1BXAy1rrYZbXzwFord+w2iYKGKa1jlEml5GitfYrdZzJwACt9a2W17OAX6oSAGQoCPsUdumMPpOBBlrV8y0qyE+cySA5Iwd3F2da1ffByUbqSb5TIS4v5Q0FYU8KqBEQbfU6xrLM2g7gRsvzGwBfpVTp+/fHA3NLLXvdkjZ6TyllM1+glJqslIpUSkUmJCTYcbqOp6DAFPgpGTnEpWSyP9506czN14TW8SpxFd/Q3wNfD1ca1fGwWfgLIRyHPY3AtkqJ0tWGJ4GPlFKTgLXASaBoKEalVAjQCVhmtc9zwCnADZgOPAO8WuaNtJ5uWU94ePjfZ+S6apadl8+Z9BwKNOgCTb7W5OZrcvMLyM0vKNpOofB2d6aerwd+Hi4lpqADczNWsyDv0ocXQjggewJADGA9DVAoEGu9gdY6FhgDoJTyAW7UWqdYbXIT8KPWOtdqnzjL02yl1ExMEDkvl/uwrvkFmmOJJm/v5AROSuGkFK7OCl93F1xdnHC3PNxcnC9o7tS/0+iwQogLY08A2Ay0Uko1w1zZjwdusd5AKRUEnNFaF2Cu7GeUOsYEy3LrfUK01nGWNoPRwO7z+QAeHh4kJSURGBh42QaB2ORMcvLyaRbsU23DGduitSYpKQkPj8t/ggwhhB0BQGudp5R6CJO+cQZmaK2jlFKvApFa60XAQOANpZTGpIAeLNxfKRWGqUGsKXXoOUqpYEyKaTtw//l8gNDQUGJiYrhc2wcycvI4k56Ln4cL0ak1PyGIh4cHoaFyA5cQjuBvPyHM353Wmq83nsDX3aXM2DbHEtO55oM/6NDQn2/u7VUmny+EEPaQCWEuQenZeTy1YAeLd53CzcWJXs3rFs0rq7Xm+R934eykeG98Vyn8hRDVTkqVWnIsMZ0xn6xj6e5T3D+gBVprPlh5qGj97/tOs+5wEk8MbV3hvLhCCHG+pAZwkWXk5DFtzRGmrz2Mh6szX94VQb9WwWTm5PH1xhNM7t+c0ABP/r14L82DvLm1d9PaPmUhxGVKAsBF9FvUKV5cGMWpc1mM6hzCcyPbFV3dPzS4FfMjY3hv+QF6hgVwOCGd6bf1kAlPhBA1RgLARXIuK5dH5m0jLNCbj27pRnhY3RLrg33duatvGB+vOsyq/afp3bwuQ2twUnMhhJDLy4tk8c44snILePPGzmUK/0KT+7XAz8OFtOw8Xrim/WV7X4MQ4tIgNYCLZMGWGFrW86FLaPlz4vp7ufL++K7EJmfRsZH9c+cKIcT5kABwERxNTCfy+FmeHdG20qv6wW0l7SOEuDgkBXQRLNgSjZOCG2QScyHEJUQCQA3LL9D8sPUk/VsHU99PxtgRolJaw+YvIPFgbZ/JZU8CQA1bdziRuJQsxvVoXPnGQgjYtQB+fQJ+frS2z+SyJwGghn0XGYO/pytD2tWr7VMR4tKXnghLngZXbzj+F0Rvru0zuqxJAKgh206cZdLMTSzaEcsN3Rrh4epc26ckRLGzxyEno7bPoqwlz0B2KtyxCDzqwLr/1fYZVY/Msya4XWIkAFSzvPwCJs+O5IZP1rEjOpmnh7fhmeEVTwovLoKMMxC7vbbP4tKQdQ6m9oFfHq/tMylp/xLYvQD6PwWh4dDzHtj7CyRaxsjKz4OVr8L2by78veKjYP4dsOVL87cBkJ0G+xbD6rfMd2SPhAPwx7twcmv525zeBx/3gs8GVRx0N0yDD8PN72XfYnM+NUyGg65m8zdH8/T3O3l4cEvuH9AC7xqcwEVUwXd3QtSPMGY6dL6peo6ZdhrWvg2dbzYFVk3T2lxF+gRf2HEiZ8Ivj4FygociIbBF5fvkZkJGEvg0AOcq/k3HR5mCPPOMKVidnGDQP8CvYfE22anwUQR41oHJa8DFzXy/73WErhNg+Juw4C7YvxiUM9y1DBr3rNp5FNIavrgaYjYDGpxcoH4HOL0X8nPMNuF3waj3yj/G5i/M9xi/y7x294c7F0ODjiW3O7UbZl8PBXmQlQwDn4OBz5Y93ravYeGDENQGUmIgN90s964H/o3ArxEM+zcEnN/YYBcyKbywU05eAf9beZAuof48MbS1FP7nIzka3u8M+5dWfV+tYdW/4a9SaYO8bDi43Pyj/3ifCQQX6sgamNYXNk2H2aMhZsuFH7MiWpsrw3dawowRsHM+5GZVvE96krnKjSl10bTtKwhoBs5u8Oe7FR8jPgoWPwXvtIH3OsBrwfDfdqYwruhqtiAfts2Bz6+CqVfC6jdgx1w49oc5958fM5+p0Oo3ITUOrvvQFP4APvWg6y2wfS58eZ2pIQx91RSI398FmckVn3t5on6AmE1w3Qcm2PR+AFy9oNd9cPsiiLgPImdA9Cbb+x9cbhqpnV1MYLrnd3D3ga9ugDNHirc7sRG+HGW+53tWQvvR8Of75m/c2t5fYLAzAzYAACAASURBVNHD0GIw3P8nPHPMnMegf0Cb4eBZF5IOgXP1TwglNYBq9NWG4/zzp93MviuC/q0v8CqttkX9BN7BENan+o6pNSSfqPgq5tuJsPdnCI2Ae5ZX7dhLnoFNn4KLBzx5ADwsd1MfWgFf3whjZ8DG6XAyEkZPg3ajwNUy1HZ6EhxeCWeOQp9HwbWCLrtr/mMCTVArGPYGLP4/k+O9fRE07Gr/OcduB50PjXrY8dmeNsGm/fVwapcpaLzrwT0rbH+f+Xnw9Rg4ugbqdYD7/wAnZ1OgT73SFFxnjkLkF/DwVtvHWPIsbJwKzu7mfZv0gtRTpv1g9wJo1h8mfGv7u1r8tPldBLWGHndCl/HgZRkCZd1H8Ns/4KbZ5rjxUTCtH3SbaApla0mH4cMeJniP+RQ63mgahmcMg/bXwdiZZruUGPN7r6x2lJsFH/U0fxv3rTHfSWnZaSZl4+EH960tWfDm5cDUK8zv5IENxcEqYT/MGA7uvuZzRP0Ip/eAf2PTnlG3ufnb/yjCFOrjZpljRP0AP06BBp3g9oUmkNSA8moAaK3/No8ePXroS1VmTp6OeH25Hjd1nS4oKKjt07kw+flav9FY6zeaaH0urvqOu+ZtrV/y0/r0PtvrDyw36z++wvw8ubX8Y53ep/WJjVrnZmldUKD1sn+YfebcbH5umV287c+Pa/1aA61zMrTOTNF6+mCzzct1tP6gu9bT+mn9kr9Z9pKf1t/ebr4DW2J3mG2+u1PrrFSz7Oxxrd/toPWbTbWO31v59xC7o/g8X6mr9Y5vy9+2oEDrpc+bbZc+b17n52t9aKXWrwZrvfAh2/sVfh/f3Wl+Rs40yxc/o/WrQVqnJ2mdHKP1K4Hm+yntzFHz/Sy4x2xb2rY55jv76kbzO7C2+wfznoufNudbWl6u1lP7aP12a60zk7X+Yrj57my9j9Za7/xO6+PrSy5b+1/zHrOu1fqdtub52621TkuwfYzS+x1eXfF2e3812639b8nlf/7PLN+/rOw+0ZFavxZi1n9+tdYbPi37mVa9UXzcqX3N86l9yv/s1QQzfW+ZMlVSQNXk6w3HiT+XzRNXt669QdwyzsCObyE/98KOk3gAslJMzvLnR0tW1aM3m0avwseR0lM9l+PkVpMGANO9r7TcLFjyFAS2NFdMrt7mat2WqB/NVewXQ+GNxub5ug8hYjJMmAt1W8DOb822WpvUQYvB5mrfww/u+BnGfQn9n4Z67cHdz+Rl7/ndpBj2/AQrXrT93oXnPvTV4qu1Ok3MMVGw4uWKv4ffX4dP+8GJdTDoBWhyBfxwL6z/xKyP3W6+8496mrTLvxvC+o/MZ7v6NVDK5NBbDIYek0yD6NljJd9j1wLzffS8F278Ahr3ht9fM+0HO+dB21Hmaty/EXS71aSEzsWWPMb6j02ufegrxVfu1rreAte+D4eWw9wJpjEUTIPtwochtCcM/Zc539KcXeDa/0FaPMwcab6Lq162/T4AncZCk94ll/V5DNpcY2pCTa+Awf80tbCfpkBBge3jnIszf7NtRkLzAba3KdR2JLS7Fta8ZVI0BQWQGm9qf62uhtZXl90ntAc8sA4ej4K7l0GvyWU/U59Hwb8JrHzF/I+Nngr3ri7/s9cwSVJXg1MpWUxdfZi+LYPo3Tywdk4iepPJy6ZEm1xq38cq36egwKQ9GvUo+QcYvdH8DL/bpAh2zIUuE0yhsPyfoK3+wZQz3L3c/PGXJycDfphsUhZ5mSZfHn5XyW3WfWj+mW/7EbyDTAGz9UtT0FpX63fON3n80Ai44gHTkBcTCX2fMIWAUibdsOp1U+XOSILUWGjzQvEx3Lygw2jzKK1Rd5NOWPeh+UftNbnk+hPrzXL/0JLL6zaDnnfD2ndMaqVus7LHzk4zx20z0vzje9aBKx+GH+6BZc/B5s/hzGFw8TQFvHegCU5BraDb7WUL076PwZZZ8Md/Te4czN/BoodNYBn2b7PPsH/D54Phq9GmkOx+m9UxHoetX8HKf8HoT8z26UlmWeebSzbUltZjksn1L3sePu4JrUdA8nFTwI+dWZwesaVRDxPUNn1qnne7vfxtbXFyggmlegO5+5mLiI3TzN9GoeRo2DDV/D3l55i/KXuM+A+cGgnf3mpSWd71IC/LpP3KExBW8TFdPWH8HEjYBx1uqJG8fpXYqhaUfgDDgf3AIeBZG+ubAiuBncBqINRqXT6w3fJYZLW8GbAROAh8C7hVdh6XYgroRFK67vvWSt3hxaV698nki38C+fla//Ge1i8HaP1eJ61njDDpjuToiveL36v1F8NMFfSX/yu57qcHtH4zTOv8PFM9/3djkxZ5yU/rebea6mpOpqlu/7e91v/rpnV2Wvnv9csTlmr3Kq2/Hqf1RxEl1587pfW/6pn3KHR6v9lnzX/M64ICrTd/YdIOM68pTr/YcuaoZd+3tV75mkllpCVW/H1Yy8/T+pvx5r1O7S5eXlCg9X9amrSILSknTUpn6fO21+/8zpzXsb/Kvt+vT2k9rb/WG6drnXHW/nP99SnznmeOah0TqfW/Q7X+X1fznVpbcI9573c7lk1vrXjVrFs/1bwuTFPYk87SWuvU01r//rr5m3nJX+sDv9m3X2aK1j8/Vn5KsKoKCszv7ZVAk8JZP1Xrr8aY7+flAPMdxO2q2jHzcs3vrTBd89s/q+dcLzLKSQHZU/g7A4eB5oAbsANoX2qb74A7LM8HA19ZrUsr57jzgfGW59OAKZWdy6UWAA7Gp+per6/QXV5ZprefqMI/rb0yzmidcKDibbbMtuStbzP51LPHtf5Xfa3nTbS9fV6uKRRfCTR51w/DTaFgnav9MFzrOTeZ50mHTUB5yd8UqKVzukfWmnWLHrX9foV5/SXPmder3zLbZ1oFy8hZZhvrwlZrrWePNvnd/cu0/nSg2Wb2DVpnp1f8nWhtAtcHPbT+5ErzvKrSEkzgWPmv4mWJh8w5bP6i/P3mTzLtJ7YC4jfjzecpr33hfKScNG0Bs0eb932vk+3gnxxtgsMf75Vdl5+v9dxbzOeNWmgK8jk3V/1cstMr/3utaelJxW0CL/mZv4Hf/qn12RMXdtyCAq1PRZn/n7+h8gKAPW0AEcAhrfURrXUOMA+4vtQ27S01AIBVNtaXoEySfDCwwLLoS8BGffzSdfpcFjd/up68As28yb3p0rhO9b6B1jDvVpg+qOLubpEzTB573JemZ0OdJtD/Sdi7yPR+sZaeaNIAa/8DHceYPuC9H4CUE6ZKCqYdIfGAyeGC6b0w8XvTx7n/k2XTEM36mTTGlpllu25mnoVFD0FwWxhiyamHhgO65I0zh1aYfs712pfcP+I+k775Zpw59+s/hlvmmxROZbrcDEkHIX43tBlR+faleQdBkytN/rfQifXmZ5Mry9+v130mt1vYBlEo86zpPthxjElfVBe/hiYVc/h3cPM1bRGl01Nglj2xB658pOw6Jye44VPTW2j+baa/fp/zGIfHzcukq2qTV13z9zrqfXhkOzwcaVI+dS5wLC6loH77qt8DcYmz5y+xEWDdcTXGsszaDuBGy/MbAF+lVGEy3EMpFamU2qCUKizkA4FkrXVeBccEQCk12bJ/ZEJCgh2ne3FMW3OE5Mxcvrm3F20b+FX/G+ycbxocc1JN7tKWU7shdit0L5UfvvJh05i6+GlTuJ7eZ/okTx9ocsSjp5oboryDTIMWwMHfzM/CPuONexUfr+mV5lGewS9A/U7w0/1w0qo//NLnzM08o6cWdxVs2N38PGl5n/xcOLIaWg4pG1xaXQ29psA1/4WHt5judfb+A7YfbbovArS9xr59Smt3LSTsLb4T9fh68Aww+eDyNO4FDTqbBmzrxvN9v0JBrgkA1a3/U6ar5R2LKu5i6+5bfvBx9zEN6N7BJsCVbnT9O6nfHsLvtN0OI0qwJwDY6tJS+uaBJ4EBSqltwADgJFBYuDfRpv/pLcD7SqkWdh7TLNR6utY6XGsdHhx8afStT0zL5ptNxxndtRGt6/tW/QCn98LcW+CTK+GLYfD1WNM4WNh7ITMZfnvBNI6F9YONn9ru2bPtK3OTSeebSy53cYeR75gGua9vhE96wYyrTYF011LTwFrIv5EpvA8UBoBNpmG3UXf7P4+LO9z8lWmEm3WtuRrdt9g0Hvd7ouSxPOuYArQw0MRshuxz0HJo2eM6OcGIN82QABU1KNriWcf0HmnYzb47XW0pDBz7fjY/T6w3jasVXcErBb3uN4HjqFUPqd3fmwbChlX4Xu3lE2x65Jzv5yxUp7GpFd4633bvHXHZsScAxADW9adQoESfMa11rNZ6jNa6G/APy7KUwnWWn0cwDcTdgESgjlLKpbxjXso+/+Mo2XkFPDioiv9w6Unw65NmHJbjf5oCwcUNzp00Bf63E82t8qvfgPQEc+V75cNmfem7V3OzYIdVl77SWgyCx/fAnUtgzOcw4m2YvNp2wd5qqCncMpNND6AGHcHNu2qfrW4zuPs3kzKacxMsfADqdzRdLUsL7WkCgNamhqKcK++Wdz6u/cAMGXC+6jQ2AWTvz6YL4JnD9l0Zd7zR9Bj5YTKc2ABpCaa7bIcxl37B6lnH1BSEQ7CnPr0ZaKWUaoa5sh+PuZovopQKAs5orQuA54AZluUBQIbWOtuyTR/gP1prrZRaBYzFtCncASysps9Uo86m5/DV+mOM6tyQ5sFVuGsv6xx8NhBSTpoukAOfM938wBSEm6ablMn0AaZfd/hdpvApKDBXzOs+hE7jiguQfb+YfvrdK+g+51vfPCrTepgZEuDQCpOb73pL5fvYfL8GMOkXmHdLcarJ1pV7ox6wfY6poRxcbtImHjUwB7KzCxfc07ntKPj9X+aOTag4/1/I1cOkY+bdArNGmWCs801gEOISUmkNwJKnfwhYBuwF5muto5RSryqlrrNsNhDYr5Q6ANQHXrcsbwdEKqV2YBqH39Ra77GsewZ4Qil1CNMm8EU1faYaNXPdMdJz8nloUMuq7bjiZdMf+Y6f4Zp3igt/sKQN7jOFRtY5M/bHkH+adU5OcMWDcGonHPuzeJ+tX5oG32bVcOUc2tPktv96H3LSTB/78+VZx9zS/tguCOlc/vuBSROd2gmtrjr/96tp7Sx/4mv+Y/rnh3Sxb7967eDe36H5QNO+EtTGDDgmxCXErssjrfViYHGpZS9aPV9AcY8e623WAZ3KOeYRTA+jv41zWbnM/OsowzrUp02DKlSTj/1lbqjq/WDFY+uE9YWHNpubTTwDipd3Hm9u1Fn5qrlzUznB0bVmsKjq6FHi5AwthpjxXQAaX+CvxdkV/ELKX1+vvRl8q3DQtpaXcAAIbm0K78T9pj2mKm0RngFwy7dm5Mjg1pd++kc4nMurT1MNe3/5QdKy83h4cBW6uuVmmjsz6zSFwf+ofHtb+XxXD9Mtb/k/TSMtgJPr+adqbGk9zAQAn/qmZlGTnF0gpKsZAsC7nmmEvpS1GwV/7DcNwFXl5Fz2bmIhLhESAOwUFZvCrHVHuSWiCR0bVSFfveYt03h4209Vb1i11ucR07Ut65zpNePqabu/9/lqeZWpWTSOuDhXqqHhJgC0vKp6+8XXhI5jzQiWhV1mhbhMSACwQ0GB5p8/7SbAy42nh1Vhdq+8bDPIV+ebTUPghXL3tfTQsHnLxIXxqmsmwGhQTt6+uhW2A7QccnHe70LUbw/Px152NwEJcYlfel0a5kdGs/VEMs+PbIe/l43Bm7S2PYVbfBTkZ5/fnai1ocekqvX/vxBtRpgBzNpXeNP4pUMKf3EZkgBQiTPpOby5dB8Rzeoypns5V9475sK8CWXv2I2zzEEbUoVJQhyFs6vpwlrboyEK4cAkAFTip20nSc7I5ZXrOtge5z813vTfh+KxYgrF7QCPOpUPESuEELVAAoAtR9bAJ1dAxhnWHkygWZA37ULKGe9nyVOmp0/jXubmJ+vxX2K3m37j0v1PCHEJkgBgS/RGOL2H3K1z2HAkif6tgmxvt2cR7FloZpPqfJOZ4Sj5uFmXl2PmBLX3xiEhhLjIJADYknoKgLyNX5CVm297gvesFFj8pOk1c+XDxaNnRlv66Z/eY2Yfqsok4UIIcRFJALAlLR4Az9Sj9HXZa3uax82fm+2ufd80ZNZrD24+xdMpSgOwEOISJwHAltRT0ORKzilfHvRZi7d7qS6AORmmf3/Lq8zAZmDu+AwNN+Pug8n/u/ub0TGFEOISJAHAlrR4Mn1C+Ta3HxHZf5mePta2fQ0ZiWYicmuNe8PpKHO3btx2MxiaNAALIS5REgBK0xpST3E8x5e5+YNx1vlm4pVC+bmw7gOT8y89S1bjCNAFZgz4+CjJ/wshLmkSAErLOAMFuew558k57zB0WH/Y8qVZDrBrAaREm6v/0lf3oeGAMjeE5edI/l8IcUmT+9tLSzM9gDYkutGvTTCq28NmUvL/tjHDFpzcama6aj2s7L4e/qYxeL9l5OyG3S7iiQshRNVIDaAUbekCeiTTh/6tg6D11TBlnZl0+8BvZmTPvo+Xn9svTAO5+0GATEothLh0SQ3A4nhSOu8tP4D/gd95BUh3D6JfK0v///odYOR/4KqXIX538UiWtjTpDVtmmhvALvVhjoUQDk1KKIu5m6JZtCOW8LrZAMx6+FqCfNxLbuTmVfl4+YWzackdwEKIS5zUACyOJ6UTFuTNtc2dINWP+oE2ZuayR0AzGPEfaD28ek9QCCGqmV01AKXUcKXUfqXUIaXUszbWN1VKrVRK7VRKrVZKhVqWd1VKrVdKRVnW3Wy1zyyl1FGl1HbLo1a7zBxPyqBpXS/TCOzb4PwPVDjBe0DT6js5IYSoAZUGAKWUM/AxMAJoD0xQSrUvtdk7wGytdWfgVeANy/IM4HatdQdgOPC+UqqO1X5Paa27Wh7bL/CznDetNceT0mka6G1u+vKpX1unIoQQF409NYAI4JDW+ojWOgeYB5Sexqk9sNLyfFXheq31Aa31QcvzWOA0YGNktdqVlJ5Dek4+TaqjBiCEEH8T9gSARkC01esYyk5KuwO40fL8BsBXKVViBDWlVATgBhy2Wvy6JTX0nlKqVItr0X6TlVKRSqnIhIQEO0636o4nZQAQFugpNQAhhMOwJwDY6vKiS71+EhiglNoGDABOAnlFB1AqBPgKuFNrXWBZ/BzQFugJ1AWesfXmWuvpWutwrXV4cHDNVB5OnEkHIMwnH/IypQYghHAI9vQCigEaW70OBWKtN7Ckd8YAKKV8gBu11imW137Ar8ALWusNVvvEWZ5mK6VmYoJIrTielIFSEOqaYhb4SAAQQlz+7KkBbAZaKaWaKaXcgPHAIusNlFJBSqnCYz0HzLAsdwN+xDQQf1dqnxDLTwWMBnZfyAe5EMeTMgjx88Atw5JikhqAEMIBVBoAtNZ5wEPAMmAvMF9rHaWUelUpdZ1ls4HAfqXUAaA+8Lpl+U1Af2CSje6ec5RSu4BdQBDwWnV9qKo6npROk0CvoolgJAAIIRyBXTeCaa0XA4tLLXvR6vkCYIGN/b4Gvi7nmIOrdKY16MSZDK5qV79oKkhpBBZCOAKHHwoiLTuPxLSc4hqAqxe4+9b2aQkhRI1z+ABwwtIFtGldb0iNM1f/MouXEMIBOHYAiI8ib6vJUDUN9DL3APiG1PJJCSHExeHYAWDps3SOfI4W6qQlBXQKfCX/L4RwDI4bAM4cgaNrAXjQYyl+Hq6Wu4ClB5AQwjE4bgDYNgeUE1s8ejNKr4WzxyAnVWoAQgiH4ZgBID8Pts+Bllfxjp6IG7nwu+U2BKkBCCEchGMGgEMrIDWO3C63sfFcXQ4H9INdlhuVpQYghHAQjhkAtn0F3sFEB/WjQEN027uL10kvICGEg3C8AJAaD/uXQJcJHE/JBcC7dX8IsYxQIXcBCyEchOMFgB1zQedD99uLbwIL8oZhr0O328AzoJZPUAghLg7HmxT+5BYIbAlBrTixYQ+ers4E+7iDb18I61vbZyeEEBeN49UAMs4UpXlikzNpWMcDJUM/CCEckAMGgETwMrNVxqZk0bCOZy2fkBBC1A7HCwDpxQEgLjmTEH+PWj4hIYSoHY4VAAoKIPMMeAeRk1dAQlo2If5SAxBCOCbHCgBZyaALwCuI+HNZaA0N60gNQAjhmBwrAKQnmp9egcQmZwJIDUAI4bAcKwBkJJmf3oHEpWQBSCOwEMJh2RUAlFLDlVL7lVKHlFLP2ljfVCm1Uim1Uym1WikVarXuDqXUQcvjDqvlPZRSuyzH/EBdjL6YGVY1gBRTA5AUkBDCUVUaAJRSzsDHwAigPTBBKdW+1GbvALO11p2BV4E3LPvWBV4CegERwEtKqcJbbacCk4FWlsfwC/40lSlKAQURl5yFv6crXm6Ody+cEEKAfTWACOCQ1vqI1joHmAdcX2qb9sBKy/NVVuuHAcu11me01meB5cBwpVQI4Ke1Xq+11sBsYPQFfpbKFaaAvAKJS5EuoEIIx2ZPAGgERFu9jrEss7YDuNHy/AbAVykVWMG+jSzPKzomAEqpyUqpSKVUZEJCgh2nW4GMJHDzAVcPYpPlJjAhhGOzJwDYys3rUq+fBAYopbYBA4CTQF4F+9pzTLNQ6+la63CtdXhwcLAdp1sB65vApAYghHBw9iTAY4DGVq9DgVjrDbTWscAYAKWUD3Cj1jpFKRUDDCy172rLMUNLLS9xzBqRkQTeQWTm5HM2I1dqAEIIh2ZPDWAz0Eop1Uwp5QaMBxZZb6CUClJKFR7rOWCG5fky4GqlVICl8fdqYJnWOg5IVUr1tvT+uR1YWA2fp2KWcYAKewBJDUAI4cgqDQBa6zzgIUxhvheYr7WOUkq9qpS6zrLZQGC/UuoAUB943bLvGeBfmCCyGXjVsgxgCvA5cAg4DCyprg9VrvSkoh5AIDeBCSEcm119ILXWi4HFpZa9aPV8AbCgnH1nUFwjsF4eCXSsyslesIwk8C6uATSSFJAQwoE5zp3AOemQl2m6gFpqAPX93Wv5pIQQovY4TgCwvgksJZMgH3fcXZxr95yEEKIWOU4AKBoHKMgyEYw0AAshHJvjBQCvQJkIRgghcKQAYDUUdFxKlvQAEkI4PMcJAJYawDnnOqRl50kKSAjh8BwoACSCkyuxmabnq9QAhBCOznECgGUcoLiUbEDmARBCCMcJAJZxgIongpEagBDCsTlWAPCqS1xyFs5Oinq+UgMQQjg2xwkA6YngFcSpc1kE+7jj7FTzM1AKIcSlzHECQEYieAeRnp2Hr4dMAymEEI4RAPJzISsFvALJyMnHy02GgBBCCMcIABmWEai9AsnMycdTAoAQQjhKALDcBewdREZuHl5ukgISQggHCQDF4wBlSA1ACCEARwkAVkNBZ+bk4+UqAUAIIRwjAJSqAUgjsBBCOFwAqGtpBJY2ACGEcIwAkJ4IHnXIw5mc/AI8JQUkhBD2BQCl1HCl1H6l1CGl1LM21jdRSq1SSm1TSu1USo20LL9VKbXd6lGglOpqWbfacszCdfWq96NZyTADwWXm5gNICkgIIYBKcyFKKWfgY2AoEANsVkot0lrvsdrsBWC+1nqqUqo9sBgI01rPAeZYjtMJWKi13m61361a68hq+iwV829EZo4JANILSAgh7AgAQARwSGt9BEApNQ+4HrAOABrwszz3B2JtHGcCMPf8T/UCjJsFQEZiOiA1ACGEAPtSQI2AaKvXMZZl1l4GJiqlYjBX/w/bOM7NlA0AMy3pn38qpWyOzqaUmqyUilRKRSYkJNhxuuXLyJEUkBBCFLInANgqmHWp1xOAWVrrUGAk8JVSqujYSqleQIbWerfVPrdqrTsB/SyP22y9udZ6utY6XGsdHhwcbMfpli8zNw9AegEJIQT2BYAYoLHV61DKpnjuBuYDaK3XAx5AkNX68ZS6+tdan7T8TAW+waSaapTUAIQQopg9AWAz0Eop1Uwp5YYpzBeV2uYEMARAKdUOEwASLK+dgHHAvMKNlVIuSqkgy3NXYBSwmxpWGACkG6gQQtjRCKy1zlNKPQQsA5yBGVrrKKXUq0Ck1noR8H/AZ0qpxzHpoUla68I0UX8gprAR2cIdWGYp/J2BFcBn1fapyiG9gIQQophdyXCt9WJM4671shetnu8B+pSz72qgd6ll6UCPKp7rBZMUkBBCFHOMO4Etim4Ec5VGYCGEcKwAkFPYC0hqAEII4VABICMnHxcnhZuLQ31sIYSwyaFKQpkMRgghijlUAMiUuQCEEKKIQwWAjNx8mQ9YCCEsHCoAZObkyU1gQghh4VABQNoAhBCimMMFAGkDEEIIw6ECQFZuvqSAhBDCwqECgNQAhBCimMMFAJkLQAghDIcKAJk5eVIDEEIIC4cJAFpry30AEgCEEAIcKABk5xWgtQwEJ4QQhRwmAMhsYEIIUZIDBQAzFLSkgIQQwnCYAFA8HaT0AhJCCHCkAFA0G5jUAIQQAuwMAEqp4Uqp/UqpQ0qpZ22sb6KUWqWU2qaU2qmUGmlZHqaUylRKbbc8plnt00MptctyzA+UUqr6PlZZMh+wEEKUVGkAUEo5Ax8DI4D2wASlVPtSm70AzNdadwPGA59YrTuste5qedxvtXwqMBloZXkMP/+PUbniFJAEACGEAPtqABHAIa31Ea11DjAPuL7UNhrwszz3B2IrOqBSKgTw01qv11prYDYwukpnXkXFNQBpAxBCCLAvADQCoq1ex1iWWXsZmKiUigEWAw9brWtmSQ2tUUr1szpmTCXHBEApNVkpFamUikxISLDjdG2TXkBCCFGSPQHAVm5el3o9AZiltQ4FRgJfKaWcgDigiSU19ATwjVLKz85jmoVaT9dah2utw4ODg+04XdsKG4ElBSSEEIY9+ZAYoLHV61DKpnjuxpLD11qvV0p5AEFa69NAtmX5FqXUYaC15ZihlRyzWsmNYEIIUZI9NYDNQCulVDOllBumkXdRioADMQAAB/5JREFUqW1OAEMAlFLtAA8gQSkVbGlERinVHNPYe0RrHQekKqV6W3r/3A4srJZPVA4JAEIIUVKlNQCtdZ5S6iFgGeAMzNBaRymlXgUitdaLgP8DPlNKPY5J5UzSWmulVH/gVaVUHpAP3K+1PmM59BRgFuAJLLE8akxmTh4erk44OdVob1MhhPjbsKtLjNZ6MaZx13rZi1bP9wB9bOz3PfB9OceMBDpW5WQvRGZuvvQAEkIIKw5zJ3BGjkwHKYQQ1hwmAGTKdJBCCFGCwwQAmQ9YCCFKcpgAkJmTL/cACCGEFYcJABm5edIGIIQQVhwnAORILyAhhLDmMAFAUkBCCFGSwwQAaQQWQoiSHCYAZOZKDUAIIaw5RADIL9Dk5BXg5SptAEIIUcghAoDMBSCEEGU5RACQ6SCFEKIshwgAMiG8EEKU5VABQG4EE0KIYg4RADJzTRuApICEEKKYQwSA4hSQ9AISQohCDhYApAYghBCFHCIAZOVKLyAhhCjNIQKA1ACEEKIsuwKAUmq4Umq/UuqQUupZG+ubKKVWKaW2KaV2KqVGWpYPVUptUUrtsvwcbLXPassxt1se9arvY5VUFADkTmAhhChSaYmolHIGPgaGAjHAZqXUIstE8IVeAOZrracqpdpjJpAPAxKBa7XWsUqpjsAyoJHVfrdaJoevUZk50gtICCFKs6cGEAEc0lof0VrnAPOA60tt8//t3W2MXFUdx/Hvz21pKQZbpBjtVimhQRCjJRtTH2PAGCjEmugLCAgmTfAFIhISAzFN1HckxqeEkPCMRPGhom4gkWAlMTFS2FKCrUUpinRptZsoSKhxu/jzxT1jpruzYep2GHrP75NMZs7Ze+eef/6T8585c2evgRPL4zcB+wBs77C9r/TvApZKWrLwYR+Zg9OvMPIGsXhEr/WhIyJet/opAKuAvV3tSQ5/Fw/wFeAySZM07/6v7vE8nwJ22P53V9+dZflns6SBzc4Hp19h2eIRBniIiIhjTj8FoNes6VntS4C7bI8CG4B7JP3vuSW9C7gR+FzXPpfafjfw4XL7TM+DS1dKmpA0MTU11cdw58rFYCIi5uqnAEwCq7vao5Qlni6bgB8B2P4tsBQ4GUDSKPBT4HLbz3R2sP18uX8J+D7NUtMctm+xPWZ7bOXKlf3ENMfBQ7kYTETEbP0UgMeAtZLWSDoOuBgYn7XNc8B5AJLOpCkAU5KWAw8AN9j+TWdjSYskdQrEYuAiYOdCg5nPv6ZnOD6/Ao6IOMyrzoq2ZyR9nuYMnhHgDtu7JH0NmLA9DlwH3CrpWprloc/adtnvdGCzpM3lKT8OvAw8WCb/EeCXwK1HO7iOdW9fwemnzAzq6SMijkmyZy/nv36NjY15YmLgZ41GRLSKpO22x2b3V/FL4IiImCsFICKiUikAERGVSgGIiKhUCkBERKVSACIiKpUCEBFRqRSAiIhKHVM/BJM0Bfzl/9z9ZJrrE9SmxrhrjBnqjDsx9+cdtuf8M7VjqgAshKSJXr+Ea7sa464xZqgz7sS8MFkCioioVApARESlaioAtwx7AENSY9w1xgx1xp2YF6Ca7wAiIuJwNX0CiIiILikAERGVqqIASDpf0h8k7ZF0/bDHMwiSVkt6WNJuSbskXVP6T5L0kKSny/2KYY/1aJM0ImmHpPtLe42kbSXmH5ZLmbaKpOWStkh6quT8/W3PtaRry2t7p6R7JS1tY64l3SHpgKSdXX09c6vGd8rc9qSkc47kWK0vAJJGgJuAC4CzgEsknTXcUQ3EDHCd7TOB9cBVJc7rga221wJbS7ttrgF2d7VvBL5ZYv4HsGkooxqsbwO/sP1O4D008bc215JWAV8AxmyfTXMp2YtpZ67vAs6f1Tdfbi8A1pbblcDNR3Kg1hcA4H3AHtt/sj0N/ADYOOQxHXW299t+vDx+iWZCWEUT691ls7uBTw5nhIMhaRS4ELittAWcC2wpm7Qx5hOBjwC3A9ietv0CLc81zTXMj5e0CFgG7KeFubb9a+Dvs7rny+1G4LtuPAIsl/TWfo9VQwFYBeztak+WvtaSdCqwDtgGvMX2fmiKBHDK8EY2EN8CvgT8p7TfDLxge6a025jv04Ap4M6y9HWbpBNoca5tPw98HXiOZuJ/EdhO+3PdMV9uFzS/1VAA1KOvtee+Snoj8BPgi7b/OezxDJKki4ADtrd3d/fYtG35XgScA9xsex3wMi1a7umlrHlvBNYAbwNOoFn+mK1tuX41C3q911AAJoHVXe1RYN+QxjJQkhbTTP7fs31f6f5b5yNhuT8wrPENwAeBT0h6lmZp71yaTwTLyzIBtDPfk8Ck7W2lvYWmILQ51x8D/mx7yvYh4D7gA7Q/1x3z5XZB81sNBeAxYG05W+A4mi+Oxoc8pqOurH3fDuy2/Y2uP40DV5THVwA/f63HNii2b7A9avtUmrz+yvalwMPAp8tmrYoZwPZfgb2Szihd5wG/p8W5pln6WS9pWXmtd2Juda67zJfbceDycjbQeuDFzlJRX2y3/gZsAP4IPAN8edjjGVCMH6L56Pck8ES5baBZE98KPF3uTxr2WAcU/0eB+8vj04BHgT3Aj4Elwx7fAOJ9LzBR8v0zYEXbcw18FXgK2AncAyxpY66Be2m+5zhE8w5/03y5pVkCuqnMbb+jOUuq72PlX0FERFSqhiWgiIjoIQUgIqJSKQAREZVKAYiIqFQKQEREpVIAIiIqlQIQEVGp/wLFVFoYBQqArQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend(['training', 'validation'], loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and validation accuracy improve consistently, but reach plateau after around 60 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1026/1026 [==============================] - 1s 919us/step - loss: 0.2381 - accuracy: 0.95870s - loss: 0.2\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.9586963057518005\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(X_test_sub)\n",
    "y_pred = predicted.argmax(axis=1)\n",
    "\n",
    "\n",
    "ImageID = np.arange(len(y_pred))+1\n",
    "Out = pd.DataFrame([test['filename'],y_pred]).T\n",
    "Out.rename(columns = {0:'filename', 1:'label'})\n",
    "#Out\n",
    "Out.to_csv('MNIST_Prediction_Batch_Elu_LR_0_1_Adam.csv', header =  ['filename', 'label' ], index = None)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Batch_Elu_LR_0_01_Adam - 0.956 Acc\n",
    "Batch_Elu_LR_0_1_Adam - 0.948 Acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dropout (Regularization)\n",
    "- Dropout is one of powerful ways to prevent overfitting\n",
    "- The idea is simple. It is disconnecting some (randomly selected) neurons in each layer\n",
    "- The probability of each neuron to be disconnected, namely 'Dropout rate', has to be designated\n",
    "- Doc: https://keras.io/layers/core/#dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://image.slidesharecdn.com/lecture29-convolutionalneuralnetworks-visionspring2015-150504114140-conversion-gate02/95/lecture-29-convolutional-neural-networks-computer-vision-spring2015-62-638.jpg?cb=1430740006\" style=\"width: 500px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(50, input_shape = (784, )))\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dropout(0.3))                        # Dropout layer after Activation\n",
    "    model.add(Dense(50))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(Dropout(0.3))                        # Dropout layer after Activation\n",
    "    model.add(Dense(50))\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dropout(0.3))                        # Dropout layer after Activation\n",
    "    model.add(Dense(50))\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dropout(0.3))                         # Dropout layer after Activation\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    adam = optimizers.Adam(lr = 0.001)\n",
    "    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = mlp_model()\n",
    "history = model.fit(X_train, y_train, validation_split = 0.3, epochs = 100, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend(['training', 'validation'], loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation results does not improve since it did not show signs of overfitting, yet.\n",
    "<br> Hence, the key takeaway message is that apply dropout when you see a signal of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1026/1026 [==============================] - 1s 797us/step - loss: 1.1125 - accuracy: 0.5871\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.587115466594696\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(X_test_sub)\n",
    "y_pred = predicted.argmax(axis=1)\n",
    "\n",
    "\n",
    "ImageID = np.arange(len(y_pred))+1\n",
    "Out = pd.DataFrame([test['filename'],y_pred]).T\n",
    "Out.rename(columns = {0:'filename', 1:'label'})\n",
    "#Out\n",
    "Out.to_csv('MNIST_Prediction_DropOut.csv', header =  ['filename', 'label' ], index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Ensemble\n",
    "- Model ensemble is a reliable and promising way to boost performance of the model\n",
    "- Usually create 8 to 10 independent networks and merge their results\n",
    "- Here, we resort to scikit-learn API, **VotingClassifier**\n",
    "- Doc: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_train = np.argmax(y_train, axis = 1)\n",
    "y_test = np.argmax(y_test, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model1():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(50, input_shape = (784, )))\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(50, kernel_initializer = 'he_normal'))\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(50, kernel_initializer = 'he_normal'))\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(50, kernel_initializer = 'he_normal'))\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    adam = optimizers.Adam(lr = 0.001)\n",
    "    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def mlp_model2():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(50, input_shape = (784, )))\n",
    "    model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(50))\n",
    "    model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(50))\n",
    "    model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(50))\n",
    "    model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(50))\n",
    "    model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    adam = optimizers.Adam(lr = 0.001)\n",
    "    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def mlp_model3():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(50, input_shape = (784, )))\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(50))\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(50))\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(50))\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    adam = optimizers.Adam(lr = 0.001)\n",
    "    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = KerasClassifier(build_fn = mlp_model1, epochs = 30, verbose = 0)\n",
    "model2 = KerasClassifier(build_fn = mlp_model2, epochs = 10, verbose = 0)\n",
    "model3 = KerasClassifier(build_fn = mlp_model3, epochs = 20, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1._estimator_type = \"classifier\"\n",
    "model2._estimator_type = \"classifier\"\n",
    "model3._estimator_type = \"classifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_clf = VotingClassifier(estimators = [('model1', model1), ('model2', model2), ('model3', model3)], voting = 'soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('model1',\n",
       "                              <tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x0000023E479B19C8>),\n",
       "                             ('model2',\n",
       "                              <tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x0000023E479B1748>),\n",
       "                             ('model3',\n",
       "                              <tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x0000023E479B1788>)],\n",
       "                 voting='soft')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ensemble_clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9627474870545233\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy:', accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(X_test_sub)\n",
    "y_pred = predicted.argmax(axis=1)\n",
    "\n",
    "\n",
    "ImageID = np.arange(len(y_pred))+1\n",
    "Out = pd.DataFrame([test['filename'],y_pred]).T\n",
    "Out.rename(columns = {0:'filename', 1:'label'})\n",
    "#Out\n",
    "Out.to_csv('MNIST_Prediction_Ensemble.csv', header =  ['filename', 'label' ], index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning by Learning rate & decaying rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "epochs = 100\n",
    "learning_rate = 0.01 # initial learning rate\n",
    "decay_rate = 0.01\n",
    "momentum = 0.8\n",
    "\n",
    "# define the optimizer function\n",
    "adam = optimizers.Adam(lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(50, input_shape = (784, )))\n",
    "    model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(50))\n",
    "    model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(50))\n",
    "    model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(50))\n",
    "    model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(50))\n",
    "    model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    #adam = optimizers.Adam(lr = 0.01)\n",
    "    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def mlp_model_initial():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(50, input_shape = (784, )))\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(50, kernel_initializer = 'he_normal'))\n",
    "    model.add(Activation('elu'))  \n",
    "    model.add(Dense(50, kernel_initializer = 'he_normal'))\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(50, kernel_initializer = 'he_normal'))\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    #adam = optimizers.Adam(lr = 0.01)\n",
    "    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mlp_model()\n",
    "history = model.fit(X_train, y_train, validation_split = 0.2, epochs = 100, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdd3yV5dnA8d+Vkz0gIQkzhLD3DksQVBzgQpEiKgpaS93aamtt+1Kl9bXtq1atq+4tInWhCKKCqKASViDsTRIIgSRkr3Pu94/7JDmZnEAicLi+nw8fznnmfU6S67mf6x6PGGNQSinlu/xOdgGUUko1Lw30Sinl4zTQK6WUj9NAr5RSPk4DvVJK+Tj/k12AmmJiYkxCQsLJLoZSSp1WVq9efdgYE1vXulMu0CckJJCUlHSyi6GUUqcVEdlb37pjpm5E5BUROSQiG+tZLyLylIjsEJFkERnisW6GiGx3/5txfMVXSil1IrzJ0b8GTGhg/USgu/vfLOA5ABFpBfwFGAEMB/4iIlEnUlillFKNd8xAb4xZDmQ1sMkk4A1j/QBEikg74CJgiTEmyxiTDSyh4QuGUkqpZtAUOfoOwH6P96nuZfUtr0VEZmHvBoiPj6+1vqysjNTUVIqLi5uguAogODiYuLg4AgICTnZRlFLNrCkCvdSxzDSwvPZCY14AXgBITEystU1qaioREREkJCQgUtdhVWMYYzhy5Aipqal07tz5ZBdHKdXMmqIffSrQ0eN9HJDewPJGKy4uJjo6WoN8ExERoqOj9Q5JqTNEUwT6T4Ab3L1vRgJHjTEHgMXAhSIS5W6EvdC97LhokG9a+n0qdeY4ZupGRN4FzgFiRCQV25MmAMAY8zywELgY2AEUAje612WJyF+BVe5DzTHGNNSoq5RS9TLG8GnyAUZ3i6FVWOBJKUNpuYu9Rwo4lFdCYkIUQf6OEzpeTmEpaTlFtAwJoEVIAOGB/vj5NX0l7JiB3hhzzTHWG+D2eta9ArxyfEU7teTk5PDOO+9w2223NWq/iy++mHfeeYfIyMh6t5k9ezZjx47l/PPPP9FiKuWzFiQf4K5313LpgHY8fe2QOrcxxnCkoJTwIH+CA7wLwqXlLlzG1Lv9rsx83l+dyhcpB9lzpBCnyzYjdogM4fZzuzFlaByB/vUnR/YeKeC/a9Lo274FF/VtW7n8p91ZzHoziZzCssplA+Ja8skdY7wqd2PIqfbgkcTERFNzZOzmzZvp3bv3SSqRtWfPHi699FI2bqw+bszpdOJwnNhV/WQ5Fb5XdXyOFpWxP6uQfh1a1rtNmdPFrswCurcOb5ZaItggVu4ydI0Nr7Z8f1Yhuw8XMLZHnSPyGy23uIzxj31DdkEpTmNY8puxdGsdAdjg/uyynXy5OYOdh/LJLS4nOiyQG0cncP2oBFqG1O5ZVlzm5Osth1iccpCvNx/CZQxPXzeEc3u2rtwmaU8Wj3y+hdV7s3H4CWO6xTAgriVdY8MJDvDjP8t3sXZfDnFRIdxxbjeuGhpHgMMGfJfL8MPuI7z6/R6+3JxBRZi9pH87HprUl2+3Z3L//A3EtQrhN+f3oKjMSW5RGS1CApia2LFWeb0hIquNMYl1rtNA751p06bx8ccf07NnTwICAggPD6ddu3asW7eOTZs2ccUVV7B//36Ki4u5++67mTVrFlA1pUN+fj4TJ05kzJgxrFixgg4dOvDxxx8TEhLCzJkzufTSS5kyZQoJCQnMmDGDBQsWUFZWxvvvv0+vXr3IzMzk2muv5ciRIwwbNoxFixaxevVqYmJijvsznQrfq2o8YwzTX/6RlTuP8P4tZzG0U/VxiC6X4dMNB3jsi63sPVJI/w4tuX9CL8Z0b9zvijGmwbactfuyueHlnxCBT+88m/joUACOFpZxyb+/JS2niHduHsmortGV+7z1w17+tWQbZ3WL4aK+bRjXI5bQQJtYEKj3gvTQghReW7GHV2YO4/a313BBnzY8OW0wAPNXp3Lf++sZ2DGS/h1akBAdxvc7DrN0aybhQf7cPb47N5/dufKzFJU6mf7yj6zem02rsEDO792ajWm5bM3I45Er+3PV0Die/noHT361jXYtQ7h+VCcmD+5A6xbBtb6fb7Zl8q8l21ifepSOrUKYdXYX9mUV8mnyAQ4cLSYqNIDrRnTi2hHxfLg2jSe/3E6Qvx95JeWM6hLN89OH0jK0abo4+1Sgf2hBCpvSc5v0nH3at+Avl/VtcBvPGv2yZcu45JJL2LhxY2X3xKysLFq1akVRURHDhg3jm2++ITo6ulqg79atG0lJSQwaNIipU6dy+eWXM3369FqB/t577+XOO+/k2WefZc2aNbz00kvccccddOjQgQceeIBFixYxceJEMjMzNdCfgb5IOcisN1cT6O9H2xbBLLz7bMKDbLDclJ7L7/+7no1pufRqG8GkQR1464e9pOUUMaZbDHMm9aVLjdp3hQNHi3j5291sOZhHanYh6TnFxEYEMTg+kiHxUYztEVNZi16/P4fpL/1IVFggOYWldGwVyn9vPYsgfz9mvbmapVsO0aZFMMYYPr97LC1DA/hpdxbXvvgDXWLDOJxfSlZBabXzBziEsd1juXxQe87v3YYw92dKST/KZf/+jmtHxPO3K/rzyOebeXH5Lpb8dhwBfn5MfHI5fdu35N1ZI3F4XChS0o/y+Bfb+GrLIWaN7cIDE3vhdBl+/eZqvt56iP+bMpArBrXH3+FHfkk5t761mm+3H6ZLbBi7MguYNKg9f7uiHxHBDQdiYwxfbznEv77cxsa03Gqf48I+bQkJrLrj35aRx+yPN9KtdTizL+3bYMqnsRoK9KfcpGani+HDh1frg/7UU0/x4YcfArB//362b99OdHR0tX06d+7MoEGDABg6dCh79uyp89iTJ0+u3OaDDz4A4Lvvvqs8/oQJE4iK0tkkTner92aRXVBGbEQQMRFB5BWXsfNQATsz8+kf17JaGqFCSbmThxduplvrcP46qR/XvfQDcxak8M8pA/l6SwZ3vLOW8CB/Hp86kEmDOuDwE24ak8BbP+zjqa+2M/HJb7nvwp7cNKZzZVA8WljGs9/s4LXv9+Ayhj7tW9K3Q0su7NuWtJwi1uzN5tPkAwD0bteC83u35vUVe4gMC2DurJFsPpDLL19P4sFPUujWOpwlmzL4n0v7MCwhisnPruB/Pt7IHy/uzW1vr6Zjq1Dm33oWoQEOVu/NJmlvdmXOO6uglMUpB/lqyyGC/P3o1jqcbq3D2Xwgl6jQQH53YS8AfnV2F95YsZcnv9xOek4RfiI8fvXAakEeoG/7lrx4QyIPLUjhheW7yCsup9zp4qsth/jbFf2YMjSuctvwIH9emTmMP324gYUbDvLYLwYyeUgHr3qniQjje7fhvF6t2ZB2lPhWoUSG1t1Y3KNNBHNnjfLit6NpnXaB/lg1759LWFhY5etly5bx5ZdfsnLlSkJDQznnnHPq7KMeFBRU+drhcFBUVFTnsSu2czgclJeXA7bWoE4t+7MKef6bnbSPDGFwfCQD4yIra6EVypwuHlm4hc6xYVw5uAPhQf4cyivmoQWb+MwdPOsz+9I+3DSm+oC2V7/fw94jhbx+03BGdY3mtnO68fTSHZS7DB+tTaNP+xa8PGMYbTzSDEH+Dn45pjOXDWjHHz/cyMMLN/PJ+nSiwwNJzS5iX1YhZU4XVwzqwG8v6EHHVqG1ypKeU8TilIN8sj6df3+9g7ioEN791UjaR4bQPjKE28/tyjNLd+IncFHfNtw02g5uvOf87jz6xTaS9mRRWOrknV+NpIW7hjyiSzQjulSvDM2+tA9Je7NZsukg2zLyWb03m/ScIv519aDKFEdMeBDTR8bz4re7AXji6kHERdUuM9hU0IOX9yUsyJ9nl+0E4K7x3Zk+slOtbQMcfvxzykD+98r++DsaX9MWEQbE1d/p4mQ67QL9yRIREUFeXl6d644ePUpUVBShoaFs2bKFH374ocnPP2bMGObNm8f999/PF198QXZ2dpOfw1eVlruY82kKAQ4/7r2wZ2WaoyFFpU5W780mNbuQ1OwiRGDW2C6Vt/GZeSVMf/lH0nOKKHPai3CQvx8vzUjk7O5VDZCvfr+bV763AenvCzdzQZ82fL3lEMXlLu69oAdje8RyOL+EQ3klhAY66NY6nLjIUO7/bzJzPt1EVkEp917YAxEhM6+Ep7/ewfherRnnbuS8+/zuLN+eyQdr0tx560GVOe+aWrcI5sUbhvLJ+nSe/HI7LmPoGhvGOT1imTwkjj7tW9T7fbSPDOHG0Z25cXRn0nKKCAt0VKu1/vaCnmxMy2V/ViH/nDKwsiZ86zndWLY1k6S92Tx73RB6tIlo8Hv38xOGd27F8M6tKpe5XKZW7n7W2K7M/Wk/5/VuzaRB7Rs8pojw+wm9aNcymJzCMu44r1uD2x9PkD/VaaD3UnR0NKNHj6Zfv36EhITQpk2bynUTJkzg+eefZ8CAAfTs2ZORI0c2+fn/8pe/cM011/Dee+8xbtw42rVrR0REw380Z4rdhwsA6BgVUuuPtLjMyS1vrWbZ1kxE4IuUDP5x1QDGdI+hpNxZuW/31hE4/IRyp4t5Sak88eU2DuWVAODwk8o+3M9cO4T46FBufO0nMnKLee/Xo+gcHca6/Tk8vHAz972/nsX3jCUyNJCM3GKe/HI75/VqzZ3ndeOtH/bxaXI6Q+KjePjKfvXmygGeuW4If/5oA08v3cGilIMUlpSTmW/L86dLqtpVAhx+vHB9It/tOMyVgzvUSl/UJCJMGtSBSYPqnHbKKx0iQ2otc/gJr904jDKnqZZ3dvgJL81IZOvBvFq1d2/V1UAbGxHE8t+fS8uQAK8H/10/KuG4zu8LTrvG2DNVSUkJDocDf39/Vq5cya233sq6detO6Jin+/fqchme+Go7T321HYBAhx+dY8IY0imKcT1iGdQxkrvnruWnPVk8fEV/erYN53fvJ7PrcAFxUSGk5xThTg8TFuhgQFwkGXnF7MosYGinKG4/tys927agTUQQa/blcOe7a8gpLKNrbDhbM/J46YZEzu1VlUffmHaUK575nov7t+OpawZz99y1fL7xIEt+M5ZO0TbVd6yeLJ6MMfxn+S5W7DxCbHgQsRFBnN09htHdjr8BXvkubYz1Afv27WPq1Km4XC4CAwN58cUXT3aRGs0YQ0m5y+uBLA05WlTGPXPXsnRrJpOHdGBkl2h2ZuazPSOfBevTefenfYCtUT5x9aDKGuzCu8/m2WU72ZmZz+QhcXSNDcMY21Vw7f4cwoP8efGGRM7v3bpaQB7euRWf3XU298xdx3c7DvPoLwZWC/IA/Tq05O7x3XlsyTZahQXy8bp07jqvW2WQh8ZNPSEi3DKuK7eM63oiX5VSWqM/k/2c36sxhvveT2bRxgM8cHFvrhsR36igd7SojM0HctmUnktKei7f7zjMkYISZl/Wl+k1jlVa7mLNvmxW7DzCyC6tOKtr09WAnS5DRm4x7etIXwCUO11MeX4l6/bn0CEyhC9/O65a9zqlmovW6FWzKHO6yCuu6hV04Ggxm9Jz2XQgl4ToUGacVTWt9PzVqfx3TSodW4Xw5482snDDAf5x1YA6e3iA7dGybFsmK3YcZmP6UfZnVfVQio0Iol/7Ftxx3mCGdmpVa99Afz9Gdolm5HHmhBvi8JN6gzzYhrzHpw7k1rfW8MdLemuQV6cEDfTquGzPyOPG11aRml27i2iQvx8l5S42pOXy96v6s/dIAbM/TmFUl2je/OVw3l+dysOfbebiJ7/l3Vkjqw3jX7Uniwc+2MCOQ/mAbfgbFB/JtGHx9G3fgj7tW9A6IrjWOU8lXWLDWfybsSe7GEpV0kCv6tRQo+GPu47wqzeSCApw8D+X9sHf3SsiOjyQvu1b0qlVKE8v3cHjS7ZxKK+YTHfXwSemDcLf4cc1w+MZ0y2GaS/8wA2v/MS8X4+iW+twVuw8zC9fS6JNiyD+59I+nNMzli4xYTqlslInSAP9Ge5Qrh3mXhFMnS7Dayv28OSX22gfGcKFfdtyYZ82hAY6yMwrISU9l79/voWOrUJ47cbh9aZe7hrfnbYtg3nggw04XYZXb6w+iKdjq1DeunkEv3h+BTe8/CO/vbAnf/5oAx2jQnnnVyOJjQiq87hKqcbTxthmEh4eTn5+Punp6dx1113Mnz+/1jbnnHMOjz76KImJdbafAPDEE08wa9YsQkNtQPVm2mNv/bQ2manv7ScuKoTLBrZnaHwUT361nQ1pRxnTLYbScher9mZR81dkWEIUL96QWO8w72rn2J1FRm4xlw2se1BLSvpRpr3wA3nF5fRqG8HbN48gOlyDvFKNpY2xJ1H79u3rDPLeeuKJJ5g+fXploF+4cGGTlCsjt5ijReWM79WacpfhheW7cLoMMeFBPH3tYC7p3w4R4XB+Cd9uzwQgNjyY2IggurUOP+bAnAqeIxzr0rd9S16/aTjzVu3n/gm9iDpJD5RQypd5FehFZALwJOAAXjLG/L3G+k7YB4zEAlnAdGNMqnvdP4FLsI8tXALcbU612wgv3H///XTq1KnywSMPPvggIsLy5cvJzs6mrKyMv/3tb0yaNKnafp6zXhYVFXHjjTeyadMmevfuXW2um1tvvZVVq1ZRVFTElClTeOihh3jqqadIT0/n3HPPJSYmhqVLl1bOhhkTE8Pjjz/OK6/Y57rcfPPN3HPPPezZs6fe6ZDB5t4z8ko4lFtMaKCDF24YjMNPOJJfwqo92YzqEl1t2tSY8CCuHBxHcxoSH8WQeJ2kTanm4s2jBB3AM8AF2Ad+rxKRT4wxmzw2exR4wxjzuoicBzwCXC8iZwGjgQHu7b4DxgHLjrvEn/8BDm447t3r1LY/TPx7g5tMmzaNe+65pzLQz5s3j0WLFvGb3/yGFi1acPjwYUaOHMnll19eb+Phc889R2hoKMnJySQnJzNkSNVTch5++GFatWqF0+lk/PjxJCcnc9ddd/H444+zdOnSWtMRr169mldffZUff/wRYwwjRoxg3LhxREVFsX37dp5+8TXufvBR/njHTbz//vvccMMNFJU5OZBTRH5JOa1CA/EPDaysmUeHBzGhX1uUUr7Hm9l7hgM7jDG7jDGlwFxgUo1t+gBfuV8v9VhvgGAgEAjCPms240QLfTIMHjyYQ4cOkZ6ezvr164mKiqJdu3b88Y9/ZMCAAZx//vmkpaWRkVH/x1u+fDnTp08HYMCAAQwYMKBy3bx58xgyZAiDBw8mJSWFTZs21XcYwE5bfOWVVxIWFkZ4eDiTJ0/m22+/BaBTQmdiE3oS4PCjS+/+JG3cxt4jBezIyKOozEn7yBA6RIWgnVmUOjN4k7rpAOz3eJ8KjKixzXrgKmx650ogQkSijTErRWQpcAD7AJmnjTGba55ARGYBswDi4+MbLs0xat7NacqUKcyfP5+DBw8ybdo03n77bTIzM1m9ejUBAQEkJCTUOT2xp7pq+7t37+bRRx9l1apVREVFMXPmzGMep77sV0mZEz//AEICHHSNDadNi1AOZuWQW1ROdHgQrSOCfHJ2PqVU/bz5i6+r3lczytwHjBORtdjUTBpQLiLdgN5AHPaCcZ6I1BpJYox5wRiTaIxJjI1tmmdMNodp06Yxd+5c5s+fz5QpUzh69CitW7cmICCApUuXsnfv3gb3Hzt2LG+//TYAGzduJDk5GYDc3FzCwsJo2bIlGRkZfP7555X71Dc98tixY/noo4/IOprH3ows5v/3A4YOH0lajs37d4oOw89PCPD3Iyo0kP5xLWkfWXt2R6WU7/OmRp8KeD6tNg5I99zAGJMOTAYQkXDgKmPMUXdN/QdjTL573efASGB5E5T9Z9e3b1/y8vLo0KED7dq147rrruOyyy4jMTGRQYMG0atXrwb3v/XWW7nxxhsZMGAAgwYNYvjw4QAMHDiQwYMH07dvX7p06cLo0aMr95k1axYTJ06kXbt2LF26tHL5kCFDuHb69YwYMQJjDJOvuYGIuB7k7d9HgEOa9BFlSqnT2zH70YuIP7ANGI+tqa8CrjXGpHhsEwNkGWNcIvIw4DTGzBaRq4FfAROwdwaLgCeMMQvqO5+v9KNvakWl5ZQ6DeFBDhx+fuQVl7H3SCEBDj86uR/KXO504e/w83p2SP1elfIdJ9SP3hhTLiJ3AIux3StfMcakiMgcIMkY8wlwDvCIiBhsbf129+7zgfOADdh0z6KGgryqW1FpOTszC3AZgyCEBjooLHMS5G/nXw+oSMc0wfS/Sinf41U/emPMQmBhjWWzPV7Pxwb1mvs5gV+fYBnPaGXlLvYcKcThJ8RHhlJQWk5+cTnhQf51PlFJKaVqOm1GxjbmyTynK2PsXOcuAxHB/oQEONhzpACny9A1NpyQQActQgKg5bGP5c25lFJnhtMi0AcHB3PkyBGio6N9Othnuh8SLdipB8A2bHSKDmvSec2NMRw5coTg4FN7ul+lVNM4LQJ9XFwcqampZGZmnuyiNJmadyjFZU6O5JcSEuggKjSAknIXxWUuAv2FtDx/0pr4/MHBwcTFNe/UBkqpU8NpEegDAgLo3LnzyS5Gk9mekcfk51bQvmUI142MZ1hCK2588QfaRATz4e1nERp4WvxYlFKnCY0oP7P8knJueWs1Qf5+BPr7Mftj20s1Itif/1w/VIO8UqrJaVT5GRljuP+/yew+XMDbN49kVNdo1u/P4YM1qUzo146EmLCTXUSllA/SQP8zevX7PXyWfID7J/RiVFf74OqBHSMZ2PHEHyKilFL10UDfzIwxrNqTzcvf7eKLTRmc37sNt4zrcrKLpZQ6g2igb0Yp6Uf54wcbWJ96lMjQAG4d15Xbzu3m011ElVKnHg30zcAY+4DtRxZuITI0gL9d0Y+rhsQ1aV94pZTylgb6JpZfUs7d767lqy2HOL93a/45ZSCt9Dmo6nTjcsFPL0CXcdBaJ7473Wmgb2J/XbCJpVsP8eBlfZhxVoKmadTpx+WCBXfC2regzySY+sbJLpE6QTojVhNasimD95L2c8u4rswc3VmDvDr9uFzw6T02yIe1hj3f2WXqtKaBvokcyS/hgQ+S6d2uBfec3+NkF0c5y2HeDPj2sZNdEu/s/hYyUo69XXNylsNnv4E1r8PZ98H5D0LhEcis9fRPdZrRQN8EjDH88cMN5BaV8/jUgfp0p1PByqdh00fw1RzY+MHJLk3DCrPgnanw+mWQd/DklCH3gD3/6tdgzG/hvD9DZ/dTP3c30wPhnGWwd2XzHFtVoxHpOJU7XXyanM4fP9zA+Me+YXFKBr+9sAe927U42UVTmdtg6f9Cz0ug4wj4+A7I2HSyS2XlZYDLWX3ZmtehrBBK8uHDX//8qZKdS+H5MXBgHVz5Apz/FxCByI4QlWDvNprDj/+BVyfAls+8297lglUvn7yL4YnIP2Tv2Cr+lRX/rKf3KtCLyAQR2SoiO0TkD3Ws7yQiX4lIsogsE5E4j3XxIvKFiGwWkU0iktB0xT95/rFoC3e8s5ZP1qWTEBPGXyf15Vdn60Cok87lhI9vh8BQuPRf8IvXISgc3rsOinJOTplK8mHt2/DqxfBYD/j0N1XrnGXw4wvQeRxc/E/YtczejfxcctPh7V9AWCzMWgYDr66+vvNYd57eWdfex88Ye/cA8MWfobz02PuseAo++y18+dDxnTPpVVj6yPHte7yy99qKxuO94bmzqv4tuPtnLcYxA72IOIBngIlAH+AaEelTY7NHgTeMMQOAOYDnt/kG8H/GmN7AcOBQUxT8ZNqWkccr3+9hamIc62ZfwCszh3H9qAQcftr4etL9+Dyk/gQT/gERbaBFO9trJGcffPmXhvctyraBz1t7V8CjPSH5/fq3SVsDT/SHj2+zNdGu420NfsdXdv2mjyEvHUbdDkNmQO/LbbrpuyfgvzfDY71sIG4u25eAqwymvAKxPWuvTxgLJUfhYHLTnnfvCjiyHfpdBVm7YNVLDW+ftga+/iv4B0PKh42/aJeX2u/1m3/A4e3HX25vFWbZC/q/h0DyPEj8pf09nPoG9LrUphWLc5u/HG7e1OiHAzuMMbuMMaXAXGBSjW36AO7fXJZWrHdfEPyNMUsAjDH5xpjCJin5SWKMYfbHG4kI9ueBib3PjEf5bV3UvLfLpQWQ8tGJ1xrT19k/5h4TYMDUquXxI6HbBbD/p9r7uJyw/Ut4fyY82gOeHgaHthz7XMbA4j9B/kH44GYbmGs+tSt9Hbx5hb2juHER3Lkapr0DMT1sja4kD354FqK72fKJwGVPQnhre1HavRxadIDtX0Dq6hP6auq1Y4k9R3195Tufbf9vbPpm/Vz48Nb6f6ZrXoegFnD5v6HrefDN321wrEtJPvz3lxDeFq55F8qLYEMDF9e6bFsERVmAge+fbNy+jWGMvfA/PQxWvw5DZ8Jda+3dWp9J9t/ou6G8GLZ82nzlqMGbKNUB2O/xPtW9zNN64Cr36yuBCBGJBnoAOSLygYisFZH/c98hVCMis0QkSUSSTvWHiyxIPsAPu7L43UU9iTqVB0KVlzRNL45Dm+Hdq+GL/znxY9Vnxb/h/Rkw7wYoKzq+YxQcgfeuh9BouPxpGzQ9xfa0NTlnefXlS2bD21fZlMnQmRAQYtM8xUcbPt/mBZC+Bi55HPpOtoH5s3vh4EZbUzuwHt6YBEEtYeZn0GmULVNAMEx6Bo6mwjtXQ9pqGHEL+Ln/FENbwa+Wwh1JcO9WuOEjGxB/eLb6+cuKba04a1f9aQ+XCw7vqP8zOMtg1zfQ7fza31eFiLb2wtSYBtmc/bY2u/4dWP1q7fWFWfbCPmAqBIbBhQ/bi96yv9d9vM9/D1m7YfIL0OVcaDvABtHGPA5z3TsQ0c7+jNfPbdydW3Gu/bnuXl7/nYSz3FaI3rzCXvijOsGvv4FLHoOWNcJl3DDb9rF+rvdlOEHeDJiq6zeg5jd8H/C0iMwElgNpQLn7+GcDg4F9wHvATODlagcz5gXgBYDExMRT9mGm+SXlPPzZJvp3aMm0YfEnuzj1MwY+utX+Mf12k/1jPV4VAWbTxzDxHzYQHS+XE5ylNph62rbIBugtn8Hrl8O17zXuPC4n/PcmW7u+aRGEx9bepmUS+3wAACAASURBVHVvm6LI3g0x3auW7/gKEs6G6f8F/yDoc4XtffLhrXD1W1UBuOb5vv6bDYBDZsDQG+0f84p/Q5L7V1v8bE155gKIrPG70nE4jLwNfngGglvCoGurr49oY/8BBEXAkBtsSuroHHselxPengJ7KmrZAu0G2u/N82e9+AG73w2f2BGuNaWugpJcG+gbknA2JL9nLwyOgIa3BVj0B/s72GEofDnHpqPCW1etT54HzhL73QG06WMD8KqXYMDVEDe0atu1b8O6t213z4TRdtnQGfaimr7GngPshc8RAH51TDOSf8jeFZ11pz3Pmjfs7/WFf6vjO1kNKR9Azl6bX8/ZB8UewV387IUmfiQEhttlJbn27yM/A0JjYOI/YdjNdZcF7EV1wNXwzT/tBadF+2N/pyfImxp9KtDR430cUO1yaIxJN8ZMNsYMBv7kXnbUve9ad9qnHPgIGNIkJT8JXvt+Nxm5JcyZ1PfUzsevfxc2/heM88S6xhUchvXvQfxZ9g8zed6JlWvZI/DvofZuo0JeBqSvhZG3wtTXbU345Qsalyr6ao6tkV/yWNUffk0V+edDHn3Cy4rh8DbbM8c/yC5LGA0XPQxbP4OvHrJ5+5rWz4XDW20XRIe/vRhc+De45XuY8iqc/5ANKjM/tTW3upz3Z4gfBeP+YGu1DRk+C4wLVr1o33//pA3y5/4ZJj0LY39n71be/oWtGYP9uf34vH298pm6j7t9Cfj5130R8NT5bCjNtz+nY9m22KYkxv0ernje9iZaMrtqvTE2bdN+MLQbULX83D9DyzjbzTRrl12WvtbeGXQeC+c8ULVt/19AQKit1YOtIDzWo/4GzuT37N/CoOugVWfoeyUkvVa9dp62xn5/L51nLziZ2+xFs99VcMEc+MVrtjIw9vc2wK9+Hb77l/23+nX7ezftHbh3C4z4df1BvvIzTAUMbJh/7O+0CXhTo18FdBeRztia+jSgWhVERGKALGOMC3gAeMVj3ygRiTXGZALnAUlNVfifkzGGD9amMbJLKwbHR53s4tTvyE747D7oNAYyNtoA6JmvboykV22Av/Rf9g5h9Wv2l7iu2/xti8ERCF3PrftYLhesexdy02Dr59D3Crt8xxL7f/eL7B9+WCy8dZX9g5+50Oa3G5KbbgPf4Ottzbc+Me5BbJlbq5ZlbrEBoG2/6tuOuMXm179/wh67bX9bg2vVxdbOlz1iA1Xvy6vv17Zf7WPVJzDU3n14I6qTbcBLetXms5c+bIPV2PuqfhYdh9tU0LwZcN6fbNDrNMamjJb/n70QeN7JAOz40l7kgls2fP4Ed57+28eh96UQ2cl+bxU13oAQu03r3rDwd/a7HnUH+AfC6LvsoLXB19vAueUzOLTJtkV4Cou2gfTlC+CtKXDNXJuKC29tL54Oj1AV3NKmyzbMtwH/x+dsimztm5B4Y/WLvTE2bRM3DGLdvwOj77YVocV/ssfasxwOboCQKBg/215YgyLq/i6OdffjrZhutpzJ8+x31MyOGeiNMeUicgewGHAArxhjUkRkDpBkjPkEOAd4REQMNnVzu3tfp4jcB3wldj6A1cCLzfNRmldKei67MgtO7S6U5aW20coRAJP/A4sesDlYY+rPwTZ0rFUv2l4irXvZ2+UFd9vb/Y7Dq29bkmd7iAS1gHs21J3uSF8Duan29bp3qgL9tsUQ0d4GU4BOZ9kuke9Osw2k18yt/kde07ZFgLGBpSGBYTZIZ3o0tGZstP+36V99WxG44jn7mfd8Z++K1r5la6cVLn+q8d/piRh1O2z+xF4EI9rBpU9UP3/3C+wFecFdtrxhsfCLVwGxF6sfn7d3PBXyMmxPmvGza52qlrAYe6HZ8hls+7z6OnHYoO/5esYCG+TBplyS37fpsIrtYnvbmnJNMd3tz/v1y20XRPGDXy62569p6AxY95YN8sNnwbj74dmRsOiP9gJa8d2kr7UXlkufqNq33UD7e73uLXAE2d/nC/5q0zrBP+M4mAFX2/aHjBRo09f+nZbm13+ROQFeTWpmjFkILKyxbLbH6/lAnfcg7h43A+padzr5eF0aAQ5hYr8TyHc3t5VP21/sqW/a2+Au42xwOLLT1iBqKsyCn16EkbfUrtWlfGBzjpPcOfp+V9ka0OrXawf6tW/bPGVJLuxbAQljap8r5UPwC7B/oEmv2NRMSCs7WKff5OpBq8eFcOnj9sLy4a9tgN7zrR30dN286sff+jlEda67a2BNsb2q1+gPbrQ1wlZ1PHjez89edDqdZdMQxtjpAHL22lx1/Mhjn68pdRxh7yIOrIerXoKQOp5KNnQG5B2AFU/D1W9W5cX7T7UX13P/VNX2sdPdSc7bGuq0t+3FPzfV1uT9/O2dRkR723C993v7M2rRvmpELdg7l8n/cf/eDLPdNWO613+RjB9pG10/utVemNoPrnu7uGE2ZdW2v+3JAvbzfXqPzZf3vcL+nFY+bbtk9ptcff/JL9rune0G2Qbyk6HvZFsZmzfDXtRy9tnPe9Pnx963kXT2Si84XYZP1qczrkdrIkNP0Z42znIbtLucC33cKYUu7jTK7mW1A73LaWv/O7+2Neaz761aZ4zN68b0hG7j7bKgCBvsN7wPE/636sLgctpaVfvBNq+Z/F7tQG8MbPrEpnVG3GpzoOvn2n1K86DHRbU/z9CZtmfK8v+zQaX9EFvOH56rOn5Jvr1jGXazd7Xr2J52e5fT5lAPboDWfY6dTwV7/LCYumuXPwcRm8LI2dfwReacP9ifpWej6chbbe11zeswxj1Ya8eXEN7GNix6yz/Qpq9a1birDYu2v3N9Lq97v4oLprf6XgG9Lmm44VfEtnN4GnKD/RtYMtte5Bb+zt61nXVn7YpMWLT9dzKFx9o04Z7lNh3W/QJbs28GZ0An8BP30+4sMnJLmDSo+VvHj9vWz+zAm+Gzqpa16gItO9o8fU1f/9UG+dBoWyP37Kq2d4W9rR95S/UAOnSGTV+s8Zi2dtsiyN4Do++B3pdByse1h3enr4Wj+2zNK6abrZ2ue8f2hHAE2lGhdTn3T/Dr5XD/Xrh5ie2lsc2jT/+upbYNoedE776j2N52++w99vNmbPA+p34qaNX52A2nUDtAtu1na9k/vWjvgLZ+bn/2Xcf/vOmnxvCmd09Nfg7bkJ6zF16daO/Arn6r7t41p4oJ/wu3fGfvmC56uHYPrCaigd4Ln6xPIzTQwfm925zsotRv1Us2qHvWjkVsEN39bfWBK5s+tr0Fhs60PQqydlYfTPTDs7ZhasC06udoP8Q2Bi75i03FAKx8FlrG2xzuwKvtKMrti6vvt+kjWyvvebF9P+g622tlzRu2dl5fg6u4uw1WrB8yA1zltrsd2IAVHOl9GiW2l/0/c4u9Wyg+Cm1Oo0B/Is66yzaEvzvN/ivK9v4CeTrpeq5trxl5G9z+k618KA30dTl4tJgVOw9TWu6ipNzJwg0Huahv25P7KMBVL9k5UeqSudU2wCXeWDsN0eUc2w/4wHr7/kAyfHQbdEi0/X37TLJ56nVv2fVZu22jW+JNNr/qScTm/zsOh/m/tPOG7P0ORsyyaZXO42w6wLMbpjH2wtJ5XFV+uO+V4B9ic/rd60jb1Cemm+1JsuYNm6ratgi6X+h97a+i10XmlqqG2Lb969/el3S/wAa+Wcvsv1tX+m4QvOhhmPDIz9uweorTHH0Nxhh+/dZq1u/PISLIn74dWnC0qIzLT2bapiQPvphta7P9JtfOE696yaZAKgageKqcavYbu80bk2y+cuobtu94xSChjR/a+WF+/I+9WAy7ue6yBIXDde/b3h/f/B0CwmzXObD79f+FPUZhlg3sB9bbVMmY31YdI7iFzecmv2cbXhtj6Ew78vDbx+yteWNqpUER0CLOXhgrZohsppzoKcmbBmvlk7RGX8PXWw6xfn8ON43uzMT+bdmWkU+HyBDGdKsRXNNW2/6+mz/1fih2fdsda/8N86GswOaXk2oMKS/Jt/3T+15Zd0NhRBvb4Jg8D9643PZAmPlp9WHZg661jaLr3rbdCPtObni0XlAEXDffpmLOub96D5ABU+0I1B+etXcgC++z3e56XVr9GONn2wE1NRv2jqX3ZTat9M0/bC+eisZib8X2dNfoN9jeOs3QlU2pU43W6D0YY3h8yTbiW4XywMW9CHD44XQZyl0uAmpOXrb8UTvYZ8cSm0c+73/s7XF98jPtqLvEm6p6PoDtqfLWVTDm7vpr0Wtet8E6op3t2z76rqqRnOvesUG6vn3Bpk1+fM7uP/PT2sG102jb6r/4T/ZiMuq2+o9VIbiFnWCqprYDbKPn8v+z71vGwwUP1e7h0DIOBl1z7PPUFBBs2w5+fM7erRxrsE9Nsb1s987i3NOrIVapE6A1eg+LUw6Skp7L3eO7VwZ2h58Q5F8j752bbvPDo+6wA2uKc+3w6Yqh23XZssA9Ve6DtiEU7IRTr19me6TUNbMi2NRH+lqblhl1m+3bXtEQemiLPV6n0bZfcX0GT7eNqDM+heiutdf7+dlavbPEDsuvr++yN0RsP+grnoO718NvNtjubU1p6Ezb77i+7nwNad3Lzn6Yvbv2QCmlfJTW6N1cLsO/lmynS2zYsbtRrn3bzj2SeJMNnDE9bW09c2v9qYjNn9pUQYehNjgXZtk+6a5yaNXVXgTqsvp1m24ZMNWmLGJ62j7uPSfC3GvtiM+rXmq4m1zbfnD9hw1/pkHX2Vz/2Psa3s4b7QZUn8ekqbXuZaf8jezU+H0ret6A1ujVGUNr9G6fbTjA1ow87h7fveE55l0u2+uj89iq2nHFxFVZu+vep/io7RXT+1K48j82B77iKTuT44wFtl95XYG+tMBeDPpMsg2bInbwy8Fk+7SinL12IrCmmP0usiP8bkfTzeXR3Fp18W6gU00Vc97AmdO1Up3xNNC7vblyL11iw7h0wDGC5q6vbapl6MyqZaGt7Dwv2Xvq3mfbF7aBstdlthvi5BftII4bP7dTtEbG23RQzXnFUz6yXRA9e9MMnGanDsjYCBf9b+NGHCrbcBzRzk6CVXP6YKV8lKZugIKSctbsy+bms7vUnn546SOwcb4Nqj0usqmUkFbVe5GI2Hk/6gv0WxbY/uUVeXSHf/W8dWQ8YODo/uo59HXvQHT36sE8IMTOAZK1q/ooWOW9+JF2quRTdVSoUk1MAz2wak8W5S7D6G41eobs/Nr2FQ+MsNPm9r7MjsYccUtVr5cKUQnVJ8yqUFZkH1U38Oq6Z3WEqpplzr6qQG+MbYgddE3tgFRzgibVOJNPywlUlTpumroBVu48QoBDSOzk8VSjgsPw4S228e63KfbBCNu+sI2ndQ1MiupsZ/WrGIhTYdcy2we+Zj9yT56BvkLeAdtt0jOnrJqGI+D45lJR6jSlNXrg+52HGRwfVTXFgTF2moCiHJj+ge2rPe53tiZ9ZGfVUHpPUQm2e2L+weqNo5s/tfngioc31KVFBzuoyDPQH95m/9dAr5Q6QWd8jT6nsJSU9FxGd/UYVZr0sp2Y68K/Vu+CF921/iH7dfW8cZbD1oU2t+/fwPTGDn87UtUz0GdqoFdKNQ2vAr2ITBCRrSKyQ0T+UMf6TiLylYgki8gyEYmrsb6FiKSJyNNNVfCm8sOuIxgDZ1Xk50sLbANswtmNa+yseHiFZ4Ns+hooyoJeFx97/8hOtrtkhcPbbE+eE3mwt1JK4UWgFxEH8AwwEegDXCMifWps9ijwhjFmADAHeKTG+r8C35x4cZveip1HCA10MDDOPV9L0itQeNhOadCYXhktO9rRmp6BvmK0a7wXXSAj42ukbrY2/CQepZTykjc1+uHADmPMLmNMKTAXmFRjmz6A+9lkLPVcLyJDgTbAFyde3Kb3/Y7DDEtoRaC/H5QWwvdP2blh4kc07kCOADt/S7ZH6iZ1lQ3gEV7MYx8Zbxtgy0vs+8Pb7ShYpZQ6Qd4E+g7Afo/3qe5lntYDFU/7vRKIEJFoEfEDHgN+19AJRGSWiCSJSFJmZqZ3JW8CGbnF7MwsqOpWufo1KDhkH8d2PKISqtfoU1c1PAeNp4qeNxUPxMg7UHejr1JKNZI3gb6u3EHNeXXvA8aJyFpgHJAGlAO3AQuNMftpgDHmBWNMojEmMTY21osiNY2VO48AcFbXGNvf/fsnbG7+eEebRnWuCvRH0+wTfbwO9O55W3L22snOQBtilVJNwpvulalAR4/3cUC65wbGmHRgMoCIhANXGWOOisgo4GwRuQ0IBwJFJN8Yc5xV5qb1/Y7DtAwJoE/bCPjuUTsz5JRXjv+AUQlQkGkfFJKWZJfFDfduX8++9A53Dx1N3SilmoA3gX4V0F1EOmNr6tOAak+wFZEYIMsY4wIeAF4BMMZc57HNTCDxVAnyTpdh6ZZD3NZ2C34vzoGDG6DbBfYZpserootl9l7bEOsI8v5RdRHt7HNVs/cCxj5Uo+J4Sil1Ao4Z6I0x5SJyB7AYcACvGGNSRGQOkGSM+QQ4B3hERAywHLi9GcvcJH7cdYR7S57hmgNLbcrlyv9AvykndlDPLpapSfaBJA31n/fk8LcDp3L22TRSdFe7TCmlTpBXkcQYsxBYWGPZbI/X84H5xzjGa8BrjS5hM1mwPo0/OH7C2etyHL94tWmCakUN/PA2OLCu4ac+1aWii2VRtp1zXSmlmsAZOTK2tNzF+o3JtJQCHN3Obbqac0iUnS5h8wIoL4a4xMbtH9kJsnbaLpqan1dKNZEzI9Dv/tb2gnH7bkcmHUvcPVvaDmzac0V1tiNiwfseN5X7doLCI3biNO1xo5RqIr4f6MuK7MO3v3qoctGC9QcYGrgfIw774I+mVJG+iWhvB1A1hueDMLQPvVKqifh+oE9bbWeV3PMdGENxmZMvUg4yNiINie1pH+TRlCoCfWPTNlA90Ed3b5LiKKWU73fr2LvS/p+bBtl7WJoWTEGpky7lu6Dd+KY/X0XPm8ambaAq0LeIg6DwpiuTUuqM5vs1+r3f2wZSgD3fsSA5nZ5hhQQWHYK2A5r+fO0G2snNuoxr/L4Vfek1baOUakK+Heid5XbgUr8pEBqNa893fLM1k2vis+36ds0Q6NsPht/vtgG/sfwc0OcK6HVJ05dLKXXG8u3UzcH19jF+CaOhIJOyXd9SUHo5o0LcPXC8HbXaWCGRx7/vlJebrhxKKYWv1+gr8vPxZ0HCGILy04iTTBLKdthukBUpHaWU8mG+Hej3rbQBvUU76DQagEmRuwnK3HB8qRWllDoN+W6gd7lg74rKKYdLo3uRY8K5NHCtnQq4OfLzSil1CvLdHP3hbfZ5re5An5yWyxFXLy48+q1drzV6pdQZwndr9Hu/t//HjwLsQ0Z+NL0RXHZ5U099oJRSpyjfDfT7VkJ4G2jVBYCVu46QEeUexBTRDsJ/vidZKaXUyeS7gX7vSpu2EaGk3Mnqvdm06T4UgiM1baOUOqP4Zo6+vARyU6HNTADW7suhpNzFqG6xMOAdCNPavFLqzOFVjV5EJojIVhHZISK1HgUoIp1E5CsRSRaRZSIS514+SERWikiKe93VTf0B6lRaYP8PjABsft5PYHjnVnbwlE4xoJQ6gxwz0IuIA3gGmAj0Aa4RkZpz+z4KvGGMGQDMAR5xLy8EbjDG9AUmAE+IyAkMG/VSZaAPBWx+vm/7lrQMCWj2Uyul1KnGmxr9cGCHMWaXMaYUmAtMqrFNH+Ar9+ulFeuNMduMMdvdr9OBQ0Dz500qA30Yxhg2pB5laKeoZj+tUkqdirwJ9B2A/R7vU93LPK0HrnK/vhKIEJFozw1EZDgQCOyseQIRmSUiSSKSlJmZ6W3Z61fmDvQBYWQVlFJU5qRTdOiJH1cppU5D3gR6qWOZqfH+PmCciKwFxgFpQHnlAUTaAW8CNxpjXLUOZswLxphEY0xibGwTVPhLC+3/gaGk5RQB0CGyiR8wopRSpwlvet2kAh093scB6Z4buNMykwFEJBy4yhhz1P2+BfAZ8GdjzA9NUehj8kjdpGW5A32UBnql1JnJmxr9KqC7iHQWkUBgGvCJ5wYiEiMiFcd6AHjFvTwQ+BDbUPt+0xX7GDxSNxU1+rhITd0opc5Mxwz0xphy4A5gMbAZmGeMSRGROSJyuXuzc4CtIrINaAM87F4+FRgLzBSRde5/g5r6Q9Ti0esmNbuI8CB/WoT45pABpZQ6Fq+inzFmIbCwxrLZHq/nA/Pr2O8t4K0TLGPjVebow0nLOUSHyBBE6mpqUEop3+ebUyBUpm5CScsu0vy8UuqM5puBvrQAxAH+QaTlFGmPG6XUGc1HA30hBIaRX+rkaFEZ7TXQK6XOYL4Z6MsKKtM2oF0rlVJnNt8M9KUFtg99jm2U1dSNUupM5qOBvtCOinXX6OO0Rq+UOoP5aKDPd3etLCbQ4UdseNDJLpFSSp00vhnoywptjj6niHaRwfj5aR96pdSZyzcDfWXqplDz80qpM56PBvoCd+pG+9ArpZRvBvqyApz+IRzKK9GulUqpM55vBvrSQvJNEMZo10qllPK9QO9yQnkRR8sDAR0spZRSvhfoy+wgqewyOzGnzkOvlDrT+V6gd89Ff7gkABFo2zL4JBdIKaVOLp8N9BklDlpHBBHo73sfUSmlGsOrKCgiE0Rkq4jsEJE/1LG+k4h8JSLJIrJMROI81s0Qke3ufzOasvB1cqduDhb6aUOsUkrhRaAXEQfwDDAR6ANcIyJ9amz2KPa5sAOAOcAj7n1bAX8BRgDDgb+ISFTTFb8O7hp9WqEfHaI0P6+UUt7U6IcDO4wxu4wxpcBcYFKNbfoAX7lfL/VYfxGwxBiTZYzJBpYAE0682A1wB/rUAqF9pObnlVLKm0DfAdjv8T7VvczTeuAq9+srgQgRifZyX0RklogkiUhSZmamt2Wvmzt1k+sMIjIk8MSOpZRSPsCbQF/XjGCmxvv7gHEishYYB6QB5V7uizHmBWNMojEmMTY21osiNcBdoy8kiJAAbYhVSil/L7ZJBTp6vI8D0j03MMakA5MBRCQcuMoYc1REUoFzauy77ATKe2wVgd4EERLoaNZTKaXU6cCbKu8qoLuIdBaRQGAa8InnBiISIyIVx3oAeMX9ejFwoYhEuRthL3Qvaz6VNfpgQgK9uY4ppZRvO2agN8aUA3dgA/RmYJ4xJkVE5ojI5e7NzgG2isg2oA3wsHvfLOCv2IvFKmCOe1nzcefoiwgiJEBr9Eop5VWV1xizEFhYY9lsj9fzgfn17PsKVTX85ldagNMRjAs/DfRKKYWPjox1+tuBUiGBvvfxlFKqsXwvEpYV4nTYgVLBWqNXSikfDPSlBZQ53DV6DfRKKeWjgd6vInWjgV4ppXwv0JcVUqI1eqWUquR7gb40nxKxc9xojl4ppXwy0BdSIsGIQJDORa+UUj4Y6MsKKSKYkAAHInVNtaOUUmcW3wv0pQXuCc00baOUUuBrgd6YykCv+XmllLJ8K9CXl4BxUmiCCdWulUopBfhaoHdPaJbvCtQ+9Eop5eZbgd49RXGe0dSNUkpV8M1A7wzUxlillHLzrUBfpoFeKaVq8q1AX2pz9DlOzdErpVQFrwK9iEwQka0iskNE/lDH+ngRWSoia0UkWUQudi8PEJHXRWSDiGwWkQea+gNU407dHC0P1By9Ukq5HTPQi4gDeAaYCPQBrhGRPjU2+zP2EYODsc+Ufda9/BdAkDGmPzAU+LWIJDRN0evgTt1klwdo6kYppdy8qdEPB3YYY3YZY0qBucCkGtsYoIX7dUsg3WN5mIj4AyFAKZB7wqWuj7tGn10WoE+XUkopN2+iYQdgv8f7VPcyTw8C00UkFfts2Tvdy+cDBcABYB/waLM+HNydo89zaWOsUkpV8CbQ1zUzmKnx/hrgNWNMHHAx8KaI+GHvBpxAe6AzcK+IdKl1ApFZIpIkIkmZmZmN+gDVuFM3hQRrjl4ppdy8CfSpQEeP93FUpWYq/BKYB2CMWQkEAzHAtcAiY0yZMeYQ8D2QWPMExpgXjDGJxpjE2NjYxn+KCqUFGHFQir/2ulFKKTdvAv0qoLuIdBaRQGxj6yc1ttkHjAcQkd7YQJ/pXn6eWGHASGBLUxW+ltJCTEAYIDrXjVJKuR0z0BtjyoE7gMXAZmzvmhQRmSMil7s3uxf4lYisB94FZhpjDLa3TjiwEXvBeNUYk9wMn8MqK8Dpr48RVEopT/7ebGSMWYhtZPVcNtvj9SZgdB375WO7WP48Sgtw+ocC+hhBpZSq4Ft9EEsLKXfYQK81eqWUsnws0OdT7rAPBtfGWKWUsnwr0JcVUuqnOXqllPLkW4G+tCrQa45eKaUsHwv0BZT4aepGKaU8+VagLyugWNyBXmv0SikF+FqgLy2kCBvoNXWjlFKW7wR6lxPKiygimEB/Pxx+dU3Ro5RSZx7fCfRldubKQoI0baOUUh58J9CXFoL4UWA00CullCffCfQRbWB2FsvCL9EJzZRSyoPvBHoAEQrLjDbEKqWUB98K9EBxmVP70CullAefC/RFZU7N0SullAffC/SlTk3dKKWUB58L9Jq6UUqp6rwK9CIyQUS2isgOEflDHevjRWSpiKwVkWQRudhj3QARWSkiKSKyQcQ9R0Ezsakbn7t+KaXUcTvmE6ZExIF9JOAF2AeFrxKRT9xPlarwZ+wjBp8TkT7Yp1EliIg/8BZwvTFmvYhEA2VN/ik8aI5eKaWq86bqOxzYYYzZZYwpBeYCk2psY4AW7tctgXT36wuBZGPMegBjzBFjjPPEi12/olInwZq6UUqpSt4E+g7Afo/3qe5lnh4EpotIKrY2f6d7eQ/AiMhiEVkjIr8/wfI2yOUylJS7tEavlFIevAn0dc0OZmq8vwZ4zRgTB1wMvCkiftjU0BjgOvf/V4rI+FonEJklIkkikpSZmdmoD+CpuNzeLGigV0qpKt4E+lSgo8f7OKpSMxV+3o7ajwAAB4dJREFUCcwDMMasBIKBGPe+3xhjDhtjCrG1/SE1T2CMecEYk2iMSYyNjW38p3ArKnUHek3dKKVUJW8C/Sqgu4h0FpFAYBrwSY1t9gHjAUSkNzbQZwKLgQEiEupumB0HbKKZFJVpjV4ppWo6Zq8bY0y5iNyBDdoO4BVjTIqIzAGSjDGfAPcCL4rIb7BpnZnGGANki8jj2IuFARYaYz5rrg9TXKY1eqWUqumYgR7AGLMQm3bxXDbb4/UmYHQ9+76F7WLZ7ApLtUavlFI1+dTIoiIN9EopVYtvBXp36kb70SulVBWfCvTF2hirlFK1+FSg1143SilVm28F+lIXoL1ulFLKk28F+oocvdbolVKqkk8Fes3RK6VUbT4V6ItKnTj8hABHXdPzKKXUmcm3Ar17LnoRDfRKKVXB5wK95ueVUqo6nwr0xaVOQrXHjVJKVeNTgV4fI6iUUrX5XKDX6Q+UUqo6nwr0haVOQgJ86iMppdQJ86moWKypG6WUqsWnAn1RqVOnP1BKqRp8K9Br90qllKrFq0AvIhNEZKuI7BCRP9SxPl5ElorIWhFJFpGL61ifLyL3NVXB66KpG6WUqu2YgV5EHMAzwESgD3CNiPSpsdmfgXnGmMHYh4c/W2P9v4DPT7y4DSsq1UCvlFI1eVOjHw7sMMbsMsaUAnOBSTW2MUAL9+uWQHrFChG5AtgFpJx4cetnjLH96DVHr5RS1XgT6DsA+z3ep7qXeXoQmC4iqdiHiN8JICJhwP3AQw2dQERmiUiSiCRlZmZ6WfTqSp0uXEanKFZKqZq8CfR1zRBmary/BnjNGBMHXAy8KSJ+2AD/L2NMfkMnMMa8YIxJNMYkxsbGelPuWoorHjqigV4pparx92KbVKCjx/s4PFIzbr8EJgAYY1aKSDAQA4wApojIP4FIwCUixcaYp0+45DUJXDKgHV1bhzf5oZVS6nTmTaBfBXQXkc5AGrax9doa2+wDxgOviUhvIBjINMacXbGBiDwI5DdLkAdahgTwzLVDmuPQSil1Wjtm6sYYUw7cASwGNmN716SIyBwRufz/27u/ECnLKI7j3x+alkaoRVEqqbBUEpSxhP0hwrpIi+yimwgyiLoJshDC6KrLIKICEUIri7DIpMSLIEzoqq3d/pilpf3DrS03SotuNDpdPM/CZDuk7L69eub3gWHmeeZl5xzO7NmZZ96dpx62BrhX0ifAZuDuiDh2ecfMzFqgk60f9/f3x+DgYNthmJmdUiQNRUT/ePel+s9YMzP7Nzd6M7Pk3OjNzJJzozczS86N3swsOTd6M7PkTrrTKyWNAt9N4EecA/w8SeGcKnoxZ+jNvHsxZ+jNvE805wsjYtzvkDnpGv1ESRrsdi5pVr2YM/Rm3r2YM/Rm3pOZs5duzMySc6M3M0suY6N/tu0AWtCLOUNv5t2LOUNv5j1pOadbozczs3/K+IrezMw6uNGbmSWXptFLuknSF5L2S1rbdjxNkTRf0k5JeyR9Jml1nZ8j6W1J++r17LZjnWySpkj6SNL2Ol4oaaDm/KqkaW3HONkkzZK0RdLeWvOrstda0kP1ub1b0mZJp2estaTnJB2UtLtjbtzaqnim9rddkk5ol6UUjV7SFGAdsBxYDNwhaXG7UTXmT2BNRFwCLAXur7muBXZERB+wo46zWU3Z/GbM45Q9ifuAXylbWmbzNPBWRFwMXEbJP22tJc0FHgD6I+JSYAplV7uMtX6BugVrh261XQ701ct9wPoTeaAUjR64EtgfEV9HxBHgFWBlyzE1IiJGIuLDevt3yi/+XEq+m+phm4Db2omwGZLmATcDG+pYwDJgSz0kY85nAdcBGwEi4khEHCJ5rSlbnJ4haSowAxghYa0j4l3gl2Omu9V2JfBiFO8BsySdf7yPlaXRzwUOdIyH61xqkhYAS4AB4LyIGIHyxwA4t73IGvEU8DDwVx2fDRyqW11CzpovAkaB5+uS1QZJM0lc64j4HniCsg/1CHAYGCJ/rcd0q+2EelyWRq9x5lKfNyrpTOB14MGI+K3teJok6RbgYEQMdU6Pc2i2mk8FrgDWR8QS4A8SLdOMp65JrwQWAhcAMynLFsfKVuv/MqHne5ZGPwzM7xjPA35oKZbGSTqN0uRfjoitdfqnsbdy9fpgW/E14BrgVknfUpblllFe4c+qb+8hZ82HgeGIGKjjLZTGn7nWNwLfRMRoRBwFtgJXk7/WY7rVdkI9Lkuj/wDoq5/MT6N8eLOt5ZgaUdemNwJ7IuLJjru2Aavq7VXAm/93bE2JiEciYl5ELKDU9p2IuBPYCdxeD0uVM0BE/AgckHRRnboB+JzEtaYs2SyVNKM+18dyTl3rDt1quw24q559sxQ4PLbEc1wiIsUFWAF8CXwFPNp2PA3meS3lLdsu4ON6WUFZs94B7KvXc9qOtaH8rwe219uLgPeB/cBrwPS242sg38uBwVrvN4DZ2WsNPAbsBXYDLwHTM9Ya2Ez5HOIo5RX7Pd1qS1m6WVf726eUs5KO+7H8FQhmZsllWboxM7Mu3OjNzJJzozczS86N3swsOTd6M7Pk3OjNzJJzozczS+5vIKWEgS+7HIcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend(['training', 'validation'], loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1026/1026 [==============================] - 2s 2ms/step - loss: 0.1968 - accuracy: 0.9639\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.9639354348182678\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(X_test_sub)\n",
    "y_pred = predicted.argmax(axis=1)\n",
    "\n",
    "\n",
    "ImageID = np.arange(len(y_pred))+1\n",
    "Out = pd.DataFrame([test['filename'],y_pred]).T\n",
    "Out.rename(columns = {0:'filename', 1:'label'})\n",
    "#Out\n",
    "Out.to_csv('MNIST_Prediction_Batch_Elu_LR_0_1_Adam.csv', header =  ['filename', 'label' ], index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Elu & batch - Test accuracy:  0.963965892791748 - AV Accu - 96.7\n",
    "\n",
    "#### With Elu, batch & Initializer - Test accuracy:  0.9626560807228088 - AV Accu - 96.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model_Initialization_Test(i):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(50, input_shape = (784, )))\n",
    "    model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(50, kernel_initializer = i))\n",
    "    model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(50, kernel_initializer = i))\n",
    "    model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(50, kernel_initializer = i))\n",
    "    model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(50, kernel_initializer = i))\n",
    "    model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    #adam = optimizers.Adam(lr = 0.01)\n",
    "    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1026/1026 [==============================] - 2s 2ms/step - loss: 0.2077 - accuracy: 0.9634\n",
      "Test accuracy:  0.9634480476379395\n",
      "1026/1026 [==============================] - 2s 2ms/step - loss: 0.2505 - accuracy: 0.9568\n",
      "Test accuracy:  0.9568077921867371\n"
     ]
    }
   ],
   "source": [
    "for i in ['he_normal','random_normal','truncated_normal']:\n",
    "    model = mlp_model_Initialization_Test(i)\n",
    "    history = model.fit(X_train, y_train, validation_split = 0.2, epochs = 100, verbose = 0)\n",
    "\n",
    "    results = model.evaluate(x_test, y_test)\n",
    "\n",
    "    print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1026/1026 [==============================] - 2s 2ms/step - loss: 0.2697 - accuracy: 0.9571\n",
      "Test accuracy:  0.9571428298950195\n"
     ]
    }
   ],
   "source": [
    "model = mlp_model_Initialization_Test('truncated_normal')\n",
    "history = model.fit(X_train, y_train, validation_split = 0.2, epochs = 100, verbose = 0)\n",
    "\n",
    "results = model.evaluate(x_test, y_test)\n",
    "\n",
    "print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(X_test_sub)\n",
    "y_pred = predicted.argmax(axis=1)\n",
    "\n",
    "\n",
    "ImageID = np.arange(len(y_pred))+1\n",
    "Out = pd.DataFrame([test['filename'],y_pred]).T\n",
    "Out.rename(columns = {0:'filename', 1:'label'})\n",
    "#Out\n",
    "Out.to_csv('MNIST_Prediction_Batch_Elu_TrucNormalInit_100Epo.csv', header =  ['filename', 'label' ], index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model_Initialization_ObjTest(i):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(50, input_shape = (784, )))\n",
    "    model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(50, kernel_initializer = i))\n",
    "    model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(50, kernel_initializer = i))\n",
    "    model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(50, kernel_initializer = i))\n",
    "    model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(50, kernel_initializer = i))\n",
    "    model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    #adam = optimizers.Adam(lr = 0.01)\n",
    "    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "initList = [keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n",
    ",keras.initializers.Ones()\n",
    ",keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
    ",keras.initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=None)\n",
    ",keras.initializers.VarianceScaling(scale=1.0, mode='fan_in', distribution='normal', seed=None)\n",
    ",keras.initializers.Orthogonal(gain=1.0, seed=None)\n",
    ",keras.initializers.lecun_uniform(seed=None)\n",
    ",keras.initializers.glorot_normal(seed=None)\n",
    ",keras.initializers.glorot_uniform(seed=None)\n",
    ",keras.initializers.he_normal(seed=None)\n",
    ",keras.initializers.lecun_normal(seed=None)\n",
    ",keras.initializers.he_uniform(seed=None)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1026/1026 [==============================] - 2s 2ms/step - loss: 0.1843 - accuracy: 0.9547\n",
      "Test accuracy:  0.9547365307807922\n",
      "1026/1026 [==============================] - 2s 2ms/step - loss: 0.4724 - accuracy: 0.8742\n",
      "Test accuracy:  0.8742004036903381\n",
      "1026/1026 [==============================] - 2s 2ms/step - loss: 0.1759 - accuracy: 0.9555\n",
      "Test accuracy:  0.9555284976959229\n",
      "1026/1026 [==============================] - 2s 2ms/step - loss: 0.1861 - accuracy: 0.9528\n",
      "Test accuracy:  0.9528175592422485\n",
      "1026/1026 [==============================] - 2s 2ms/step - loss: 0.1699 - accuracy: 0.9580\n",
      "Test accuracy:  0.9579652547836304\n",
      "1026/1026 [==============================] - 2s 2ms/step - loss: 0.1953 - accuracy: 0.9512\n",
      "Test accuracy:  0.9512031674385071\n",
      "1026/1026 [==============================] - 2s 2ms/step - loss: 0.1644 - accuracy: 0.9580\n",
      "Test accuracy:  0.9580261707305908\n",
      "1026/1026 [==============================] - 2s 2ms/step - loss: 0.1998 - accuracy: 0.9521\n",
      "Test accuracy:  0.9521169662475586\n",
      "1026/1026 [==============================] - 2s 2ms/step - loss: 0.1989 - accuracy: 0.9526\n",
      "Test accuracy:  0.952573835849762\n",
      "1026/1026 [==============================] - 2s 2ms/step - loss: 0.1837 - accuracy: 0.9557\n",
      "Test accuracy:  0.955680787563324\n",
      "1026/1026 [==============================] - 2s 2ms/step - loss: 0.1768 - accuracy: 0.9576\n",
      "Test accuracy:  0.9576302170753479\n",
      "1026/1026 [==============================] - 2s 2ms/step - loss: 0.1753 - accuracy: 0.9560\n",
      "Test accuracy:  0.9559853672981262\n"
     ]
    }
   ],
   "source": [
    "for i in initList:\n",
    "    model = mlp_model_Initialization_ObjTest(i)\n",
    "    history = model.fit(X_train, y_train, validation_split = 0.2, epochs = 20, verbose = 0)\n",
    "\n",
    "    results = model.evaluate(x_test, y_test)\n",
    "\n",
    "    print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with Init: <tensorflow.python.keras.initializers.initializers_v2.HeUniform object at 0x0000021E87B9BFC8>  is 0.9559853672981262\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy with Init:', i.distribution,' is', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1026/1026 [==============================] - 2s 2ms/step - loss: 0.2060 - accuracy: 0.9620\n",
      "Accuracy-Init: <tensorflow.python.keras.initializers.initializers_v2.RandomUniform object at 0x0000021E87B9B1C8>  is 0.9620164632797241\n",
      "1026/1026 [==============================] - 2s 2ms/step - loss: 0.3829 - accuracy: 0.9219\n",
      "Accuracy-Init: <tensorflow.python.keras.initializers.initializers_v2.Ones object at 0x0000021E87B9B848>  is 0.921870231628418\n",
      "1026/1026 [==============================] - 2s 2ms/step - loss: 0.2337 - accuracy: 0.9611\n",
      "Accuracy-Init: <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x0000021E87B9B888>  is 0.9610722064971924\n",
      "1026/1026 [==============================] - 1s 938us/step - loss: 0.2251 - accuracy: 0.9634\n",
      "Accuracy-Init: <tensorflow.python.keras.initializers.initializers_v2.TruncatedNormal object at 0x0000021E87B9BDC8>  is 0.9634480476379395\n",
      "1026/1026 [==============================] - 1s 941us/step - loss: 0.2090 - accuracy: 0.9623\n",
      "Accuracy-Init: <tensorflow.python.keras.initializers.initializers_v2.VarianceScaling object at 0x0000021E87B9B208>  is 0.9623210430145264\n",
      "1026/1026 [==============================] - 1s 986us/step - loss: 0.2551 - accuracy: 0.9567\n",
      "Accuracy-Init: <tensorflow.python.keras.initializers.initializers_v2.Orthogonal object at 0x0000021E87B9B7C8>  is 0.9567164182662964\n",
      "1026/1026 [==============================] - 1s 1ms/step - loss: 0.2420 - accuracy: 0.9596\n",
      "Accuracy-Init: <tensorflow.python.keras.initializers.initializers_v2.LecunUniform object at 0x0000021E87B9B5C8>  is 0.9595796465873718\n",
      "1026/1026 [==============================] - 1s 962us/step - loss: 0.2231 - accuracy: 0.96190s - loss: 0.2244 - accuracy: 0.96\n",
      "Accuracy-Init: <tensorflow.python.keras.initializers.initializers_v2.GlorotNormal object at 0x0000021E87B9B8C8>  is 0.9619250893592834\n",
      "1026/1026 [==============================] - 1s 946us/step - loss: 0.2281 - accuracy: 0.9618\n",
      "Accuracy-Init: <tensorflow.python.keras.initializers.initializers_v2.GlorotUniform object at 0x0000021E87B9B908>  is 0.9617727398872375\n",
      "1026/1026 [==============================] - 1s 969us/step - loss: 0.2084 - accuracy: 0.9642\n",
      "Accuracy-Init: <tensorflow.python.keras.initializers.initializers_v2.HeNormal object at 0x0000021E87B9BC48>  is 0.9642095565795898\n",
      "1026/1026 [==============================] - 1s 971us/step - loss: 0.2095 - accuracy: 0.9633\n",
      "Accuracy-Init: <tensorflow.python.keras.initializers.initializers_v2.LecunNormal object at 0x0000021E87B9BA48>  is 0.9632957577705383\n",
      "1026/1026 [==============================] - 1s 970us/step - loss: 0.2066 - accuracy: 0.9627\n",
      "Accuracy-Init: <tensorflow.python.keras.initializers.initializers_v2.HeUniform object at 0x0000021E87B9BFC8>  is 0.9626560807228088\n"
     ]
    }
   ],
   "source": [
    "for i in initList:\n",
    "    model = mlp_model_Initialization_ObjTest(i)\n",
    "    history = model.fit(X_train, y_train, validation_split = 0.2, epochs = 100, verbose = 0)\n",
    "\n",
    "    results = model.evaluate(x_test, y_test)\n",
    "\n",
    "    print('Accuracy-Init:', i,' is', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = optimizers.Adam(learning_rate=0.01,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07,\n",
    "    amsgrad=False,\n",
    "    name='Adam')\n",
    "sgd = optimizers.SGD(learning_rate=0.01,momentum=0.0,    nesterov=False,    name='SGD')\n",
    "momentum = optimizers.SGD(learning_rate=0.01,\n",
    "    momentum=0.8,\n",
    "    nesterov=False,\n",
    "    name='SGD-Momentum')\n",
    "opts = [adam, sgd, momentum]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model_Optimization_Test(i):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(50, input_shape = (784, )))\n",
    "    model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(50, kernel_initializer = 'he_normal'))\n",
    "    model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(50, kernel_initializer = 'he_normal'))\n",
    "    model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(50, kernel_initializer = 'he_normal'))\n",
    "    model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(50, kernel_initializer = 'he_normal'))\n",
    "    model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    #adam = optimizers.Adam(lr = 0.01)\n",
    "    model.compile(optimizer = i, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1026/1026 [==============================] - 2s 2ms/step - loss: 0.2053 - accuracy: 0.9642\n",
      "Accuracy with: <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x00000286A0AD2BC8>  is 0.9641790986061096\n",
      "1026/1026 [==============================] - 2s 2ms/step - loss: 0.1720 - accuracy: 0.9579\n",
      "Accuracy with: <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x00000286A0AD2AC8>  is 0.9578738808631897\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-d0e7b22f4139>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mopts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmlp_model_Optimization_Test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in opts:\n",
    "    model = mlp_model_Optimization_Test(i)\n",
    "    history = model.fit(X_train, y_train, validation_split = 0.2, epochs = 100, verbose = 0)\n",
    "\n",
    "    results = model.evaluate(x_test, y_test)\n",
    "\n",
    "    print('Accuracy with:', i,' is', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    model = mlp_model_Optimization_Test(opts[0])\n",
    "    history = model.fit(X_train, y_train, validation_split = 0.2, epochs = 100, verbose = 0)\n",
    "\n",
    "    results = model.evaluate(x_test, y_test)\n",
    "\n",
    "    print('Accuracy with:', i,' is', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-a6ad9beb9cc0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_sub\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mImageID\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "predicted = model.predict(X_test_sub)\n",
    "y_pred = predicted.argmax(axis=1)\n",
    "\n",
    "\n",
    "ImageID = np.arange(len(y_pred))+1\n",
    "Out = pd.DataFrame([test['filename'],y_pred]).T\n",
    "Out.rename(columns = {0:'filename', 1:'label'})\n",
    "#Out\n",
    "Out.to_csv('MNIST_Prediction_Batch_Elu_OptimizerAdam.csv', header =  ['filename', 'label' ], index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    model = mlp_model_Optimization_Test(opts[1])\n",
    "    history = model.fit(X_train, y_train, validation_split = 0.2, epochs = 100, verbose = 0)\n",
    "\n",
    "    results = model.evaluate(x_test, y_test)\n",
    "\n",
    "    print('Accuracy with:', i,' is', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(X_test_sub)\n",
    "y_pred = predicted.argmax(axis=1)\n",
    "\n",
    "\n",
    "ImageID = np.arange(len(y_pred))+1\n",
    "Out = pd.DataFrame([test['filename'],y_pred]).T\n",
    "Out.rename(columns = {0:'filename', 1:'label'})\n",
    "#Out\n",
    "Out.to_csv('MNIST_Prediction_Batch_Elu_OptimizerSGD.csv', header =  ['filename', 'label' ], index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1026/1026 [==============================] - 2s 2ms/step - loss: 0.1637 - accuracy: 0.9640\n",
      "Accuracy with: <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x0000021EA06C3DC8>  is 0.9639963507652283\n"
     ]
    }
   ],
   "source": [
    "    model = mlp_model_Optimization_Test(opts[2])\n",
    "    history = model.fit(X_train, y_train, validation_split = 0.2, epochs = 100, verbose = 0)\n",
    "\n",
    "    results = model.evaluate(x_test, y_test)\n",
    "\n",
    "    print('Accuracy with:', i,' is', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(X_test_sub)\n",
    "y_pred = predicted.argmax(axis=1)\n",
    "\n",
    "\n",
    "ImageID = np.arange(len(y_pred))+1\n",
    "Out = pd.DataFrame([test['filename'],y_pred]).T\n",
    "Out.rename(columns = {0:'filename', 1:'label'})\n",
    "#Out\n",
    "Out.to_csv('MNIST_Prediction_Batch_Elu_OptimizerMomentum.csv', header =  ['filename', 'label' ], index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result:\n",
    "    1026/1026 [==============================] - 2s 2ms/step - loss: 0.2053 - accuracy: 0.9642\n",
    "    Accuracy with: <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x00000286A0AD2BC8>  is 0.9641790986061096\n",
    "    \n",
    "### Optimizer Used:\n",
    "    adam = optimizers.Adam(learning_rate=0.01,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07,\n",
    "    amsgrad=False,\n",
    "    name='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Activation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def activation(layer, layer_in, layerId, tensor=True):\n",
    "    out = {}\n",
    "    if (layer['info']['type'] == 'ReLU'):\n",
    "        if ('negative_slope' in layer['params'] and layer['params']['negative_slope'] != 0):\n",
    "            out[layerId] = LeakyReLU(alpha=layer['params']['negative_slope'])\n",
    "        else:\n",
    "            out[layerId] = Activation('relu')\n",
    "    elif (layer['info']['type'] == 'PReLU'):\n",
    "        out[layerId] = PReLU()\n",
    "    elif (layer['info']['type'] == 'ELU'):\n",
    "        out[layerId] = ELU(alpha=layer['params']['alpha'])\n",
    "    elif (layer['info']['type'] == 'ThresholdedReLU'):\n",
    "        out[layerId] = ThresholdedReLU(theta=layer['params']['theta'])\n",
    "    elif (layer['info']['type'] == 'Sigmoid'):\n",
    "        out[layerId] = Activation('sigmoid')\n",
    "    elif (layer['info']['type'] == 'TanH'):\n",
    "        out[layerId] = Activation('tanh')\n",
    "    elif (layer['info']['type'] == 'Softmax'):\n",
    "        out[layerId] = Activation('softmax')\n",
    "    elif (layer['info']['type'] == 'SELU'):\n",
    "        out[layerId] = Activation('selu')\n",
    "    elif (layer['info']['type'] == 'Softplus'):\n",
    "        out[layerId] = Activation('softplus')\n",
    "    elif (layer['info']['type'] == 'Softsign'):\n",
    "        out[layerId] = Activation('softsign')\n",
    "    elif (layer['info']['type'] == 'HardSigmoid'):\n",
    "        out[layerId] = Activation('hard_sigmoid')\n",
    "    elif (layer['info']['type'] == 'Linear'):\n",
    "        out[layerId] = Activation('linear')\n",
    "    if tensor:\n",
    "        out[layerId] = out[layerId](*layer_in)\n",
    "    return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model_Activation_Test(i):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(50, input_shape = (784, )))\n",
    "    model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "    model.add(Activation(i))    \n",
    "    model.add(Dense(50, kernel_initializer = 'he_normal'))\n",
    "    model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "    model.add(Activation(i))    \n",
    "    model.add(Dense(50, kernel_initializer = 'he_normal'))\n",
    "    model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "    model.add(Activation(i))    \n",
    "    model.add(Dense(50, kernel_initializer = 'he_normal'))\n",
    "    model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "    model.add(Activation(i))    \n",
    "    model.add(Dense(50, kernel_initializer = 'he_normal'))\n",
    "    model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "    model.add(Activation(i))    \n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    adam = optimizers.Adam(learning_rate=0.01,beta_1=0.9,beta_2=0.999,epsilon=1e-07,amsgrad=False,name='Adam')\n",
    "    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ActList = ['relu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1026/1026 [==============================] - 2s 2ms/step - loss: 0.2208 - accuracy: 0.9628\n",
      "Accuracy with: <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x0000021EA06C3DC8>  is 0.962838888168335\n"
     ]
    }
   ],
   "source": [
    "model = mlp_model_Activation_Test('selu')\n",
    "history = model.fit(X_train, y_train, validation_split = 0.2, epochs = 100, verbose = 0)\n",
    "\n",
    "results = model.evaluate(x_test, y_test)\n",
    "\n",
    "print('Accuracy with:', i,' is', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune with Layer size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model_LayerSize():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(300, input_shape = (784, )))\n",
    "    model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(200, kernel_initializer = 'he_normal'))\n",
    "    model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(100, kernel_initializer = 'he_normal'))\n",
    "    model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(50, kernel_initializer = 'he_normal'))\n",
    "    model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(20, kernel_initializer = 'he_normal'))\n",
    "    model.add(BatchNormalization())                    # Add Batchnorm layer before Activation\n",
    "    model.add(Activation('elu'))    \n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    adam = optimizers.Adam(learning_rate=0.01,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07,\n",
    "    amsgrad=True,\n",
    "    name='Adam')\n",
    "    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "405/405 [==============================] - 3s 9ms/step - loss: 0.5136 - accuracy: 0.8419 - val_loss: 0.3575 - val_accuracy: 0.8964\n",
      "Epoch 2/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.3130 - accuracy: 0.9037 - val_loss: 0.2838 - val_accuracy: 0.9150\n",
      "Epoch 3/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.2326 - accuracy: 0.9286 - val_loss: 0.2055 - val_accuracy: 0.9385\n",
      "Epoch 4/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.1874 - accuracy: 0.9429 - val_loss: 0.1925 - val_accuracy: 0.9416\n",
      "Epoch 5/100\n",
      "405/405 [==============================] - 3s 8ms/step - loss: 0.1510 - accuracy: 0.9512 - val_loss: 0.3117 - val_accuracy: 0.9224\n",
      "Epoch 6/100\n",
      "405/405 [==============================] - 3s 8ms/step - loss: 0.1170 - accuracy: 0.9625 - val_loss: 0.1686 - val_accuracy: 0.9518\n",
      "Epoch 7/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.1076 - accuracy: 0.9636 - val_loss: 0.1940 - val_accuracy: 0.9459\n",
      "Epoch 8/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0972 - accuracy: 0.9692 - val_loss: 0.1606 - val_accuracy: 0.9552\n",
      "Epoch 9/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0807 - accuracy: 0.9739 - val_loss: 0.1651 - val_accuracy: 0.9555\n",
      "Epoch 10/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0650 - accuracy: 0.9791 - val_loss: 0.1845 - val_accuracy: 0.9499\n",
      "Epoch 11/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0585 - accuracy: 0.9815 - val_loss: 0.1824 - val_accuracy: 0.9524\n",
      "Epoch 12/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0518 - accuracy: 0.9828 - val_loss: 0.1869 - val_accuracy: 0.9502\n",
      "Epoch 13/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0576 - accuracy: 0.9797 - val_loss: 0.1693 - val_accuracy: 0.9558\n",
      "Epoch 14/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0683 - accuracy: 0.9775 - val_loss: 0.1626 - val_accuracy: 0.9620\n",
      "Epoch 15/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0409 - accuracy: 0.9870 - val_loss: 0.1516 - val_accuracy: 0.9626\n",
      "Epoch 16/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0367 - accuracy: 0.9879 - val_loss: 0.1610 - val_accuracy: 0.9607\n",
      "Epoch 17/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0281 - accuracy: 0.9906 - val_loss: 0.1695 - val_accuracy: 0.9604\n",
      "Epoch 18/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0353 - accuracy: 0.9892 - val_loss: 0.1580 - val_accuracy: 0.9620\n",
      "Epoch 19/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0345 - accuracy: 0.9892 - val_loss: 0.1580 - val_accuracy: 0.9632\n",
      "Epoch 20/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0303 - accuracy: 0.9910 - val_loss: 0.1535 - val_accuracy: 0.9576\n",
      "Epoch 21/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0229 - accuracy: 0.9923 - val_loss: 0.1553 - val_accuracy: 0.9623\n",
      "Epoch 22/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0221 - accuracy: 0.9921 - val_loss: 0.1869 - val_accuracy: 0.9558\n",
      "Epoch 23/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0204 - accuracy: 0.9937 - val_loss: 0.1486 - val_accuracy: 0.9644\n",
      "Epoch 24/100\n",
      "405/405 [==============================] - 3s 8ms/step - loss: 0.0181 - accuracy: 0.9940 - val_loss: 0.1593 - val_accuracy: 0.9669\n",
      "Epoch 25/100\n",
      "405/405 [==============================] - 3s 8ms/step - loss: 0.0209 - accuracy: 0.9934 - val_loss: 0.1975 - val_accuracy: 0.9561\n",
      "Epoch 26/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0176 - accuracy: 0.9934 - val_loss: 0.1556 - val_accuracy: 0.9638\n",
      "Epoch 27/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0227 - accuracy: 0.9920 - val_loss: 0.1684 - val_accuracy: 0.9623\n",
      "Epoch 28/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0215 - accuracy: 0.9923 - val_loss: 0.1809 - val_accuracy: 0.9592\n",
      "Epoch 29/100\n",
      "405/405 [==============================] - 3s 8ms/step - loss: 0.0222 - accuracy: 0.9934 - val_loss: 0.1435 - val_accuracy: 0.9663\n",
      "Epoch 30/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0275 - accuracy: 0.9910 - val_loss: 0.1639 - val_accuracy: 0.9654\n",
      "Epoch 31/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0189 - accuracy: 0.9931 - val_loss: 0.1601 - val_accuracy: 0.9623\n",
      "Epoch 32/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0108 - accuracy: 0.9964 - val_loss: 0.1483 - val_accuracy: 0.9691\n",
      "Epoch 33/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0120 - accuracy: 0.9953 - val_loss: 0.1539 - val_accuracy: 0.9669\n",
      "Epoch 34/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0130 - accuracy: 0.9958 - val_loss: 0.1781 - val_accuracy: 0.9604\n",
      "Epoch 35/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0136 - accuracy: 0.9956 - val_loss: 0.1745 - val_accuracy: 0.9644\n",
      "Epoch 36/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0078 - accuracy: 0.9974 - val_loss: 0.1510 - val_accuracy: 0.9691\n",
      "Epoch 37/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0163 - accuracy: 0.9954 - val_loss: 0.1567 - val_accuracy: 0.9651\n",
      "Epoch 38/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0112 - accuracy: 0.9962 - val_loss: 0.1741 - val_accuracy: 0.9647\n",
      "Epoch 39/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0081 - accuracy: 0.9970 - val_loss: 0.1732 - val_accuracy: 0.9666\n",
      "Epoch 40/100\n",
      "405/405 [==============================] - 3s 8ms/step - loss: 0.0078 - accuracy: 0.9971 - val_loss: 0.1618 - val_accuracy: 0.9654\n",
      "Epoch 41/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0113 - accuracy: 0.9964 - val_loss: 0.1651 - val_accuracy: 0.9641\n",
      "Epoch 42/100\n",
      "405/405 [==============================] - 3s 8ms/step - loss: 0.0067 - accuracy: 0.9978 - val_loss: 0.1628 - val_accuracy: 0.9663\n",
      "Epoch 43/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0102 - accuracy: 0.9969 - val_loss: 0.1420 - val_accuracy: 0.9678\n",
      "Epoch 44/100\n",
      "405/405 [==============================] - 3s 8ms/step - loss: 0.0265 - accuracy: 0.9913 - val_loss: 0.1883 - val_accuracy: 0.9601\n",
      "Epoch 45/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0091 - accuracy: 0.9972 - val_loss: 0.1521 - val_accuracy: 0.9669\n",
      "Epoch 46/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0057 - accuracy: 0.9980 - val_loss: 0.1509 - val_accuracy: 0.9663\n",
      "Epoch 47/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0114 - accuracy: 0.9963 - val_loss: 0.1617 - val_accuracy: 0.9620\n",
      "Epoch 48/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0119 - accuracy: 0.9964 - val_loss: 0.1583 - val_accuracy: 0.9672\n",
      "Epoch 49/100\n",
      "405/405 [==============================] - 3s 8ms/step - loss: 0.0124 - accuracy: 0.9963 - val_loss: 0.1561 - val_accuracy: 0.9678\n",
      "Epoch 50/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0079 - accuracy: 0.9976 - val_loss: 0.1691 - val_accuracy: 0.9644\n",
      "Epoch 51/100\n",
      "405/405 [==============================] - 3s 8ms/step - loss: 0.0059 - accuracy: 0.9981 - val_loss: 0.1951 - val_accuracy: 0.9635\n",
      "Epoch 52/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0062 - accuracy: 0.9981 - val_loss: 0.1733 - val_accuracy: 0.9620\n",
      "Epoch 53/100\n",
      "405/405 [==============================] - 3s 8ms/step - loss: 0.0110 - accuracy: 0.9966 - val_loss: 0.1607 - val_accuracy: 0.9638\n",
      "Epoch 54/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0086 - accuracy: 0.9974 - val_loss: 0.1649 - val_accuracy: 0.9644\n",
      "Epoch 55/100\n",
      "405/405 [==============================] - 3s 8ms/step - loss: 0.0128 - accuracy: 0.9960 - val_loss: 0.1627 - val_accuracy: 0.9657\n",
      "Epoch 56/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0057 - accuracy: 0.9981 - val_loss: 0.1684 - val_accuracy: 0.9651\n",
      "Epoch 57/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0112 - accuracy: 0.9964 - val_loss: 0.1685 - val_accuracy: 0.9666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "405/405 [==============================] - 3s 8ms/step - loss: 0.0068 - accuracy: 0.9977 - val_loss: 0.1707 - val_accuracy: 0.9632\n",
      "Epoch 59/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0069 - accuracy: 0.9975 - val_loss: 0.1637 - val_accuracy: 0.9672\n",
      "Epoch 60/100\n",
      "405/405 [==============================] - 3s 8ms/step - loss: 0.0067 - accuracy: 0.9971 - val_loss: 0.1572 - val_accuracy: 0.9706\n",
      "Epoch 61/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0101 - accuracy: 0.9972 - val_loss: 0.1670 - val_accuracy: 0.9694\n",
      "Epoch 62/100\n",
      "405/405 [==============================] - 3s 8ms/step - loss: 0.0108 - accuracy: 0.9961 - val_loss: 0.1709 - val_accuracy: 0.9629\n",
      "Epoch 63/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0071 - accuracy: 0.9972 - val_loss: 0.1778 - val_accuracy: 0.9644\n",
      "Epoch 64/100\n",
      "405/405 [==============================] - 3s 8ms/step - loss: 0.0084 - accuracy: 0.9971 - val_loss: 0.1813 - val_accuracy: 0.9620\n",
      "Epoch 65/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0122 - accuracy: 0.9957 - val_loss: 0.1706 - val_accuracy: 0.9635\n",
      "Epoch 66/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0081 - accuracy: 0.9974 - val_loss: 0.1791 - val_accuracy: 0.9626\n",
      "Epoch 67/100\n",
      "405/405 [==============================] - 3s 8ms/step - loss: 0.0082 - accuracy: 0.9974 - val_loss: 0.1611 - val_accuracy: 0.9654\n",
      "Epoch 68/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0104 - accuracy: 0.9964 - val_loss: 0.1824 - val_accuracy: 0.9629\n",
      "Epoch 69/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0109 - accuracy: 0.9965 - val_loss: 0.1793 - val_accuracy: 0.9610\n",
      "Epoch 70/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0084 - accuracy: 0.9971 - val_loss: 0.1623 - val_accuracy: 0.9660\n",
      "Epoch 71/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0066 - accuracy: 0.9985 - val_loss: 0.1745 - val_accuracy: 0.9638\n",
      "Epoch 72/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0038 - accuracy: 0.9989 - val_loss: 0.1627 - val_accuracy: 0.9685\n",
      "Epoch 73/100\n",
      "405/405 [==============================] - 3s 8ms/step - loss: 0.0123 - accuracy: 0.9961 - val_loss: 0.1993 - val_accuracy: 0.9607\n",
      "Epoch 74/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0051 - accuracy: 0.9978 - val_loss: 0.1824 - val_accuracy: 0.9641\n",
      "Epoch 75/100\n",
      "405/405 [==============================] - 3s 8ms/step - loss: 0.0097 - accuracy: 0.9971 - val_loss: 0.1802 - val_accuracy: 0.9632\n",
      "Epoch 76/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0060 - accuracy: 0.9977 - val_loss: 0.1670 - val_accuracy: 0.9647\n",
      "Epoch 77/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0036 - accuracy: 0.9991 - val_loss: 0.1614 - val_accuracy: 0.9663\n",
      "Epoch 78/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0047 - accuracy: 0.9987 - val_loss: 0.1682 - val_accuracy: 0.9638\n",
      "Epoch 79/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0036 - accuracy: 0.9990 - val_loss: 0.1740 - val_accuracy: 0.9641\n",
      "Epoch 80/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0048 - accuracy: 0.9985 - val_loss: 0.1708 - val_accuracy: 0.9651\n",
      "Epoch 81/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.1768 - val_accuracy: 0.9635\n",
      "Epoch 82/100\n",
      "405/405 [==============================] - 3s 8ms/step - loss: 0.0184 - accuracy: 0.9949 - val_loss: 0.1673 - val_accuracy: 0.9647\n",
      "Epoch 83/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0146 - accuracy: 0.9958 - val_loss: 0.1594 - val_accuracy: 0.9685\n",
      "Epoch 84/100\n",
      "405/405 [==============================] - 3s 8ms/step - loss: 0.0058 - accuracy: 0.9982 - val_loss: 0.1784 - val_accuracy: 0.9660\n",
      "Epoch 85/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0071 - accuracy: 0.9978 - val_loss: 0.1701 - val_accuracy: 0.9647\n",
      "Epoch 86/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0031 - accuracy: 0.9989 - val_loss: 0.1700 - val_accuracy: 0.9644\n",
      "Epoch 87/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0078 - accuracy: 0.9979 - val_loss: 0.1690 - val_accuracy: 0.9647\n",
      "Epoch 88/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0100 - accuracy: 0.9972 - val_loss: 0.1679 - val_accuracy: 0.9647\n",
      "Epoch 89/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0039 - accuracy: 0.9990 - val_loss: 0.1633 - val_accuracy: 0.9647\n",
      "Epoch 90/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0056 - accuracy: 0.9981 - val_loss: 0.1493 - val_accuracy: 0.9660\n",
      "Epoch 91/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0046 - accuracy: 0.9989 - val_loss: 0.1918 - val_accuracy: 0.9647\n",
      "Epoch 92/100\n",
      "405/405 [==============================] - 3s 8ms/step - loss: 0.0063 - accuracy: 0.9979 - val_loss: 0.1584 - val_accuracy: 0.9669\n",
      "Epoch 93/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0037 - accuracy: 0.9991 - val_loss: 0.1545 - val_accuracy: 0.9691\n",
      "Epoch 94/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0085 - accuracy: 0.9978 - val_loss: 0.1865 - val_accuracy: 0.9604\n",
      "Epoch 95/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0077 - accuracy: 0.9976 - val_loss: 0.1579 - val_accuracy: 0.9697\n",
      "Epoch 96/100\n",
      "405/405 [==============================] - 3s 8ms/step - loss: 0.0044 - accuracy: 0.9989 - val_loss: 0.1634 - val_accuracy: 0.9644\n",
      "Epoch 97/100\n",
      "405/405 [==============================] - 3s 8ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.1523 - val_accuracy: 0.9685\n",
      "Epoch 98/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0033 - accuracy: 0.9990 - val_loss: 0.1520 - val_accuracy: 0.9678\n",
      "Epoch 99/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0057 - accuracy: 0.9979 - val_loss: 0.1556 - val_accuracy: 0.9666\n",
      "Epoch 100/100\n",
      "405/405 [==============================] - 3s 7ms/step - loss: 0.0067 - accuracy: 0.9983 - val_loss: 0.1469 - val_accuracy: 0.9675\n",
      "1026/1026 [==============================] - 3s 3ms/step - loss: 0.1510 - accuracy: 0.9736\n",
      "Accuracy 0.973621666431427\n"
     ]
    }
   ],
   "source": [
    "model = mlp_model_LayerSize()\n",
    "history = model.fit(X_train, y_train, validation_split = 0.2, epochs = 100, verbose = 1)\n",
    "\n",
    "results = model.evaluate(x_test, y_test)\n",
    "\n",
    "print('Accuracy', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(X_test_sub)\n",
    "y_pred = predicted.argmax(axis=1)\n",
    "\n",
    "\n",
    "ImageID = np.arange(len(y_pred))+1\n",
    "Out = pd.DataFrame([test['filename'],y_pred]).T\n",
    "Out.rename(columns = {0:'filename', 1:'label'})\n",
    "#Out\n",
    "Out.to_csv('MNIST_Digit_TuningNetworkSize_100Epo.csv', header =  ['filename', 'label' ], index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF210GPUJupyter",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
